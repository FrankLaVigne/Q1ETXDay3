{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c20833-7e84-403a-81df-844fc4929aff",
   "metadata": {},
   "source": [
    "# 0 Recap and the Evidence That Brought Us Here (10 min)\n",
    "Purpose: \n",
    "\n",
    "Reconnect participants to the Day 2 workflow, surface the specific failures that RAG alone could not resolve, and build the evidence-based case for why we are now considering model adaptation.\n",
    "No one should leave this section wondering \"why are we here today?\" The answer should be concrete, traceable, and grounded in outputs they already generated.\n",
    "\n",
    "## 0.1 Where Day 2 Left Us \n",
    "\n",
    "Day 2 followed a deliberate progression. We started with nothing: a model, no customer data, no retrieval, no context. We asked questions and watched the model improvise. It sounded fluent. It was often wrong.\n",
    "\n",
    "Then we added structure. We ingested documents with Docling. We preserved tables, headings, and semantic layout. We chunked those documents with care, respecting word boundaries and choosing chunk sizes that balanced coherence against precision. We embedded those chunks, stored them in a vector database, and wired up a retrieval pipeline.\n",
    "\n",
    "And it worked. For many questions, the system returned grounded, defensible answers drawn directly from the customer's documents. That was real progress.\n",
    "\n",
    "But it was not complete progress.\n",
    "\n",
    "Some questions still failed. Not because the system was broken, but because the nature of the failure had changed. The model was no longer hallucinating from ignorance. It was receiving relevant context and still producing answers that missed the mark.\n",
    "\n",
    "That distinction matters enormously, because the fix for \"model never saw the data\" is completely different from the fix for \"model saw the data and still got it wrong.\"\n",
    "\n",
    "Day 2 gave us a working RAG pipeline. It also gave us something more important: a clear, observable boundary where RAG stops being sufficient.\n",
    "\n",
    "Today starts at that boundary.\n",
    "\n",
    "## 0.2 The Escalation Ladder (Revisited) \n",
    "On Day 2 we introduced the escalation ladder. It looked like this:\n",
    "\n",
    "* Improve chunking and structure\n",
    "* Improve retrieval strategy\n",
    "* Improve prompting and grounding\n",
    "* Improve evaluation coverage\n",
    "\n",
    "Only then: consider changing the model itself\n",
    "\n",
    "We spent Day 2 working through steps 1 through 4. We improved ingestion quality. We tuned chunk boundaries. We tested retrieval. We ran questions, inspected outputs, and identified where the system performed well and where it did not.\n",
    "\n",
    "Today we are standing at step 5.\n",
    "\n",
    "But we are not here because someone decided fine-tuning sounded interesting. We are here because the evidence from the previous steps told us that the remaining failures cannot be fixed by better retrieval, better chunking, or better prompts.\n",
    "\n",
    "That is the only legitimate reason to be here.\n",
    "\n",
    ">**Facilitator note**: Pause here and reinforce this framing. If participants skipped Day 2, give them the short version: \"We built a RAG pipeline. It solved most problems. The ones it didn't solve are why we're in this room.\" If anyone asks whether they can skip to the model training section, point them back to this moment.\n",
    ">The escalation ladder is not a suggestion. It is the governing logic of the entire workshop.\n",
    "Say explicitly: \"We earned the right to be at step 5 because we did steps 1 through 4 first.\"\n",
    "\n",
    "## 0.3 The Questions That Still Fail \n",
    "\n",
    "Not all failures are equal. Before we can talk about what to do next, we need to look at what is actually going wrong and sort those failures into categories that tell us something useful.\n",
    "\n",
    "At the end of Day 2, we ran a structured set of questions through the RAG pipeline and captured the results: the question, the model's answer, the retrieved sources, the similarity distances, and the expected answer. Those results were saved so we could pick up exactly where we left off.\n",
    "\n",
    "That is what we are loading now.\n",
    "\n",
    "### 0.3.1 Pre-Built Evaluation Set (From Day 2 Results)\n",
    "\n",
    "The first cell loads the Day 2 evaluation results directly from the saved JSON artifact.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcca941f-46dc-4c26-b36a-704f34f13aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 2 Evaluation Results Loaded\n",
      "============================================================\n",
      "  Source document:   multi-document corpus\n",
      "  Model:             granite-3-2-8b-instruct\n",
      "  Chunking strategy: markdown_aware\n",
      "  Chunk size:        1000\n",
      "  Overlap:           200\n",
      "  Total chunks:      4732\n",
      "  Embedding model:   ibm-granite/granite-embedding-30m-english\n",
      "  Retrieval:         top-3, cosine similarity\n",
      "  Questions:         10\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../prebuilt/eval_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "metadata = eval_data[\"metadata\"]\n",
    "results = eval_data[\"results\"]\n",
    "\n",
    "print(\"Day 2 Evaluation Results Loaded\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Source document:   {metadata['source_document']}\")\n",
    "print(f\"  Model:             {metadata['model']}\")\n",
    "print(f\"  Chunking strategy: {metadata['chunking']['strategy']}\")\n",
    "print(f\"  Chunk size:        {metadata['chunking']['chunk_size']}\")\n",
    "print(f\"  Overlap:           {metadata['chunking']['overlap']}\")\n",
    "print(f\"  Total chunks:      {metadata['chunking']['total_chunks']}\")\n",
    "print(f\"  Embedding model:   {metadata['embedding_model']}\")\n",
    "print(f\"  Retrieval:         top-{metadata['retrieval']['n_results']}, {metadata['retrieval']['similarity']} similarity\")\n",
    "print(f\"  Questions:         {len(results)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12cbbc2-2d25-42cb-82a6-c1ad5c3ef67e",
   "metadata": {},
   "source": [
    "These are the same results you generated on Day 2. Same model. Same chunks. Same retrieval parameters. Nothing has been modified. We are picking up exactly where the previous lab left off.\n",
    "\n",
    "Now let's look at what came back.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016f5621-7c39-4b17-83a5-e2c3fdb4c733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DAY 2 RAG RESULTS\n",
      "======================================================================\n",
      "\n",
      "   [q01] What happens if a Thief fails an Open Locks attempt?\n",
      "   Category:       explicit_rule\n",
      "   Classification: pass\n",
      "   Top distance:   0.1726\n",
      "   Answer preview: If a Thief fails an Open Locks attempt, they must wait until they have gained another level of experience before trying ...\n",
      "   ---\n",
      "\n",
      ">> [q02] Why can't Elves roll higher than a d6 for hit points?\n",
      "   Category:       terminology\n",
      "   Classification: implicit_reasoning_failure\n",
      "   Top distance:   0.2227\n",
      "   Answer preview: According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. The reason for this ...\n",
      "   ---\n",
      "\n",
      "   [q03] Can a character wear leather armor and cast spells?\n",
      "   Category:       implicit_reasoning\n",
      "   Classification: pass\n",
      "   Top distance:   0.1730\n",
      "   Answer preview: Yes, according to the provided context, characters who can cast spells may wear leather armor....\n",
      "   ---\n",
      "\n",
      ">> [q04] What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "   Category:       table_lookup\n",
      "   Classification: implicit_reasoning_failure\n",
      "   Top distance:   0.2017\n",
      "   Answer preview: The context does not provide specific information on the saving throw for a 3rd level Fighter against Dragon Breath. How...\n",
      "   ---\n",
      "\n",
      "   [q05] How does a Cleric turn undead?\n",
      "   Category:       multi_step_rule\n",
      "   Classification: pass\n",
      "   Top distance:   0.1636\n",
      "   Answer preview: To turn undead, the GM looks up the Cleric's level on the Clerics vs. Undead table and cross-references it with the unde...\n",
      "   ---\n",
      "\n",
      ">> [q06] If a character has a Strength of 16, what bonus do they get on melee attack rolls?\n",
      "   Category:       table_lookup\n",
      "   Classification: implicit_reasoning_failure\n",
      "   Top distance:   0.2507\n",
      "   Answer preview: The provided context does not specify how Strength affects melee attack rolls. In Basic Fantasy RPG, it is recommended n...\n",
      "   ---\n",
      "\n",
      ">> [q07] What is the difference between a retainer and a hireling?\n",
      "   Category:       terminology\n",
      "   Classification: implicit_reasoning_failure\n",
      "   Top distance:   0.2082\n",
      "   Answer preview: In the Basic Fantasy RPG system, a retainer and a hireling are both types of NPCs that can be hired by player characters...\n",
      "   ---\n",
      "\n",
      "   [q08] When can a Magic-User learn new spells?\n",
      "   Category:       implicit_reasoning\n",
      "   Classification: pass\n",
      "   Top distance:   0.1195\n",
      "   Answer preview: A Magic-User can learn new spells at any point, but there are specific conditions. They must find a teacher or acquire a...\n",
      "   ---\n",
      "\n",
      "   [q09] What happens to a character at exactly 0 hit points?\n",
      "   Category:       explicit_rule\n",
      "   Classification: pass\n",
      "   Top distance:   0.1636\n",
      "   Answer preview: When a character's hit point total reaches exactly 0, the character may be dead. However, this is not necessarily the en...\n",
      "   ---\n",
      "\n",
      "   [q10] Can a Halfling use a longbow?\n",
      "   Category:       implicit_reasoning\n",
      "   Classification: pass\n",
      "   Top distance:   0.1954\n",
      "   Answer preview: No, according to the provided context, Halflings may not use Large weapons, and longbows are typically around 6 feet uns...\n",
      "   ---\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DAY 2 RAG RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    status = \"PASS\" if r[\"classification\"] == \"pass\" else \"FAIL\"\n",
    "    marker = \"  \" if status == \"PASS\" else \">>\"\n",
    "\n",
    "    print(f\"\\n{marker} [{r['id']}] {r['question']}\")\n",
    "    print(f\"   Category:       {r['category']}\")\n",
    "    print(f\"   Classification: {r['classification']}\")\n",
    "    print(f\"   Top distance:   {r['distances'][0]:.4f}\")\n",
    "    print(f\"   Answer preview: {r['answer'][:120]}...\")\n",
    "    print(f\"   {'---'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a59a06-1028-4134-be45-7d9da9cb088a",
   "metadata": {},
   "source": [
    "Take a moment to scan the output. Some questions passed cleanly. The model found the right chunks, read the context, and produced a grounded answer. \n",
    "Those are the wins from Day 2.\n",
    "\n",
    "\n",
    "Now look at the ones marked with >>. Those are the questions we are here to talk about.\n",
    "\n",
    ">**Facilitator note**: Give the room 30 seconds to read the output. Don't narrate every line. Let the pattern emerge on its own. If someone notices that all four failures share the same classification, call it out. That is exactly the observation this section is designed to produce.\n",
    "\n",
    "### 0.3.2 Sorting the Failures\n",
    "\n",
    "Now we sort. Not every wrong answer has the same cause, and the fix depends entirely on correctly diagnosing the failure mode.\n",
    "\n",
    "Before looking at the code output, here is the framework for thinking about failure categories:\n",
    "\n",
    "**Retrieval failures** occur when the system pulls back the wrong chunks. The model never had a chance because the relevant information was not in the context window. These are infrastructure problems. The fix is upstream: better chunking, better embeddings, better retrieval strategy. Model adaptation will not help here.\n",
    "\n",
    "**Terminology failures** occur when the retrieved context contains the right information but uses domain-specific language that the model maps to the wrong concept. The model sees \"hit die\" and interprets it through the lens of its pre-training rather than through the customer's specific system. The context is present but the model's prior training creates interference.\n",
    "\n",
    "**Implicit reasoning failures** occur when the answer requires combining information from multiple rules, applying a rule that is stated indirectly, or making an inference that the document assumes the reader already understands. The context is retrieved. The terminology is not the issue. But the model fails to connect the pieces, hedges when it should commit, or says the information is not present when it actually is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dee6a9f-43cc-455f-8b37-9cc828400509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAILURE ANALYSIS BY CLASSIFICATION\n",
      "======================================================================\n",
      "\n",
      "  Pass: 6 question(s)\n",
      "      [q01] [explicit_rule] What happens if a Thief fails an Open Locks attempt?\n",
      "      [q03] [implicit_reasoning] Can a character wear leather armor and cast spells?\n",
      "      [q05] [multi_step_rule] How does a Cleric turn undead?\n",
      "      [q08] [implicit_reasoning] When can a Magic-User learn new spells?\n",
      "      [q09] [explicit_rule] What happens to a character at exactly 0 hit points?\n",
      "      [q10] [implicit_reasoning] Can a Halfling use a longbow?\n",
      "\n",
      "  Retrieval Failure: 0 question(s)\n",
      "\n",
      "  Terminology Failure: 0 question(s)\n",
      "\n",
      "  Implicit Reasoning Failure: 4 question(s)\n",
      "      [q02] [terminology] Why can't Elves roll higher than a d6 for hit points?\n",
      "      [q04] [table_lookup] What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "      [q06] [table_lookup] If a character has a Strength of 16, what bonus do they get on melee attack rolls?\n",
      "      [q07] [terminology] What is the difference between a retainer and a hireling?\n"
     ]
    }
   ],
   "source": [
    "categories = {}\n",
    "for r in results:\n",
    "    c = r[\"classification\"]\n",
    "    if c not in categories:\n",
    "        categories[c] = []\n",
    "    categories[c].append(r)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FAILURE ANALYSIS BY CLASSIFICATION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for cat in [\"pass\", \"retrieval_failure\", \"terminology_failure\", \"implicit_reasoning_failure\"]:\n",
    "    items = categories.get(cat, [])\n",
    "    label = cat.replace(\"_\", \" \").title()\n",
    "    print(f\"\\n  {label}: {len(items)} question(s)\")\n",
    "    for item in items:\n",
    "        print(f\"      [{item['id']}] [{item['category']}] {item['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b562fee-cc75-4954-9fe4-31ed7b119735",
   "metadata": {},
   "source": [
    "Now let's look at the failures in detail. For each one, we inspect what the model actually said, what it should have said, and what sources it was working from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cebb59-6a91-4c93-856a-8994af49b0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DETAILED FAILURE INSPECTION (4 failures)\n",
      "======================================================================\n",
      "\n",
      "--- Failure 1: q02 ---\n",
      "Classification: IMPLICIT REASONING FAILURE\n",
      "Category:       terminology\n",
      "Question:       Why can't Elves roll higher than a d6 for hit points?\n",
      "Expected:       Elves use a d6 for hit points because that is the hit die assigned to the Elf combination class in Basic Fantasy RPG.\n",
      "Got:            According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. The reason for this restriction is not explicitly stated in the context.\n",
      "Sources:        ['Basic-Fantasy-RPG-Rules-r142.pdf', 'Basic-Fantasy-RPG-Rules-r142.pdf', 'Basic-Fantasy-RPG-Rules-r107-bookmarked.pdf']\n",
      "Distances:      ['0.2227', '0.2454', '0.2882']\n",
      "\n",
      "\n",
      "--- Failure 2: q04 ---\n",
      "Classification: IMPLICIT REASONING FAILURE\n",
      "Category:       table_lookup\n",
      "Question:       What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "Expected:       Based on the Fighter saving throw table, a 3rd level Fighter has a Dragon Breath saving throw of 15.\n",
      "Got:            The context does not provide specific information on the saving throw for a 3rd level Fighter against Dragon Breath. However, it does mention that characters may make a save vs. Dragon Breath for half\n",
      "Sources:        ['Basic-Fantasy-RPG-Rules-r107-bookmarked.pdf', 'Basic-Fantasy-RPG-Rules-r107-lite.pdf', 'Basic-Fantasy-RPG-Rules-r107.pdf']\n",
      "Distances:      ['0.2017', '0.2017', '0.2017']\n",
      "\n",
      "\n",
      "--- Failure 3: q06 ---\n",
      "Classification: IMPLICIT REASONING FAILURE\n",
      "Category:       table_lookup\n",
      "Question:       If a character has a Strength of 16, what bonus do they get on melee attack rolls?\n",
      "Expected:       A Strength score of 16 gives a +2 bonus, which applies to melee attack rolls and damage rolls.\n",
      "Got:            The provided context does not specify how Strength affects melee attack rolls. In Basic Fantasy RPG, it is recommended not to waste time in detailing the ability score or other statistics of character\n",
      "Sources:        ['Basic-Fantasy-RPG-Rules-r107-bookmarked.pdf', 'Basic-Fantasy-RPG-Rules-r107-lite.pdf', 'Basic-Fantasy-RPG-Rules-r107.pdf']\n",
      "Distances:      ['0.2507', '0.2507', '0.2507']\n",
      "\n",
      "\n",
      "--- Failure 4: q07 ---\n",
      "Classification: IMPLICIT REASONING FAILURE\n",
      "Category:       terminology\n",
      "Question:       What is the difference between a retainer and a hireling?\n",
      "Expected:       Retainers are NPCs who accompany the party on adventures and gain experience. Hirelings are hired for specific non-adventuring tasks.\n",
      "Got:            In the Basic Fantasy RPG system, a retainer and a hireling are both types of NPCs that can be hired by player characters. However, the key difference lies in their loyalty and willingness to take risk\n",
      "Sources:        ['Basic-Fantasy-RPG-Rules-r107-bookmarked.pdf', 'Basic-Fantasy-RPG-Rules-r107-lite.pdf', 'Basic-Fantasy-RPG-Rules-r107.pdf']\n",
      "Distances:      ['0.2082', '0.2082', '0.2082']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failures = [r for r in results if r[\"classification\"] != \"pass\"]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DETAILED FAILURE INSPECTION ({len(failures)} failures)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for i, r in enumerate(failures):\n",
    "    print(f\"\\n--- Failure {i+1}: {r['id']} ---\")\n",
    "    print(f\"Classification: {r['classification'].replace('_', ' ').upper()}\")\n",
    "    print(f\"Category:       {r['category']}\")\n",
    "    print(f\"Question:       {r['question']}\")\n",
    "    print(f\"Expected:       {r['expected']}\")\n",
    "    print(f\"Got:            {r['answer'][:200]}\")\n",
    "    print(f\"Sources:        {r['sources']}\")\n",
    "    print(f\"Distances:      {[f'{d:.4f}' for d in r['distances']]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc44868-19f8-4522-8ec2-69781ed37a17",
   "metadata": {},
   "source": [
    ">**Facilitator note**: Walk through the failures one at a time with the room. For each, ask two questions:\n",
    "\"Did the retriever find relevant content?\" Check the distances. Anything under roughly 0.30 means retrieval was in the right neighborhood.\n",
    "\"If the model had relevant context, why did it get the answer wrong?\"\n",
    "\n",
    "\n",
    "Look at the specifics:\n",
    "\n",
    "* **q02** (Elf hit dice): The model retrieved relevant chunks (distances around 0.22 to 0.29) but said \"the reason is not explicitly stated.\" The answer is in the rules, but it requires understanding that Elves are a combination class and that the d6 is tied to that class structure. The model hedged instead of reasoning through it.\n",
    "\n",
    "* **q04** (Fighter saving throw, Dragon Breath): Distances around 0.20, so retrieval was reasonable. But the model could not extract the specific value from a saving throw table. It acknowledged the concept exists but could not produce the number 15.\n",
    "\n",
    "* **q06** (Strength 16 melee bonus): Same pattern. The ability score bonus table was likely in the chunks, but the model could not perform the lookup. It said the context \"does not specify\" when the table was right there.\n",
    "\n",
    "* **q07** (Retainer vs. hireling): The model gave a detailed answer about retainers but admitted it could not find specific details about hirelings. Partial retrieval, partial reasoning, incomplete answer.\n",
    "\n",
    "Two of these failures involve table lookups. Two involve reasoning across implicit rules. All four share one trait: the model received relevant context and still could not produce the right answer.\n",
    "\n",
    "These are not retrieval problems. These are model problems.\n",
    "\n",
    "## 0.4 What the Evidence Shows\n",
    "\n",
    "Let's make the summary explicit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a669fca-8285-489e-8c35-8da232d93e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVIDENCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "  Total questions evaluated:     10\n",
      "  Passed:                        6 (60%)\n",
      "  Retrieval failures:            0 (0%)\n",
      "  Terminology failures:          0 (0%)\n",
      "  Implicit reasoning failures:   4 (40%)\n",
      "\n",
      "======================================================================\n",
      "DIAGNOSIS\n",
      "======================================================================\n",
      "\n",
      "  Implicit reasoning failures present (4).\n",
      "  The model receives relevant context but cannot connect the pieces.\n",
      "  This is the strongest signal that model adaptation may be warranted.\n",
      "\n",
      "  CONCLUSION: 4 failure(s) fall outside what\n",
      "  retrieval improvements alone can fix. This is the evidence that\n",
      "  justifies exploring model adaptation today.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total = len(results)\n",
    "passed = len(categories.get(\"pass\", []))\n",
    "retrieval = len(categories.get(\"retrieval_failure\", []))\n",
    "terminology = len(categories.get(\"terminology_failure\", []))\n",
    "implicit = len(categories.get(\"implicit_reasoning_failure\", []))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVIDENCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n  Total questions evaluated:     {total}\")\n",
    "print(f\"  Passed:                        {passed} ({passed/total*100:.0f}%)\")\n",
    "print(f\"  Retrieval failures:            {retrieval} ({retrieval/total*100:.0f}%)\")\n",
    "print(f\"  Terminology failures:          {terminology} ({terminology/total*100:.0f}%)\")\n",
    "print(f\"  Implicit reasoning failures:   {implicit} ({implicit/total*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DIAGNOSIS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if retrieval > 0:\n",
    "    print(f\"\\n  Retrieval failures present ({retrieval}).\")\n",
    "    print(f\"  These should be addressed with chunking/embedding improvements,\")\n",
    "    print(f\"  NOT model adaptation. Fix the pipeline first.\")\n",
    "\n",
    "if terminology > 0:\n",
    "    print(f\"\\n  Terminology failures present ({terminology}).\")\n",
    "    print(f\"  The model confuses domain-specific language with general knowledge.\")\n",
    "    print(f\"  This is a candidate for model adaptation.\")\n",
    "\n",
    "if implicit > 0:\n",
    "    print(f\"\\n  Implicit reasoning failures present ({implicit}).\")\n",
    "    print(f\"  The model receives relevant context but cannot connect the pieces.\")\n",
    "    print(f\"  This is the strongest signal that model adaptation may be warranted.\")\n",
    "\n",
    "model_failures = terminology + implicit\n",
    "if model_failures > 0:\n",
    "    print(f\"\\n  CONCLUSION: {model_failures} failure(s) fall outside what\")\n",
    "    print(f\"  retrieval improvements alone can fix. This is the evidence that\")\n",
    "    print(f\"  justifies exploring model adaptation today.\")\n",
    "else:\n",
    "    print(f\"\\n  All failures appear to be retrieval-related.\")\n",
    "    print(f\"  Model adaptation is NOT yet justified. Fix retrieval first.\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cffd90e-f19a-4cad-b459-39525c3318fa",
   "metadata": {},
   "source": [
    "This is the moment to be explicit about what we just did and why it matters.\n",
    "\n",
    "We did not wake up this morning and decide to fine-tune a model. We loaded the evaluation results from Day 2. We categorized the failures. We identified which failures are retrieval problems (none, in this case) and which are model problems (all four). The retrieval was reasonable. The distances were low. The sources were relevant. The model simply could not do the work.\n",
    "\n",
    "That is the evidence. That is why we are here.\n",
    "\n",
    "\n",
    "## 0.5 What This Session Is\n",
    "\n",
    "This session is:\n",
    "* A continuation of the pipeline you built yesterday\n",
    "* Focused on diagnosing failures and preparing data for model adaptation\n",
    "* Designed to teach you when and how synthetic data generation supports that process\n",
    "\n",
    "This session is not:\n",
    "\n",
    "* Starting over\n",
    "* Introducing a new architecture\n",
    "* Treating fine-tuning as the default answer\n",
    "\n",
    "## 0.6 What Success Looks Like Today\n",
    "At the end of Day 3, participants should be able to:\n",
    "* Trace a specific eval failure to its root cause\n",
    "* Explain why retrieval alone cannot fix it\n",
    "* Articulate the role of synthetic data generation as preparation, not shortcut\n",
    "* Describe the conditions under which model adaptation is justified\n",
    "\n",
    "What success is not:\n",
    "\n",
    "* 10/10 on the eval\n",
    "* A trained model\n",
    "* A production deployment\n",
    "\n",
    "The eval is a diagnostic tool, not a scoreboard.\n",
    "\n",
    "## 0.7 How the Lab Will Run\n",
    "Same rules as yesterday:\n",
    "Guided Jupyter notebook, code and explanation interleaved\n",
    "Pre-generated outputs available for every major step\n",
    "Participants are not expected to write code from scratch\n",
    "If a cell takes longer than 2 to 3 minutes, move forward using pre-built results\n",
    "\n",
    ">Facilitator guidance:\n",
    ">* Anchor every discussion on the eval results\n",
    ">* When something improves, ask \"why did it improve?\"\n",
    ">* When something does not improve, ask \"what layer is responsible?\"\n",
    ">* Resist the urge to skip ahead to model training\n",
    "## 0.8 Setting the Tone\n",
    "\"Yesterday you built a system that works 60% of the time. Today you figure out why the other 40% fails, and whether the fix lives in the data or in the model. That is how real engineering works. That is what customers trust.\"\n",
    "\n",
    "**Transition to Section 1:**\n",
    "\n",
    "\"Now that we know what the system cannot do and why, we can talk about what synthetic data generation actually is, what it is not, and how to use it responsibly to prepare for model adaptation.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}