{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd566e5c-ff5b-4573-b44f-c82b437928e1",
   "metadata": {},
   "source": [
    "# 1. Inference-Time Scaling: Pushing the Model Before Changing It\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Before committing to model adaptation, test whether spending more compute at inference time can close the gap. This is the last and most rigorous check before escalating to training. If inference-time scaling solves the problem, we avoid the cost and complexity of fine-tuning entirely. If it does not, we have even stronger evidence that the model's weights need to change.\n",
    "\n",
    "\n",
    "## 1.1 The Idea: Spend Compute at Inference, Not Training\n",
    "\n",
    "There is a common assumption in AI engagements that when a model gives a wrong answer, the model needs to be retrained. Sometimes that is true. But retraining is expensive, slow, and organizationally complex. Before going there, it is worth asking a simpler question:\n",
    "\n",
    "What if we just let the model try harder?\n",
    "\n",
    "That is the core idea behind inference-time scaling. Instead of changing the model's weights (which is what fine-tuning does), you change how much work the model does at inference time. You let it generate multiple candidate answers, evaluate them, and select the best one. The model itself is unchanged. You are spending compute instead of training data.\n",
    "\n",
    "This matters for two reasons.\n",
    "\n",
    "First, it is cheaper and faster to try. There is no training pipeline to build, no data to curate, no model to validate. You are using the same model, the same endpoint, the same API key. The only thing that changes is how many times you call it and how you pick the winner.\n",
    "\n",
    "Second, the results are diagnostic. If the model produces the right answer on attempt 3 out of 5, that tells you something important: the knowledge is accessible but the model's default sampling path does not reliably surface it. That is a very different problem than \"the model fundamentally cannot do this.\" And the fix might be as simple as a better decoding strategy rather than a full training run.\n",
    "\n",
    "If the model cannot produce the right answer in N attempts with good context, that is also diagnostic. It means the failure is not a sampling issue. The model genuinely does not know how to handle this class of question. That is the strongest possible evidence for model adaptation.\n",
    "\n",
    "Either way, you learn something. And learning before spending is the entire philosophy of this workshop.\n",
    "\n",
    "## 1.2 Introducing `its_hub`\n",
    "\n",
    "``its_hub`` is an inference-time scaling library built by the Red Hat AI Innovation Team. It implements several algorithms for improving model output quality without modifying model weights, and it works with any OpenAI-compatible API endpoint. That means it works with our MaaS setup without any infrastructure changes.\n",
    "\n",
    "The library has three layers:\n",
    "\n",
    "A **language model wrapper** (`OpenAICompatibleLanguageModel`) that handles generation against any OpenAI-compatible endpoint, including batched and async calls.\n",
    "\n",
    "A **reward model** that scores candidate responses. For Best-of-N, this is ``LLMJudgeRewardModel``, which uses a separate LLM to evaluate response quality. For more advanced algorithms like Particle Filtering, it can be a local Process Reward Model that scores each reasoning step.\n",
    "\n",
    "A **scaling algorithm** that orchestrates the generation and scoring. `BestOfN` generates N candidates and picks the best. `SelfConsistency` generates N candidates and picks the most common answer. `ParticleFiltering` and `BeamSearch` operate at the step level, pruning weak reasoning paths as they go.\n",
    "\n",
    "For this lab, we will use **Best-of-N with LLM Judge**. The idea is straightforward:\n",
    "\n",
    "1. Send the same prompt to the model N times (the \"budget\")\n",
    "2. Collect N candidate responses\n",
    "3. Use a separate LLM judge to evaluate and rank the candidates\n",
    "4. Return the highest-scoring response\n",
    "\n",
    "The generation model is not retrained. It is not prompted differently. It simply gets multiple chances, and a judge picks the best attempt.\n",
    "\n",
    "### 1.2.1 Environment Setup\n",
    "\n",
    "Before we touch any code, we need two values: an API key and an endpoint URL for the MaaS (Model as a Service) platform. If you attended Day 2, you already have these. If you are starting fresh today, follow the setup steps below. If you already have a working `.env` file from Day 2, skip ahead to the verification cell.\n",
    "\n",
    "**For participants who did not attend Day 2:**\n",
    "\n",
    "You need access to the Red Hat Demo Platform MaaS service. Using your Red Hat SSO credentials, log in at:\n",
    "\n",
    "> https://litellm-prod-frontend.apps.maas.redhatworkshops.io/home\n",
    "\n",
    "Once logged in:\n",
    "\n",
    "1. Click Subscriptions in the left sidebar, then New Subscription. Subscribe to the following models: granite-3-2-8b-instruct, granite-4-0-h-tiny, microsoft-phi-4, and qwen3-14b. For each one, click the model name to open the model card, then click Subscribe.\n",
    "2. Click API Keys in the left sidebar, then Create API Key. Give it a name (anything you like), check Select All to attach all your subscribed models, and click Create API Key.\n",
    "3. When the key appears, click the copy button and save it somewhere. You will not be able to see the full key again after closing the dialog.\n",
    "4. Before closing the dialog, note the API URL displayed on the key details screen. It will look like `https://litellm-prod.apps.maas.redhatworkshops.io/v1`. Copy this as well.\n",
    "\n",
    "Now create a file named `.env` in the parent of this folder. The file should contain exactly two lines:\n",
    "\n",
    "```\n",
    "API_KEY=your-api-key-here\n",
    "ENDPOINT_BASE=endpoint-here\n",
    "```\n",
    "\n",
    "**For everyone:**\n",
    "\n",
    "Replace your-api-key-here with the key you copied and endpoint-here with the correct endpoint\n",
    "\n",
    "\n",
    "No quotes around the values.  \n",
    "\n",
    "No spaces around the equals sign.\n",
    "If you are unsure where to create the file, run this in a notebook cell to confirm the expected path:\n",
    "\n",
    "\n",
    "The lab uses a small config helper that reads the `.env` file. \n",
    "\n",
    "Run the following cell to verify that everything loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18b1822-df94-40e8-b42e-ea0b873f35f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.py expects .env at: /opt/app-root/src/Q1ETXDay3/.env\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "expected = Path(os.getcwd()).parent / \".env\"\n",
    "print(f\"config.py expects .env at: {expected}\")\n",
    "print(f\"File exists: {expected.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3615a81-23ae-4869-a782-42ee35c9589e",
   "metadata": {},
   "source": [
    "If it shows `File exists: False`, create the file at the printed path and re-run the config cell.\n",
    "\n",
    "The lab uses a small config helper that reads the `.env` file. Run the following cell to verify that everything loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aaa3e2d-8176-4698-8353-33328549ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: https://litellm-prod.apps.maas.redhatworkshops.io/v1\n",
      "API Key:  sk-UFHcL...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from config import API_KEY as key, ENDPOINT_BASE as endpoint_base\n",
    "\n",
    "print(f\"Endpoint: {endpoint_base}\")\n",
    "print(f\"API Key:  {key[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acaed1b-30c3-4aab-b038-fa88d0ec0651",
   "metadata": {},
   "source": [
    "The output will show your endpoint URL in full and the first 8 characters of your API key followed by an ellipsis. \n",
    "\n",
    "The key is deliberately truncated so it is not exposed in notebook output, screenshots, or screen shares. \n",
    "\n",
    "If either value is blank or shows `None`, check that the `.env` file exists in the Day3 directory and that the variable names match exactly: `API_KEY` and `ENDPOINT_BASE`.\n",
    "\n",
    "### 1.2.2 Install and Verify Dependencies\n",
    "\n",
    "The `its_hub` library is pre-installed in the lab environment along with most of its dependencies. Run the following cell to confirm the installation and install `nest_asyncio`, which is needed for running async code inside Jupyter notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a976069-9b99-4059-9fd7-7ea24eccd2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting its_hub\n",
      "  Downloading its_hub-0.3.5-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: nest_asyncio in /opt/app-root/lib64/python3.12/site-packages (1.6.0)\n",
      "Collecting openai>=1.68.2 (from its_hub)\n",
      "  Downloading openai-2.21.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/app-root/lib64/python3.12/site-packages (from its_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/app-root/lib64/python3.12/site-packages (from its_hub) (4.15.0)\n",
      "Collecting reward-hub>=0.1.8 (from its_hub)\n",
      "  Downloading reward_hub-0.1.9-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting transformers>=4.53.2 (from its_hub)\n",
      "  Downloading transformers-5.2.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting backoff>=2.2.0 (from its_hub)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: click>=8.1.0 in /opt/app-root/lib64/python3.12/site-packages (from its_hub) (8.1.8)\n",
      "Requirement already satisfied: fastapi>=0.115.5 in /opt/app-root/lib64/python3.12/site-packages (from its_hub) (0.128.0)\n",
      "Requirement already satisfied: pydantic>=2.7.2 in /opt/app-root/lib64/python3.12/site-packages (from its_hub) (2.10.6)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib64/python3.12/site-packages (from its_hub) (2.3.5)\n",
      "Requirement already satisfied: uvicorn in /opt/app-root/lib64/python3.12/site-packages (from its_hub) (0.34.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.12/site-packages (from its_hub) (2.32.5)\n",
      "Requirement already satisfied: aiohttp>=3.9.0 in /opt/app-root/lib64/python3.12/site-packages (from its_hub) (3.13.3)\n",
      "Collecting litellm<1.75.0,>=1.70.0 (from its_hub)\n",
      "  Downloading litellm-1.74.15.post2.tar.gz (9.7 MB)\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m173.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpx>=0.23.0 in /opt/app-root/lib64/python3.12/site-packages (from litellm<1.75.0,>=1.70.0->its_hub) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /opt/app-root/lib64/python3.12/site-packages (from litellm<1.75.0,>=1.70.0->its_hub) (8.7.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/app-root/lib64/python3.12/site-packages (from litellm<1.75.0,>=1.70.0->its_hub) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /opt/app-root/lib64/python3.12/site-packages (from litellm<1.75.0,>=1.70.0->its_hub) (4.26.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /opt/app-root/lib64/python3.12/site-packages (from litellm<1.75.0,>=1.70.0->its_hub) (1.2.1)\n",
      "Collecting tiktoken>=0.7.0 (from litellm<1.75.0,>=1.70.0->its_hub)\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tokenizers (from litellm<1.75.0,>=1.70.0->its_hub)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm<1.75.0,>=1.70.0->its_hub) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/app-root/lib64/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<1.75.0,>=1.70.0->its_hub) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/app-root/lib64/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<1.75.0,>=1.70.0->its_hub) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/app-root/lib64/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<1.75.0,>=1.70.0->its_hub) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /opt/app-root/lib64/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<1.75.0,>=1.70.0->its_hub) (0.30.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/app-root/lib64/python3.12/site-packages (from pydantic>=2.7.2->its_hub) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/app-root/lib64/python3.12/site-packages (from pydantic>=2.7.2->its_hub) (2.27.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/app-root/lib64/python3.12/site-packages (from aiohttp>=3.9.0->its_hub) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/app-root/lib64/python3.12/site-packages (from aiohttp>=3.9.0->its_hub) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib64/python3.12/site-packages (from aiohttp>=3.9.0->its_hub) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib64/python3.12/site-packages (from aiohttp>=3.9.0->its_hub) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/app-root/lib64/python3.12/site-packages (from aiohttp>=3.9.0->its_hub) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/app-root/lib64/python3.12/site-packages (from aiohttp>=3.9.0->its_hub) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/app-root/lib64/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.9.0->its_hub) (3.11)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /opt/app-root/lib64/python3.12/site-packages (from fastapi>=0.115.5->its_hub) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/app-root/lib64/python3.12/site-packages (from fastapi>=0.115.5->its_hub) (0.0.4)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/app-root/lib64/python3.12/site-packages (from starlette<0.51.0,>=0.40.0->fastapi>=0.115.5->its_hub) (4.12.1)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.12/site-packages (from httpx>=0.23.0->litellm<1.75.0,>=1.70.0->its_hub) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.12/site-packages (from httpx>=0.23.0->litellm<1.75.0,>=1.70.0->its_hub) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/app-root/lib64/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm<1.75.0,>=1.70.0->its_hub) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/app-root/lib64/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm<1.75.0,>=1.70.0->its_hub) (3.23.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.68.2->its_hub)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai>=1.68.2->its_hub)\n",
      "  Downloading jiter-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.68.2->its_hub)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: torch>=2.7.0 in /opt/app-root/lib64/python3.12/site-packages (from reward-hub>=0.1.8->its_hub) (2.7.1+cu128)\n",
      "Requirement already satisfied: tenacity>=8.0.0 in /opt/app-root/lib64/python3.12/site-packages (from reward-hub>=0.1.8->its_hub) (8.5.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken>=0.7.0->litellm<1.75.0,>=1.70.0->its_hub)\n",
      "  Downloading regex-2026.2.19-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/app-root/lib64/python3.12/site-packages (from requests->its_hub) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.12/site-packages (from requests->its_hub) (2.6.3)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (3.6.1)\n",
      "Requirement already satisfied: fsspec in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (2026.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/app-root/lib64/python3.12/site-packages (from torch>=2.7.0->reward-hub>=0.1.8->its_hub) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/app-root/lib64/python3.12/site-packages (from sympy>=1.13.3->torch>=2.7.0->reward-hub>=0.1.8->its_hub) (1.3.0)\n",
      "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers>=4.53.2->its_hub)\n",
      "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib64/python3.12/site-packages (from transformers>=4.53.2->its_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.12/site-packages (from transformers>=4.53.2->its_hub) (6.0.3)\n",
      "Collecting typer-slim (from transformers>=4.53.2->its_hub)\n",
      "  Downloading typer_slim-0.24.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.53.2->its_hub)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=1.3.0->transformers>=4.53.2->its_hub)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=1.3.0->transformers>=4.53.2->its_hub)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer>=0.24.0 (from typer-slim->transformers>=4.53.2->its_hub)\n",
      "  Downloading typer-0.24.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting click>=8.1.0 (from its_hub)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: rich>=12.3.0 in /opt/app-root/lib64/python3.12/site-packages (from typer>=0.24.0->typer-slim->transformers>=4.53.2->its_hub) (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/app-root/lib64/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers>=4.53.2->its_hub) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers>=4.53.2->its_hub) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/app-root/lib64/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers>=4.53.2->its_hub) (0.1.2)\n",
      "Downloading its_hub-0.3.5-py3-none-any.whl (46 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading openai-2.21.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m688.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n",
      "Downloading reward_hub-0.1.9-py2.py3-none-any.whl (31 kB)\n",
      "Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m686.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2026.2.19-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m803.7/803.7 kB\u001b[0m \u001b[31m689.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-5.2.0-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m220.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m623.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m186.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m287.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading typer_slim-0.24.0-py3-none-any.whl (3.4 kB)\n",
      "Downloading typer-0.24.0-py3-none-any.whl (56 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Building wheels for collected packages: litellm\n",
      "  Building wheel for litellm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for litellm: filename=litellm-1.74.15.post2-py3-none-any.whl size=8838867 sha256=78ad4ae0b1cb8df73cb243191f4977211ad7a4476e49e0f204ee0481bd236ac7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-kv4a6bq4/wheels/8f/38/bc/e863a58966fe126384a6c606ec555b408e1b78da949ee70667\n",
      "Successfully built litellm\n",
      "Installing collected packages: sniffio, shellingham, safetensors, regex, jiter, hf-xet, distro, click, backoff, tiktoken, typer, openai, typer-slim, huggingface-hub, tokenizers, transformers, litellm, reward-hub, its_hub\n",
      "\u001b[2K  Attempting uninstall: click\n",
      "\u001b[2K    Found existing installation: click 8.1.8\n",
      "\u001b[2K    Uninstalling click-8.1.8:\n",
      "\u001b[2K      Successfully uninstalled click-8.1.8\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 7/19\u001b[0m [click]\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/19\u001b[0m [its_hub]8/19\u001b[0m [its_hub]mers]ub]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.14.6 requires click==8.1.8, but you have click 8.3.1 which is incompatible.\n",
      "odh-elyra 4.2.4 requires click==8.1.8, but you have click 8.3.1 which is incompatible.\n",
      "ray 2.52.1 requires click!=8.3.*,>=7.0, but you have click 8.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 click-8.3.1 distro-1.9.0 hf-xet-1.2.0 huggingface-hub-1.4.1 its_hub-0.3.5 jiter-0.13.0 litellm-1.74.15.post2 openai-2.21.0 regex-2026.2.19 reward-hub-0.1.9 safetensors-0.7.0 shellingham-1.5.4 sniffio-1.3.1 tiktoken-0.12.0 tokenizers-0.22.2 transformers-5.2.0 typer-0.24.0 typer-slim-0.24.0\n"
     ]
    }
   ],
   "source": [
    "! pip install its_hub nest_asyncio -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ddc6e-9966-4b2f-8279-802b258f84e9",
   "metadata": {},
   "source": [
    "You will likely see \"Requirement already satisfied\" for most packages. That is expected. The output confirms what is present and at what version. If any package fails to install or reports a version conflict, notify the instructor.\n",
    "\n",
    "\n",
    "A quick note on why `nest_asyncio` is necessary. Under the hood, `its_hub` uses Python's `asyncio` to send generation requests in parallel. That is how Best-of-N can fire off 5 candidate requests concurrently rather than sequentially. The problem is that Jupyter itself already runs an event loop, and Python's default behavior is to reject a second event loop inside an existing one. `nest_asyncio` patches that restriction so the two can coexist. \n",
    "\n",
    "Without it, every call to `scaling_alg.infer()` would raise a `RuntimeError: This event loop is already running` exception. This is not specific to `its_hub`. Any async library used inside a Jupyter notebook has the same issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4c58fa-c454-4de3-b4a6-f5ff7337053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9ab78-a316-48ac-8bb4-d5c29d1a0971",
   "metadata": {},
   "source": [
    "No output from that cell means it worked. If it throws an `ImportError`, the `pip install` cell above did not complete successfully.\n",
    "\n",
    "### 1.2.3 Connecting the Generator and Judge to MaaS\n",
    "\n",
    "No new infrastructure. No new credentials. We are using the same MaaS endpoint and the same API key from the previous cell.\n",
    "\n",
    "We need two model connections: one for generating answers and one for judging them. The generator is `granite-3-2-8b-instruct`, the same model we have been using throughout. The judge will be `qwen3-14b`, the largest model available on our MaaS endpoint. Using a separate, larger model for judgment matters because a model is not always the best evaluator of its own output. A bigger model with broader training can often spot weaknesses that the smaller model cannot recognize in its own responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a376d9-e931-4d55-a58a-dcca061f2785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLLM is not installed, skipping VLLM support. To enable VLLM support for Qwen/Qwen2.5-Math-PRM-7B, install vllm with `pip install reward_hub[vllm]`\n",
      "VLLM is not installed, skipping VLLM support. To enable VLLM support, install vllm with `pip install reward_hub[vllm]`\n",
      "Generator: granite-3-2-8b-instruct\n",
      "Judge:     qwen3-14b (groupwise, overall_quality)\n",
      "Algorithm: Best-of-N\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "from its_hub.lms import OpenAICompatibleLanguageModel\n",
    "from its_hub.algorithms import BestOfN\n",
    "from its_hub.integration.reward_hub import LLMJudgeRewardModel\n",
    "\n",
    "# The generation model: same as Day 2\n",
    "lm = OpenAICompatibleLanguageModel(\n",
    "    endpoint=endpoint_base,\n",
    "    api_key=key,\n",
    "    model_name=\"granite-3-2-8b-instruct\",\n",
    "    temperature=0.7,  # Some variation across candidates\n",
    ")\n",
    "\n",
    "# The judge: a separate, larger model for evaluating candidates\n",
    "# Note: LLMJudgeRewardModel uses litellm internally, which requires\n",
    "# the \"openai/\" prefix to route to an OpenAI-compatible endpoint.\n",
    "judge = LLMJudgeRewardModel(\n",
    "    model=\"openai/qwen3-14b\",\n",
    "    criterion=\"overall_quality\",\n",
    "    judge_type=\"groupwise\",\n",
    "    api_key=key,\n",
    "    base_url=endpoint_base,\n",
    ")\n",
    "\n",
    "# Wire them together\n",
    "scaling_alg = BestOfN(judge)\n",
    "\n",
    "print(\"Generator: granite-3-2-8b-instruct\")\n",
    "print(\"Judge:     qwen3-14b (groupwise, overall_quality)\")\n",
    "print(\"Algorithm: Best-of-N\")\n",
    "print(\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a69da-2a41-47d7-a525-04102c08149b",
   "metadata": {},
   "source": [
    "Let's walk through what each piece does.\n",
    "\n",
    "`OpenAICompatibleLanguageModel` wraps any OpenAI-compatible API endpoint. It handles batched requests, retries, and concurrency internally. The `temperature=0.7` is important here. In Day 2 we used `temperature=0` for deterministic single-shot answers. Now we want variation. If every candidate is identical, there is nothing for the judge to choose between. A temperature of 0.7 gives the model enough freedom to explore different phrasings and reasoning paths while staying coherent.\n",
    "\n",
    "`LLMJudgeRewardModel` uses a second LLM to evaluate and rank candidate responses. The `criterion=\"overall_quality\"` tells the judge to evaluate on general quality rather than a narrow metric. The `judge_type=\"groupwise\"` means the judge sees all candidates at once and picks a winner, rather than scoring each one independently. This tends to produce better selections because the judge can compare directly.\n",
    "\n",
    "Two details on the judge configuration deserve attention.\n",
    "\n",
    "First, the `base_url` parameter. Without it, `LLMJudgeRewardModel` defaults to calling OpenAI's API. Setting base_url=endpoint_base routes the judge's calls through our MaaS endpoint instead. This is a common gotcha when using LLM-as-judge with custom serving infrastructure.\n",
    "\n",
    "Second, the `openai/` prefix on the model name. The `LLMJudgeRewardModel` uses `litellm` internally for its API calls, and litellm uses that prefix to determine which provider protocol to speak. Without it, litellm does not know that MaaS is an OpenAI-compatible endpoint and the call fails with a \"LLM Provider NOT provided\" error. The generator does not need this prefix because `OpenAICompatibleLanguageModel` makes direct HTTP requests to the endpoint, bypassing litellm entirely. This is a subtle but important difference between the two components. If you hit a provider routing error on the judge but the generator works fine, check the prefix first.\n",
    "\n",
    "`BestOfN` is the algorithm that ties them together. It takes the judge at construction time. When we call `scaling_alg.infer()`, it will generate N candidates using the language model, pass all of them to the judge, and return the one the judge ranked highest.\n",
    "\n",
    "If the cell runs without error, you should see the three confirmation lines printed. If you get a connection error, verify that your endpoint and API key are correct and that both `granite-3-2-8b-instruct` and `qwen3-14b` are in your MaaS subscriptions.\n",
    "\n",
    "## 1.3 Best-of-N with LLM Judge\n",
    "\n",
    "Now we apply this to the questions from Day 2. We will run all 10 questions through the Best-of-N pipeline so we can observe two things: whether the passing questions still pass (no regression), and whether the failing questions improve.\n",
    "\n",
    "\n",
    "### 1.3.1 Running the Target Questions Through Best-of-N\n",
    "\n",
    "First, we load the Day 2 results. This version of the evaluation data includes the retrieved context for each question so we can reconstruct the exact RAG prompts from Day 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a5d3651-46ae-4128-8ccf-bc8a8e9aa343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 questions from Day 2 evaluation\n",
      "Passes:   6\n",
      "Failures: 4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../prebuilt/eval_with_context.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "results_day2 = eval_data[\"results\"]\n",
    "\n",
    "print(f\"Loaded {len(results_day2)} questions from Day 2 evaluation\")\n",
    "print(f\"Passes:   {sum(1 for r in results_day2 if r['classification'] == 'pass')}\")\n",
    "print(f\"Failures: {sum(1 for r in results_day2 if r['classification'] != 'pass')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2ca37-9ce6-4450-aacc-5a59e2888938",
   "metadata": {},
   "source": [
    "Let's walk through the structure of this loop before running it.\n",
    "\n",
    "For each of the 10 questions, we build a prompt that includes the system instruction, the retrieved context from Day 2, and the question itself. This is the same prompt structure the model saw during the Day 2 evaluation. The only difference is what happens after the prompt is sent. Instead of a single generation call, `scaling_alg.infer()` sends 5 identical requests (the `budget`), collects the 5 candidate responses, passes them to the `qwen3-14b` judge, and returns the winner.\n",
    "\n",
    "The `return_response_only=False` flag tells `BestOfN` to return the full result object rather than just the winning answer. That gives us access to all 5 scores, which candidate was selected, and how many candidates were generated. We capture all of this for comparison in the next cell.\n",
    "\n",
    "Each question generates 5 candidate requests plus 1 judge request, so 10 questions means roughly 60 API calls total. \n",
    "\n",
    "Expect this to take 4 to 5 minutes depending on endpoint load, with individual questions ranging from 15 to 60 seconds. The variation is normal: longer answers and more complex comparisons take longer for the judge to evaluate.\n",
    "\n",
    "The cell below controls whether the loop runs live or loads previously saved results. For the first time through, leave `RUN_LIVE = True` and let it run. If you have already run it once, or if you need to move through the material faster, set it to `False` and the notebook will load the results from `../prebuilt/bon_results.json` instead. Either way, the analysis cells that follow work identically.\n",
    "\n",
    "\n",
    "> **Facilitator note**: If time is tight or the endpoint is slow, set RUN_LIVE = False and move on. This is expected behavior during the workshop, not a failure. The first clean run of the day should save results to the prebuilt directory, and every subsequent participant or session can load from there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762b9833-4d50-4a6c-80b3-4a52cd2645a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to run Best-of-N live. Set to False to load saved results.\n",
    "RUN_LIVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce1daa36-ba72-4d68-9707-abca3de2d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x7eff6f121b40> is already entered\n",
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x7eff6f121b40> is already entered\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-11' coro=<_async_in_context.<locals>.run_in_context() done, defined at /opt/app-root/lib64/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-17' coro=<Kernel.shell_main() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /opt/app-root/lib64/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n",
      "/usr/lib64/python3.12/traceback.py:727: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n",
      "  _seen = set()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-17' coro=<Kernel.shell_main() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\n",
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x7eff6f121b40> is already entered\n",
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x7eff6f121b40> is already entered\n",
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x7eff6f121b40> is already entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING BEST-OF-N (budget=5) WITH LLM JUDGE\n",
      "======================================================================\n",
      "\n",
      "  [q01] What happens if a Thief fails an Open Locks attempt?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-23' coro=<_async_in_context.<locals>.run_in_context() done, defined at /opt/app-root/lib64/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-24' coro=<Kernel.shell_main() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /opt/app-root/lib64/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n",
      "/usr/lib64/python3.12/asyncio/futures.py:220: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n",
      "  def add_done_callback(self, fn, *, context=None):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-24' coro=<Kernel.shell_main() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Day 2: PASS\n",
      "    Best candidate: #1 of 5\n",
      "    Answer: If a Thief fails an Open Locks attempt, they must wait until they have gained another level of exper...\n",
      "    Time: 10.6s  (1/10)\n",
      "\n",
      "  [q02] Why can't Elves roll higher than a d6 for hit points?...\n",
      "    Day 2: FAIL\n",
      "    Best candidate: #1 of 5\n",
      "    Answer: According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. ...\n",
      "    Time: 9.3s  (2/10)\n",
      "\n",
      "  [q03] Can a character wear leather armor and cast spells?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-8' coro=<_async_in_context.<locals>.run_in_context() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/utils.py:60> wait_for=<Task pending name='Task-10' coro=<Kernel.shell_main() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /opt/app-root/lib64/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n",
      "/usr/lib64/python3.12/selectors.py:192: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n",
      "  raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-10' coro=<Kernel.shell_main() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-20' coro=<Kernel.shell_main() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-21' coro=<_async_in_context.<locals>.run_in_context() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/utils.py:60> wait_for=<Task pending name='Task-22' coro=<Kernel.shell_main() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /opt/app-root/lib64/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-18' coro=<_async_in_context.<locals>.run_in_context() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/utils.py:60> wait_for=<Task pending name='Task-20' coro=<Kernel.shell_main() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /opt/app-root/lib64/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-22' coro=<Kernel.shell_main() running at /opt/app-root/lib64/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Day 2: PASS\n",
      "    Best candidate: #5 of 5\n",
      "    Answer: Yes, an Elf character can wear leather armor and cast spells. Elves are a combination class, advanci...\n",
      "    Time: 11.2s  (3/10)\n",
      "\n",
      "  [q04] What is the saving throw for a 3rd level Fighter against Dra...\n",
      "    Day 2: FAIL\n",
      "    Best candidate: #2 of 5\n",
      "    Answer: 15 is the saving throw for a 3rd level Fighter against Dragon Breath. This information can be found ...\n",
      "    Time: 27.6s  (4/10)\n",
      "\n",
      "  [q05] How does a Cleric turn undead?...\n",
      "    Day 2: PASS\n",
      "    Best candidate: #3 of 5\n",
      "    Answer: A Cleric turns undead by looking up their level on the Clerics vs. Undead table and cross-referencin...\n",
      "    Time: 21.6s  (5/10)\n",
      "\n",
      "  [q06] If a character has a Strength of 16, what bonus do they get ...\n",
      "    Day 2: FAIL\n",
      "    Best candidate: #2 of 5\n",
      "    Answer: A character with a Strength score of 16 receives a bonus of +2 on melee attack rolls. This is becaus...\n",
      "    Time: 62.7s  (6/10)\n",
      "\n",
      "  [q07] What is the difference between a retainer and a hireling?...\n",
      "    Day 2: FAIL\n",
      "    Best candidate: #1 of 5\n",
      "    Answer: A retainer is a Non-Player Character who is a close associate of a player character and is typically...\n",
      "    Time: 27.5s  (7/10)\n",
      "\n",
      "  [q08] When can a Magic-User learn new spells?...\n",
      "    Day 2: PASS\n",
      "    Best candidate: #1 of 5\n",
      "    Answer: A Magic-User can learn new spells at any point, but there are specific conditions. They must find a ...\n",
      "    Time: 20.4s  (8/10)\n",
      "\n",
      "  [q09] What happens to a character at exactly 0 hit points?...\n",
      "    Day 2: PASS\n",
      "    Best candidate: #4 of 5\n",
      "    Answer: At exactly 0 hit points, a character may be dead. However, this is not necessarily the end for the c...\n",
      "    Time: 12.3s  (9/10)\n",
      "\n",
      "  [q10] Can a Halfling use a longbow?...\n",
      "    Day 2: PASS\n",
      "    Best candidate: #4 of 5\n",
      "    Answer: No, a Halfling cannot use a longbow. According to the context, Halflings must use Medium weapons two...\n",
      "    Time: 27.5s  (10/10)\n",
      "\n",
      "======================================================================\n",
      "Best-of-N complete. Total time: 230.7s (3.8 min)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from its_hub.utils import extract_content_from_lm_response\n",
    "\n",
    "if RUN_LIVE:\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"Answer the question using only the provided context. \"\n",
    "        \"Be specific and cite rules where possible.\"\n",
    "    )\n",
    "\n",
    "    BUDGET = 5\n",
    "\n",
    "    bon_results = []\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RUNNING BEST-OF-N (budget={BUDGET}) WITH LLM JUDGE\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    for i, r in enumerate(results_day2):\n",
    "        print(f\"\\n  [{r['id']}] {r['question'][:60]}...\")\n",
    "\n",
    "        context = r.get(\"retrieved_context\", \"\")\n",
    "        prompt = f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {r['question']}\n",
    "Answer:\"\"\"\n",
    "\n",
    "        q_start = time.time()\n",
    "        result = scaling_alg.infer(\n",
    "            lm, prompt, budget=BUDGET, return_response_only=False\n",
    "        )\n",
    "        q_elapsed = time.time() - q_start\n",
    "\n",
    "        best_answer = extract_content_from_lm_response(result.the_one)\n",
    "\n",
    "        bon_results.append({\n",
    "            \"id\": r[\"id\"],\n",
    "            \"question\": r[\"question\"],\n",
    "            \"expected\": r[\"expected\"],\n",
    "            \"category\": r[\"category\"],\n",
    "            \"day2_answer\": r[\"answer\"],\n",
    "            \"day2_classification\": r[\"classification\"],\n",
    "            \"bon_answer\": best_answer,\n",
    "            \"bon_scores\": result.scores,\n",
    "            \"bon_selected_index\": result.selected_index,\n",
    "            \"bon_n_candidates\": len(result.responses),\n",
    "        })\n",
    "\n",
    "        day2_status = \"PASS\" if r[\"classification\"] == \"pass\" else \"FAIL\"\n",
    "        print(f\"    Day 2: {day2_status}\")\n",
    "        print(f\"    Best candidate: #{result.selected_index + 1} of {len(result.responses)}\")\n",
    "        print(f\"    Answer: {best_answer[:100]}...\")\n",
    "        print(f\"    Time: {q_elapsed:.1f}s  ({i+1}/{len(results_day2)})\")\n",
    "\n",
    "    total_elapsed = time.time() - total_start\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Best-of-N complete. Total time: {total_elapsed:.1f}s ({total_elapsed/60:.1f} min)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "else:\n",
    "    with open(\"../prebuilt/bon_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        bon_results = json.load(f)\n",
    "    BUDGET = bon_results[0][\"bon_n_candidates\"]\n",
    "    print(f\"Loaded {len(bon_results)} pre-built Best-of-N results (budget={BUDGET})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c27ffc4-7c85-41d8-95a9-625e61270a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 results to ../prebuilt/bon_results.json\n",
      "Saved 10 results to ./bon_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save results only if we ran live\n",
    "if RUN_LIVE:\n",
    "    with open(\"../prebuilt/bon_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bon_results, f, indent=2)\n",
    "\n",
    "    with open(\"bon_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bon_results, f, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(bon_results)} results to ../prebuilt/bon_results.json\")\n",
    "    print(f\"Saved {len(bon_results)} results to ./bon_results.json\")\n",
    "else:\n",
    "    print(\"Using pre-built results. Nothing to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2ab5e-e475-4020-9680-e46098267585",
   "metadata": {},
   "source": [
    "### 1.3.2 Comparison: Single-Shot vs. Best-of-N\n",
    "\n",
    "\n",
    "Now we put the results side by side. Questions that failed on Day 2 are marked with `>>` so they stand out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2dd8a60-25ed-4b4c-b080-6c2746f284e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARISON: DAY 2 SINGLE-SHOT vs. BEST-OF-N\n",
      "======================================================================\n",
      "\n",
      "   [q01] What happens if a Thief fails an Open Locks attempt?\n",
      "   Category:     explicit_rule\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     If a Thief fails an Open Locks attempt, they must wait until they have gained another level of experience before trying again.\n",
      "\n",
      "   Best-of-N answer (#1 of 5):\n",
      "     If a Thief fails an Open Locks attempt, they must wait until they have gained another level of experience before trying again.\n",
      "   Scores: [1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      ">> [q02] Why can't Elves roll higher than a d6 for hit points?\n",
      "   Category:     terminology\n",
      "   Day 2 status: implicit_reasoning_failure\n",
      "\n",
      "   Day 2 answer:\n",
      "     According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. The reason for this restriction is not explicitly \n",
      "\n",
      "   Best-of-N answer (#1 of 5):\n",
      "     According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. However, the context does not provide a specific r\n",
      "   Scores: [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      "   [q03] Can a character wear leather armor and cast spells?\n",
      "   Category:     implicit_reasoning\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     Yes, according to the provided context, characters who can cast spells may wear leather armor.\n",
      "\n",
      "   Best-of-N answer (#5 of 5):\n",
      "     Yes, an Elf character can wear leather armor and cast spells. Elves are a combination class, advancing as both Fighters and Magic-Users simultaneously\n",
      "   Scores: [0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "   ---\n",
      "\n",
      ">> [q04] What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "   Category:     table_lookup\n",
      "   Day 2 status: implicit_reasoning_failure\n",
      "\n",
      "   Day 2 answer:\n",
      "     The context does not provide specific information on the saving throw for a 3rd level Fighter against Dragon Breath. However, it does mention that cha\n",
      "\n",
      "   Best-of-N answer (#2 of 5):\n",
      "     15 is the saving throw for a 3rd level Fighter against Dragon Breath. This information can be found in the \"Fighter\" table under the \"Dragon Breath\" c\n",
      "   Scores: [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      "   [q05] How does a Cleric turn undead?\n",
      "   Category:     multi_step_rule\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     To turn undead, the GM looks up the Cleric's level on the Clerics vs. Undead table and cross-references it with the undead type or Hit Dice. If the ta\n",
      "\n",
      "   Best-of-N answer (#3 of 5):\n",
      "     A Cleric turns undead by looking up their level on the Clerics vs. Undead table and cross-referencing it with the type or Hit Dice of the undead monst\n",
      "   Scores: [0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      ">> [q06] If a character has a Strength of 16, what bonus do they get on melee attack rolls?\n",
      "   Category:     table_lookup\n",
      "   Day 2 status: implicit_reasoning_failure\n",
      "\n",
      "   Day 2 answer:\n",
      "     The provided context does not specify how Strength affects melee attack rolls. In Basic Fantasy RPG, it is recommended not to waste time in detailing \n",
      "\n",
      "   Best-of-N answer (#2 of 5):\n",
      "     A character with a Strength score of 16 receives a bonus of +2 on melee attack rolls. This is because, according to the table in the provided context,\n",
      "   Scores: [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      ">> [q07] What is the difference between a retainer and a hireling?\n",
      "   Category:     terminology\n",
      "   Day 2 status: implicit_reasoning_failure\n",
      "\n",
      "   Day 2 answer:\n",
      "     In the Basic Fantasy RPG system, a retainer and a hireling are both types of NPCs that can be hired by player characters. However, the key difference \n",
      "\n",
      "   Best-of-N answer (#1 of 5):\n",
      "     A retainer is a Non-Player Character who is a close associate of a player character and is typically very loyal, willing to take reasonable risks, inc\n",
      "   Scores: [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      "   [q08] When can a Magic-User learn new spells?\n",
      "   Category:     implicit_reasoning\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     A Magic-User can learn new spells at any point, but there are specific conditions. They must find a teacher or acquire a reference work, such as anoth\n",
      "\n",
      "   Best-of-N answer (#1 of 5):\n",
      "     A Magic-User can learn new spells at any point, but there are specific conditions. They must find a teacher or acquire a reference work, such as anoth\n",
      "   Scores: [1.0, 1.0, 0.0, 0.0, 1.0]\n",
      "   ---\n",
      "\n",
      "   [q09] What happens to a character at exactly 0 hit points?\n",
      "   Category:     explicit_rule\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     When a character's hit point total reaches exactly 0, the character may be dead. However, this is not necessarily the end for the character; the chara\n",
      "\n",
      "   Best-of-N answer (#4 of 5):\n",
      "     At exactly 0 hit points, a character may be dead. However, this is not necessarily the end for the character. The rules suggest not to tear up the cha\n",
      "   Scores: [0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "   ---\n",
      "\n",
      "   [q10] Can a Halfling use a longbow?\n",
      "   Category:     implicit_reasoning\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     No, according to the provided context, Halflings may not use Large weapons, and longbows are typically around 6 feet unstrung, which would be consider\n",
      "\n",
      "   Best-of-N answer (#4 of 5):\n",
      "     No, a Halfling cannot use a longbow. According to the context, Halflings must use Medium weapons two-handed and may not use Large weapons. The longbow\n",
      "   Scores: [0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "   ---\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARISON: DAY 2 SINGLE-SHOT vs. BEST-OF-N\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for br in bon_results:\n",
    "    day2_status = \"PASS\" if br[\"day2_classification\"] == \"pass\" else \"FAIL\"\n",
    "    marker = \"  \" if day2_status == \"PASS\" else \">>\"\n",
    "\n",
    "    print(f\"\\n{marker} [{br['id']}] {br['question']}\")\n",
    "    print(f\"   Category:     {br['category']}\")\n",
    "    print(f\"   Day 2 status: {br['day2_classification']}\")\n",
    "    print()\n",
    "    print(f\"   Day 2 answer:\")\n",
    "    print(f\"     {br['day2_answer'][:150]}\")\n",
    "    print()\n",
    "    print(f\"   Best-of-N answer (#{br['bon_selected_index']+1} of {br['bon_n_candidates']}):\")\n",
    "    print(f\"     {br['bon_answer'][:150]}\")\n",
    "    print(f\"   Scores: {br['bon_scores']}\")\n",
    "    print(f\"   {'---'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60a9f7-472e-4752-aacc-fecae00a4252",
   "metadata": {},
   "source": [
    "Read through the output carefully. For each question, ask yourself: did Best-of-N produce a better answer than single-shot?\n",
    "\n",
    "If yes, the model had the capability. It just did not surface it on the first try. This is a sampling win, not a knowledge win. The model's weights already contain something useful, and multiple attempts with a good judge found it.\n",
    "\n",
    "If no, the model tried 5 times, a separate larger judge picked the best attempt, and it is still wrong. That means the failure is not about sampling luck. The model genuinely struggles with this class of question.\n",
    "\n",
    "A note about the scores. Because we are using a groupwise judge, the scores are binary: `1.0` for the selected response and `0.0` for the others. The judge is making a ranking decision, not assigning granular scores. If you see all responses scoring identically, it means even the judge could not differentiate them, which itself is a signal.\n",
    "\n",
    "### 1.3.3 Where It Helps and Where It Doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e1e06f-1581-463b-b5dc-2c151c24e919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BEST-OF-N IMPACT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "  Previously passing: 6 questions\n",
      "  Previously failing: 4 questions\n",
      "\n",
      "  --- Previously Failing Questions ---\n",
      "\n",
      "  [q02] Why can't Elves roll higher than a d6 for hit points?\n",
      "    Category:  terminology\n",
      "    Expected:  Elves use a d6 for hit points because that is the hit die assigned to the Elf combination class in Basic Fantasy RPG.\n",
      "    Day 2:     According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. The reason for this \n",
      "    Best-of-N: According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. However, the context\n",
      "    Scores:    [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "  [q04] What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "    Category:  table_lookup\n",
      "    Expected:  Based on the Fighter saving throw table, a 3rd level Fighter has a Dragon Breath saving throw of 15.\n",
      "    Day 2:     The context does not provide specific information on the saving throw for a 3rd level Fighter against Dragon Breath. How\n",
      "    Best-of-N: 15 is the saving throw for a 3rd level Fighter against Dragon Breath. This information can be found in the \"Fighter\" tab\n",
      "    Scores:    [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "  [q06] If a character has a Strength of 16, what bonus do they get on melee attack rolls?\n",
      "    Category:  table_lookup\n",
      "    Expected:  A Strength score of 16 gives a +2 bonus, which applies to melee attack rolls and damage rolls.\n",
      "    Day 2:     The provided context does not specify how Strength affects melee attack rolls. In Basic Fantasy RPG, it is recommended n\n",
      "    Best-of-N: A character with a Strength score of 16 receives a bonus of +2 on melee attack rolls. This is because, according to the \n",
      "    Scores:    [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "  [q07] What is the difference between a retainer and a hireling?\n",
      "    Category:  terminology\n",
      "    Expected:  Retainers are NPCs who accompany the party on adventures and gain experience. Hirelings are hired for specific non-adven\n",
      "    Day 2:     In the Basic Fantasy RPG system, a retainer and a hireling are both types of NPCs that can be hired by player characters\n",
      "    Best-of-N: A retainer is a Non-Player Character who is a close associate of a player character and is typically very loyal, willing\n",
      "    Scores:    [1.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BEST-OF-N IMPACT ANALYSIS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "day2_failures = [br for br in bon_results if br[\"day2_classification\"] != \"pass\"]\n",
    "day2_passes = [br for br in bon_results if br[\"day2_classification\"] == \"pass\"]\n",
    "\n",
    "print(f\"\\n  Previously passing: {len(day2_passes)} questions\")\n",
    "print(f\"  Previously failing: {len(day2_failures)} questions\")\n",
    "\n",
    "print(f\"\\n  --- Previously Failing Questions ---\")\n",
    "for br in day2_failures:\n",
    "    print(f\"\\n  [{br['id']}] {br['question']}\")\n",
    "    print(f\"    Category:  {br['category']}\")\n",
    "    print(f\"    Expected:  {br['expected'][:120]}\")\n",
    "    print(f\"    Day 2:     {br['day2_answer'][:120]}\")\n",
    "    print(f\"    Best-of-N: {br['bon_answer'][:120]}\")\n",
    "    print(f\"    Scores:    {br['bon_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3aa174-1854-4ed6-95bb-312c5f6a04fc",
   "metadata": {},
   "source": [
    "> **Facilitator note:** This is a discussion moment, not just a code output. Walk the room through each of the 4 previously failing questions and ask:\n",
    "> \"Is this answer better? Is it correct? Is it good enough?\"\n",
    "\n",
    "Point participants to the specific patterns.\n",
    "\n",
    "**q04 (saving throw table lookup):** Day 2 said \"the context does not provide specific information.\" Best-of-N returned the correct answer: 15. One of the 5 candidates actually read the table. The judge picked it. This is the textbook sampling win. The model can do it. It just does not do it reliably on the first try.\n",
    "\n",
    "**q07 (retainer vs hireling):** Day 2 hedged with \"the context does not provide specific details about hirelings.\" Best-of-N correctly states that hirelings are normal people hired for specific services who do not go on adventures. One of the 5 candidates used both sections of the retrieved context. The judge picked it.\n",
    "\n",
    "**q06 (Strength bonus):** The answer lands on \"+2\" which is the correct number, but look at the reasoning. It says \"as per the standard D&D rules\" or refers to an ability score table \"not provided in the context.\" The model guessed correctly from its general training data, not from the retrieved context. In a customer setting where answers must be grounded in their documents, that is still a failure. The right number for the wrong reason is not trustworthy.\n",
    "\n",
    "**q02 (Elf hit dice):** All 5 candidates produced the same hedge: \"The reason for this restriction is not explicitly stated in the context.\" The model consistently cannot infer the \"why\" from the text. Five attempts, same wall. This is the clearest signal that the failure is systematic, not stochastic.\n",
    "\n",
    "The scorecard: \n",
    "\n",
    "* 2 clear wins (q04, q07),\n",
    "* 1 false positive (q06, right answer from wrong source),\n",
    "* 1 persistent failure (q02).\n",
    "\n",
    "Inference-time scaling helped but did not solve everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c84d1f-a83b-4068-98b0-aa16269f2422",
   "metadata": {},
   "source": [
    "## 1.4 Other Inference-Time Scaling Strategies (Conceptual Only)\n",
    "\n",
    "Best-of-N is the simplest algorithm in `its_hub`. There are others, and they are worth understanding conceptually even though we will not implement them all today. The point is not to teach every algorithm. It is to show that inference-time scaling is a family of techniques with different tradeoffs, and Best-of-N is just the entry point.\n",
    "\n",
    "### 1.4.1 Self-Consistency\n",
    "\n",
    "Self-consistency generates multiple reasoning paths and selects the answer that appears most frequently. Instead of using a judge to evaluate quality, it uses agreement as a signal. If 4 out of 5 attempts arrive at the same answer, that answer is probably more reliable than one that only appears once.\n",
    "\n",
    "The implementation in `its_hub` takes a **projection function** that extracts the \"answer\" from each response. For math problems, that might mean pulling a number out of a paragraph of reasoning. It then votes on the projected answers. The response whose projected answer matches the majority is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49923080-ad91-40c8-8610-ade6e083fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual only. Do not run this cell.\n",
    "# Shown to illustrate the Self-Consistency API pattern.\n",
    "\n",
    "from its_hub.algorithms import SelfConsistency\n",
    "\n",
    "# For math: extract the boxed answer and vote\n",
    "# sc = SelfConsistency(lambda s: extract_boxed(s))\n",
    "# result = sc.infer(lm, prompt, budget=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90656a6-77c6-46ac-81dd-0d545bc94dae",
   "metadata": {},
   "source": [
    "Self-consistency works well when the model can reason through the problem but occasionally takes a wrong turn. The majority vote filters out the unlucky paths. It works poorly when the model consistently reasons in the same incorrect direction, because all 5 attempts will agree on the wrong answer.\n",
    "\n",
    "For our use case, self-consistency is less useful because our questions have free-text answers, not extractable numeric results. There is no clean projection function for \"explain the difference between a retainer and a hireling.\" The algorithm needs a way to determine when two answers are \"the same,\" and for open-ended text that comparison is not straightforward.\n",
    "\n",
    "### 1.4.2 Particle Filtering and Beam Search\n",
    "\n",
    "These approaches operate at the token or step level rather than the response level. Instead of generating complete responses and selecting among them, they maintain multiple partial generations simultaneously and prune weak reasoning paths as they go.\n",
    "\n",
    "Particle Filtering treats generation as a search problem: generate one reasoning step at a time, score each partial solution with a Process Reward Model, resample the population toward higher-scoring paths, and continue. Think of it as Best-of-N applied at every sentence rather than at the end. `its_hub` includes both standard and entropic variants.\n",
    "\n",
    "Beam Search maintains the top-K partial sequences at each step and only expands from those. It is more aggressive about pruning than Particle Filtering, keeping only the highest-scoring paths alive at each step.\n",
    "\n",
    "Both require a **Process Reward Model** (PRM), which scores intermediate reasoning steps, not just final answers. That means they need `pip install its_hub[prm]` and a local GPU to run the PRM. They are most relevant when you control the serving infrastructure and can run reward models locally.\n",
    "\n",
    "**Why this matters for the field:** When a customer asks \"what else can we try before fine-tuning,\" you now have a layered answer. Best-of-N works with any API endpoint and no extra infrastructure. Self-consistency works when answers are extractable and comparable. Particle Filtering and Beam Search offer the most control but require GPU resources for the reward model. Each is a step up in infrastructure complexity, and each is still cheaper than model training.\n",
    "\n",
    "> **Facilitator note:** Keep this conceptual. Do not attempt to run Particle Filtering or Beam Search in this lab. The point is awareness, not implementation. If participants ask about these in depth, point them to the `its_hub` documentation and the quick-start examples in the repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d24a298-e563-48f4-b322-a1134b78e54c",
   "metadata": {},
   "source": [
    "## 1.5 Decision Point: Is Inference-Time Scaling Enough?\n",
    "\n",
    "This is the second decision point in the workshop. The first was at the end of Day 2: \"Is RAG enough?\" The answer was no, for 4 out of 10 questions. Now we ask: \"Is inference-time scaling enough?\"\n",
    "\n",
    "### 1.5.1 When It Solves the Problem\n",
    "\n",
    "If Best-of-N reliably produces correct answers for the previously failing questions, you may not need to train at all. The model has the capability. You just need a selection layer at inference time to surface it reliably.\n",
    "\n",
    "In that case, the customer conversation sounds like:\n",
    "\n",
    "\"The model can answer these questions correctly. It just needs a selection layer to do it reliably. We can deploy this without any model training, using the same infrastructure you already have.\"\n",
    "\n",
    "That is a fast, low-risk win. Take it if the evidence supports it.\n",
    "\n",
    "### 1.5.2 When It Confirms You Need to Train\n",
    "\n",
    "If Best-of-N does not meaningfully improve the failing questions, or if all 5 candidates exhibit the same failure pattern, that tells you something definitive: the model's weights do not contain a reliable path to the right answer for this domain. More sampling will not fix that. You need to change what the model knows, not how many times it tries.\n",
    "\n",
    "In that case, the customer conversation sounds like:\n",
    "\n",
    "\"We tried letting the model work harder at inference time. For these specific question types, it still cannot reliably produce the right answer. The gap is not about compute or sampling. It is about knowledge. This is where model adaptation becomes justified.\"\n",
    "\n",
    "This is the strongest possible position to be in when recommending fine-tuning: you have evidence from multiple independent approaches (RAG, inference-time scaling) showing that the current model is insufficient for specific, documented failure cases.\n",
    "\n",
    "### 1.5.3 Where We Are Now\n",
    "\n",
    "Our results fall in between, which is the most realistic and most instructive outcome.\n",
    "\n",
    "Some failures responded to inference-time scaling (q04, q07). The model had the capability but did not surface it on the first try. For those question types, a selection layer at inference time might be sufficient.\n",
    "\n",
    "Other failures did not respond (q02). Five attempts, same hedge, same wall. That is a knowledge gap, not a sampling gap.\n",
    "\n",
    "And one result looked like a win but was not (q06). The model produced the right number but grounded it in general training knowledge rather than the customer's documents. In a production system where answers must be traceable to source material, that is not acceptable.\n",
    "\n",
    "The honest assessment: inference-time scaling narrowed the gap but did not close it. The remaining failures are not things we can sample our way out of. They require changing what the model knows about this domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58efc945-1b07-4d3d-ba68-8a95a028dcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 1 SUMMARY\n",
      "======================================================================\n",
      "\n",
      "  Day 2 RAG baseline:  6/10 passing\n",
      "  Failing categories:  implicit_reasoning_failure\n",
      "  Best-of-N budget:    5 candidates per question\n",
      "  Judge model:         qwen3-14b (groupwise)\n",
      "\n",
      "  --- Previously Failing Questions ---\n",
      "  [q02] terminology\n",
      "    Day 2:     According to the provided context, Elves never roll larger than six-sided dice (...\n",
      "    Best-of-N: According to the provided context, Elves never roll larger than six-sided dice (...\n",
      "    Scores:    [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  [q04] table_lookup\n",
      "    Day 2:     The context does not provide specific information on the saving throw for a 3rd ...\n",
      "    Best-of-N: 15 is the saving throw for a 3rd level Fighter against Dragon Breath. This infor...\n",
      "    Scores:    [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "  [q06] table_lookup\n",
      "    Day 2:     The provided context does not specify how Strength affects melee attack rolls. I...\n",
      "    Best-of-N: A character with a Strength score of 16 receives a bonus of +2 on melee attack r...\n",
      "    Scores:    [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "  [q07] terminology\n",
      "    Day 2:     In the Basic Fantasy RPG system, a retainer and a hireling are both types of NPC...\n",
      "    Best-of-N: A retainer is a Non-Player Character who is a close associate of a player charac...\n",
      "    Scores:    [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "  Inference-time scaling narrowed the gap but did not close it.\n",
      "  Remaining failures are knowledge gaps, not sampling gaps.\n",
      "  The case for model adaptation is now supported by two\n",
      "  independent approaches: RAG evaluation and inference-time scaling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SECTION 1 SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\n  Day 2 RAG baseline:  6/10 passing\")\n",
    "print(f\"  Failing categories:  implicit_reasoning_failure\")\n",
    "print(f\"  Best-of-N budget:    {BUDGET} candidates per question\")\n",
    "print(f\"  Judge model:         qwen3-14b (groupwise)\")\n",
    "\n",
    "print(f\"\\n  --- Previously Failing Questions ---\")\n",
    "for br in day2_failures:\n",
    "    print(f\"  [{br['id']}] {br['category']}\")\n",
    "    print(f\"    Day 2:     {br['day2_answer'][:80]}...\")\n",
    "    print(f\"    Best-of-N: {br['bon_answer'][:80]}...\")\n",
    "    print(f\"    Scores:    {br['bon_scores']}\")\n",
    "\n",
    "print(f\"\\n  Inference-time scaling narrowed the gap but did not close it.\")\n",
    "print(f\"  Remaining failures are knowledge gaps, not sampling gaps.\")\n",
    "print(f\"  The case for model adaptation is now supported by two\")\n",
    "print(f\"  independent approaches: RAG evaluation and inference-time scaling.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b8506-2ca7-4f01-b977-feafddffcdfc",
   "metadata": {},
   "source": [
    "> **Facilitator note:** Pose the question to the room:\n",
    ">\n",
    "> \"We have tried giving the model the right context. We have tried giving it multiple attempts with a judge selecting the best one. If it still cannot answer these questions, what is left to change?\"\n",
    ">\n",
    "> The answer is the model itself. But we do not jump straight to training. First, we need training data. And for a domain where the documents are the only source of truth, generating that training data responsibly is its own challenge.\n",
    "\n",
    "**Transition to Section 2:**\n",
    "\n",
    "\"We now have evidence from two sources: RAG evaluation and inference-time scaling. Both point to the same conclusion for the remaining failures: these problems live in the model, not in the pipeline. The next question is practical. If we are going to adapt the model, what do we train it on? That is the problem synthetic data generation solves.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}