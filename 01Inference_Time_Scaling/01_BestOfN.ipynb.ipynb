{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd566e5c-ff5b-4573-b44f-c82b437928e1",
   "metadata": {},
   "source": [
    "# 1. Inference-Time Scaling: Pushing the Model Before Changing It\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Before committing to model adaptation, test whether spending more compute at inference time can close the gap. This is the last and most rigorous check before escalating to training. If inference-time scaling solves the problem, we avoid the cost and complexity of fine-tuning entirely. If it does not, we have even stronger evidence that the model's weights need to change.\n",
    "\n",
    "\n",
    "## 1.1 The Idea: Spend Compute at Inference, Not Training\n",
    "\n",
    "There is a common assumption in AI engagements that when a model gives a wrong answer, the model needs to be retrained. Sometimes that is true. But retraining is expensive, slow, and organizationally complex. Before going there, it is worth asking a simpler question:\n",
    "\n",
    "What if we just let the model try harder?\n",
    "\n",
    "That is the core idea behind inference-time scaling. Instead of changing the model's weights (which is what fine-tuning does), you change how much work the model does at inference time. You let it generate multiple candidate answers, evaluate them, and select the best one. The model itself is unchanged. You are spending compute instead of training data.\n",
    "\n",
    "This matters for two reasons.\n",
    "\n",
    "First, it is cheaper and faster to try. There is no training pipeline to build, no data to curate, no model to validate. You are using the same model, the same endpoint, the same API key. The only thing that changes is how many times you call it and how you pick the winner.\n",
    "\n",
    "Second, the results are diagnostic. If the model produces the right answer on attempt 3 out of 5, that tells you something important: the knowledge is accessible but the model's default sampling path does not reliably surface it. That is a very different problem than \"the model fundamentally cannot do this.\" And the fix might be as simple as a better decoding strategy rather than a full training run.\n",
    "\n",
    "If the model cannot produce the right answer in N attempts with good context, that is also diagnostic. It means the failure is not a sampling issue. The model genuinely does not know how to handle this class of question. That is the strongest possible evidence for model adaptation.\n",
    "\n",
    "Either way, you learn something. And learning before spending is the entire philosophy of this workshop.\n",
    "\n",
    "## 1.2 Introducing `its_hub`\n",
    "\n",
    "``its_hub`` is an inference-time scaling library built by the Red Hat AI Innovation Team. It implements several algorithms for improving model output quality without modifying model weights, and it works with any OpenAI-compatible API endpoint. That means it works with our MaaS setup without any infrastructure changes.\n",
    "\n",
    "The library has three layers:\n",
    "\n",
    "A **language model wrapper** (`OpenAICompatibleLanguageModel`) that handles generation against any OpenAI-compatible endpoint, including batched and async calls.\n",
    "\n",
    "A **reward model** that scores candidate responses. For Best-of-N, this is ``LLMJudgeRewardModel``, which uses a separate LLM to evaluate response quality. For more advanced algorithms like Particle Filtering, it can be a local Process Reward Model that scores each reasoning step.\n",
    "\n",
    "A **scaling algorithm** that orchestrates the generation and scoring. `BestOfN` generates N candidates and picks the best. `SelfConsistency` generates N candidates and picks the most common answer. `ParticleFiltering` and `BeamSearch` operate at the step level, pruning weak reasoning paths as they go.\n",
    "\n",
    "For this lab, we will use **Best-of-N with LLM Judge**. The idea is straightforward:\n",
    "\n",
    "1. Send the same prompt to the model N times (the \"budget\")\n",
    "2. Collect N candidate responses\n",
    "3. Use a separate LLM judge to evaluate and rank the candidates\n",
    "4. Return the highest-scoring response\n",
    "\n",
    "The generation model is not retrained. It is not prompted differently. It simply gets multiple chances, and a judge picks the best attempt.\n",
    "\n",
    "### 1.2.1 Environment Setup\n",
    "\n",
    "Open the `01/01Inference_Time_Scaling/01_BestOfN.ipynb` notebook.\n",
    "\n",
    "\n",
    "Before we touch any code, we need two values: an API key and an endpoint URL for the MaaS (Model as a Service) platform. If you attended Day 2, you already have these. If you are starting fresh today, follow the setup steps below. If you already have a working `.env` file from Day 2, skip ahead to the verification cell.\n",
    "\n",
    "**For participants who did not attend Day 2:**\n",
    "\n",
    "You need access to the Red Hat Demo Platform MaaS service. Using your Red Hat SSO credentials, log in at:\n",
    "\n",
    "> https://litellm-prod-frontend.apps.maas.redhatworkshops.io/home\n",
    "\n",
    "Once logged in:\n",
    "\n",
    "1. Click Subscriptions in the left sidebar, then New Subscription. Subscribe to the following models: granite-3-2-8b-instruct, granite-4-0-h-tiny, microsoft-phi-4, and qwen3-14b. For each one, click the model name to open the model card, then click Subscribe.\n",
    "2. Click API Keys in the left sidebar, then Create API Key. Give it a name (anything you like), check Select All to attach all your subscribed models, and click Create API Key.\n",
    "3. When the key appears, click the copy button and save it somewhere. You will not be able to see the full key again after closing the dialog.\n",
    "4. Before closing the dialog, note the API URL displayed on the key details screen. It will look like `https://litellm-prod.apps.maas.redhatworkshops.io/v1`. Copy this as well.\n",
    "\n",
    "Now create a file named `.env` in the parent of this folder. The file should contain exactly two lines:\n",
    "\n",
    "```\n",
    "API_KEY=your-api-key-here\n",
    "ENDPOINT_BASE=endpoint-here\n",
    "```\n",
    "\n",
    "**For everyone:**\n",
    "\n",
    "Replace your-api-key-here with the key you copied and endpoint-here with the correct endpoint\n",
    "\n",
    "\n",
    "No quotes around the values.  \n",
    "\n",
    "No spaces around the equals sign.\n",
    "If you are unsure where to create the file, run this in a notebook cell to confirm the expected path:\n",
    "\n",
    "\n",
    "The lab uses a small config helper that reads the `.env` file. \n",
    "\n",
    "Run the following cell to verify that everything loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d18b1822-df94-40e8-b42e-ea0b873f35f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.py expects .env at: /opt/app-root/src/Day3/.env\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "expected = Path(os.getcwd()).parent / \".env\"\n",
    "print(f\"config.py expects .env at: {expected}\")\n",
    "print(f\"File exists: {expected.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3615a81-23ae-4869-a782-42ee35c9589e",
   "metadata": {},
   "source": [
    "If it shows `File exists: False`, create the file at the printed path and re-run the config cell.\n",
    "\n",
    "The lab uses a small config helper that reads the `.env` file. Run the following cell to verify that everything loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aaa3e2d-8176-4698-8353-33328549ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: https://litellm-prod.apps.maas.redhatworkshops.io/v1\n",
      "API Key:  sk-UFHcL...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from config import API_KEY as key, ENDPOINT_BASE as endpoint_base\n",
    "\n",
    "print(f\"Endpoint: {endpoint_base}\")\n",
    "print(f\"API Key:  {key[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acaed1b-30c3-4aab-b038-fa88d0ec0651",
   "metadata": {},
   "source": [
    "The output will show your endpoint URL in full and the first 8 characters of your API key followed by an ellipsis. \n",
    "\n",
    "The key is deliberately truncated so it is not exposed in notebook output, screenshots, or screen shares. \n",
    "\n",
    "If either value is blank or shows `None`, check that the `.env` file exists in the Day3 directory and that the variable names match exactly: `API_KEY` and `ENDPOINT_BASE`.\n",
    "\n",
    "### 1.2.2 Install and Verify Dependencies\n",
    "\n",
    "The `its_hub` library is pre-installed in the lab environment along with most of its dependencies. Run the following cell to confirm the installation and install `nest_asyncio`, which is needed for running async code inside Jupyter notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a976069-9b99-4059-9fd7-7ea24eccd2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install its_hub nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ddc6e-9966-4b2f-8279-802b258f84e9",
   "metadata": {},
   "source": [
    "You will likely see \"Requirement already satisfied\" for most packages. That is expected. The output confirms what is present and at what version. If any package fails to install or reports a version conflict, notify the instructor.\n",
    "\n",
    "\n",
    "A quick note on why `nest_asyncio` is necessary. Under the hood, `its_hub` uses Python's `asyncio` to send generation requests in parallel. That is how Best-of-N can fire off 5 candidate requests concurrently rather than sequentially. The problem is that Jupyter itself already runs an event loop, and Python's default behavior is to reject a second event loop inside an existing one. `nest_asyncio` patches that restriction so the two can coexist. \n",
    "\n",
    "Without it, every call to `scaling_alg.infer()` would raise a `RuntimeError: This event loop is already running` exception. This is not specific to `its_hub`. Any async library used inside a Jupyter notebook has the same issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb4c58fa-c454-4de3-b4a6-f5ff7337053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9ab78-a316-48ac-8bb4-d5c29d1a0971",
   "metadata": {},
   "source": [
    "No output from that cell means it worked. If it throws an `ImportError`, the `pip install` cell above did not complete successfully.\n",
    "\n",
    "### 1.2.3 Connecting the Generator and Judge to MaaS\n",
    "\n",
    "No new infrastructure. No new credentials. We are using the same MaaS endpoint and the same API key from the previous cell.\n",
    "\n",
    "We need two model connections: one for generating answers and one for judging them. The generator is `granite-3-2-8b-instruct`, the same model we have been using throughout. The judge will be `qwen3-14b`, the largest model available on our MaaS endpoint. Using a separate, larger model for judgment matters because a model is not always the best evaluator of its own output. A bigger model with broader training can often spot weaknesses that the smaller model cannot recognize in its own responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a376d9-e931-4d55-a58a-dcca061f2785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: granite-3-2-8b-instruct\n",
      "Judge:     qwen3-14b (groupwise, overall_quality)\n",
      "Algorithm: Best-of-N\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "from its_hub.lms import OpenAICompatibleLanguageModel\n",
    "from its_hub.algorithms import BestOfN\n",
    "from its_hub.integration.reward_hub import LLMJudgeRewardModel\n",
    "\n",
    "# The generation model: same as Day 2\n",
    "lm = OpenAICompatibleLanguageModel(\n",
    "    endpoint=endpoint_base,\n",
    "    api_key=key,\n",
    "    model_name=\"granite-3-2-8b-instruct\",\n",
    "    temperature=0.7,  # Some variation across candidates\n",
    ")\n",
    "\n",
    "# The judge: a separate, larger model for evaluating candidates\n",
    "# Note: LLMJudgeRewardModel uses litellm internally, which requires\n",
    "# the \"openai/\" prefix to route to an OpenAI-compatible endpoint.\n",
    "judge = LLMJudgeRewardModel(\n",
    "    model=\"openai/qwen3-14b\",\n",
    "    criterion=\"overall_quality\",\n",
    "    judge_type=\"groupwise\",\n",
    "    api_key=key,\n",
    "    base_url=endpoint_base,\n",
    ")\n",
    "\n",
    "# Wire them together\n",
    "scaling_alg = BestOfN(judge)\n",
    "\n",
    "print(\"Generator: granite-3-2-8b-instruct\")\n",
    "print(\"Judge:     qwen3-14b (groupwise, overall_quality)\")\n",
    "print(\"Algorithm: Best-of-N\")\n",
    "print(\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a69da-2a41-47d7-a525-04102c08149b",
   "metadata": {},
   "source": [
    "Let's walk through what each piece does.\n",
    "\n",
    "`OpenAICompatibleLanguageModel` wraps any OpenAI-compatible API endpoint. It handles batched requests, retries, and concurrency internally. The `temperature=0.7` is important here. In Day 2 we used `temperature=0` for deterministic single-shot answers. Now we want variation. If every candidate is identical, there is nothing for the judge to choose between. A temperature of 0.7 gives the model enough freedom to explore different phrasings and reasoning paths while staying coherent.\n",
    "\n",
    "`LLMJudgeRewardModel` uses a second LLM to evaluate and rank candidate responses. The `criterion=\"overall_quality\"` tells the judge to evaluate on general quality rather than a narrow metric. The `judge_type=\"groupwise\"` means the judge sees all candidates at once and picks a winner, rather than scoring each one independently. This tends to produce better selections because the judge can compare directly.\n",
    "\n",
    "Two details on the judge configuration deserve attention.\n",
    "\n",
    "First, the `base_url` parameter. Without it, `LLMJudgeRewardModel` defaults to calling OpenAI's API. Setting base_url=endpoint_base routes the judge's calls through our MaaS endpoint instead. This is a common gotcha when using LLM-as-judge with custom serving infrastructure.\n",
    "\n",
    "Second, the `openai/` prefix on the model name. The `LLMJudgeRewardModel` uses `litellm` internally for its API calls, and litellm uses that prefix to determine which provider protocol to speak. Without it, litellm does not know that MaaS is an OpenAI-compatible endpoint and the call fails with a \"LLM Provider NOT provided\" error. The generator does not need this prefix because `OpenAICompatibleLanguageModel` makes direct HTTP requests to the endpoint, bypassing litellm entirely. This is a subtle but important difference between the two components. If you hit a provider routing error on the judge but the generator works fine, check the prefix first.\n",
    "\n",
    "`BestOfN` is the algorithm that ties them together. It takes the judge at construction time. When we call `scaling_alg.infer()`, it will generate N candidates using the language model, pass all of them to the judge, and return the one the judge ranked highest.\n",
    "\n",
    "If the cell runs without error, you should see the three confirmation lines printed. If you get a connection error, verify that your endpoint and API key are correct and that both `granite-3-2-8b-instruct` and `qwen3-14b` are in your MaaS subscriptions.\n",
    "\n",
    "## 1.3 Best-of-N with LLM Judge\n",
    "\n",
    "Now we apply this to the questions from Day 2. We will run all 10 questions through the Best-of-N pipeline so we can observe two things: whether the passing questions still pass (no regression), and whether the failing questions improve.\n",
    "\n",
    "\n",
    "### 1.3.1 Running the Target Questions Through Best-of-N\n",
    "\n",
    "First, we load the Day 2 results. This version of the evaluation data includes the retrieved context for each question so we can reconstruct the exact RAG prompts from Day 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a5d3651-46ae-4128-8ccf-bc8a8e9aa343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 questions from Day 2 evaluation\n",
      "Passes:   6\n",
      "Failures: 4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../prebuilt/eval_with_context.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "results_day2 = eval_data[\"results\"]\n",
    "\n",
    "print(f\"Loaded {len(results_day2)} questions from Day 2 evaluation\")\n",
    "print(f\"Passes:   {sum(1 for r in results_day2 if r['classification'] == 'pass')}\")\n",
    "print(f\"Failures: {sum(1 for r in results_day2 if r['classification'] != 'pass')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2ca37-9ce6-4450-aacc-5a59e2888938",
   "metadata": {},
   "source": [
    "Let's walk through the structure of this loop before running it.\n",
    "\n",
    "For each of the 10 questions, we build a prompt that includes the system instruction, the retrieved context from Day 2, and the question itself. This is the same prompt structure the model saw during the Day 2 evaluation. The only difference is what happens after the prompt is sent. Instead of a single generation call, `scaling_alg.infer()` sends 5 identical requests (the `budget`), collects the 5 candidate responses, passes them to the `qwen3-14b` judge, and returns the winner.\n",
    "\n",
    "The return_response_only=False flag tells BestOfN to return the full result object rather than just the winning answer. That gives us access to all 5 scores, which candidate was selected, and how many candidates were generated. We capture all of this for comparison in the next cell.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
