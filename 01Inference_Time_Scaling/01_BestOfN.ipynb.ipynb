{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd566e5c-ff5b-4573-b44f-c82b437928e1",
   "metadata": {},
   "source": [
    "# 1. Inference-Time Scaling: Pushing the Model Before Changing It\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Before committing to model adaptation, test whether spending more compute at inference time can close the gap. This is the last and most rigorous check before escalating to training. If inference-time scaling solves the problem, we avoid the cost and complexity of fine-tuning entirely. If it does not, we have even stronger evidence that the model's weights need to change.\n",
    "\n",
    "\n",
    "## 1.1 The Idea: Spend Compute at Inference, Not Training\n",
    "\n",
    "There is a common assumption in AI engagements that when a model gives a wrong answer, the model needs to be retrained. Sometimes that is true. But retraining is expensive, slow, and organizationally complex. Before going there, it is worth asking a simpler question:\n",
    "\n",
    "What if we just let the model try harder?\n",
    "\n",
    "That is the core idea behind inference-time scaling. Instead of changing the model's weights (which is what fine-tuning does), you change how much work the model does at inference time. You let it generate multiple candidate answers, evaluate them, and select the best one. The model itself is unchanged. You are spending compute instead of training data.\n",
    "\n",
    "This matters for two reasons.\n",
    "\n",
    "First, it is cheaper and faster to try. There is no training pipeline to build, no data to curate, no model to validate. You are using the same model, the same endpoint, the same API key. The only thing that changes is how many times you call it and how you pick the winner.\n",
    "\n",
    "Second, the results are diagnostic. If the model produces the right answer on attempt 3 out of 5, that tells you something important: the knowledge is accessible but the model's default sampling path does not reliably surface it. That is a very different problem than \"the model fundamentally cannot do this.\" And the fix might be as simple as a better decoding strategy rather than a full training run.\n",
    "\n",
    "If the model cannot produce the right answer in N attempts with good context, that is also diagnostic. It means the failure is not a sampling issue. The model genuinely does not know how to handle this class of question. That is the strongest possible evidence for model adaptation.\n",
    "\n",
    "Either way, you learn something. And learning before spending is the entire philosophy of this workshop.\n",
    "\n",
    "## 1.2 Introducing `its_hub`\n",
    "\n",
    "``its_hub`` is an inference-time scaling library built by the Red Hat AI Innovation Team. It implements several algorithms for improving model output quality without modifying model weights, and it works with any OpenAI-compatible API endpoint. That means it works with our MaaS setup without any infrastructure changes.\n",
    "\n",
    "The library has three layers:\n",
    "\n",
    "A **language model wrapper** (`OpenAICompatibleLanguageModel`) that handles generation against any OpenAI-compatible endpoint, including batched and async calls.\n",
    "\n",
    "A **reward model** that scores candidate responses. For Best-of-N, this is ``LLMJudgeRewardModel``, which uses a separate LLM to evaluate response quality. For more advanced algorithms like Particle Filtering, it can be a local Process Reward Model that scores each reasoning step.\n",
    "\n",
    "A **scaling algorithm** that orchestrates the generation and scoring. `BestOfN` generates N candidates and picks the best. `SelfConsistency` generates N candidates and picks the most common answer. `ParticleFiltering` and `BeamSearch` operate at the step level, pruning weak reasoning paths as they go.\n",
    "\n",
    "For this lab, we will use **Best-of-N with LLM Judge**. The idea is straightforward:\n",
    "\n",
    "1. Send the same prompt to the model N times (the \"budget\")\n",
    "2. Collect N candidate responses\n",
    "3. Use a separate LLM judge to evaluate and rank the candidates\n",
    "4. Return the highest-scoring response\n",
    "\n",
    "The generation model is not retrained. It is not prompted differently. It simply gets multiple chances, and a judge picks the best attempt.\n",
    "\n",
    "### 1.2.1 Environment Setup\n",
    "\n",
    "Open the `01/01Inference_Time_Scaling/01_BestOfN.ipynb` notebook.\n",
    "\n",
    "\n",
    "Before we touch any code, we need two values: an API key and an endpoint URL for the MaaS (Model as a Service) platform. If you attended Day 2, you already have these. If you are starting fresh today, follow the setup steps below. If you already have a working `.env` file from Day 2, skip ahead to the verification cell.\n",
    "\n",
    "**For participants who did not attend Day 2:**\n",
    "\n",
    "You need access to the Red Hat Demo Platform MaaS service. Using your Red Hat SSO credentials, log in at:\n",
    "\n",
    "> https://litellm-prod-frontend.apps.maas.redhatworkshops.io/home\n",
    "\n",
    "Once logged in:\n",
    "\n",
    "1. Click Subscriptions in the left sidebar, then New Subscription. Subscribe to the following models: granite-3-2-8b-instruct, granite-4-0-h-tiny, microsoft-phi-4, and qwen3-14b. For each one, click the model name to open the model card, then click Subscribe.\n",
    "2. Click API Keys in the left sidebar, then Create API Key. Give it a name (anything you like), check Select All to attach all your subscribed models, and click Create API Key.\n",
    "3. When the key appears, click the copy button and save it somewhere. You will not be able to see the full key again after closing the dialog.\n",
    "4. Before closing the dialog, note the API URL displayed on the key details screen. It will look like `https://litellm-prod.apps.maas.redhatworkshops.io/v1`. Copy this as well.\n",
    "\n",
    "Now create a file named `.env` in the parent of this folder. The file should contain exactly two lines:\n",
    "\n",
    "```\n",
    "API_KEY=your-api-key-here\n",
    "ENDPOINT_BASE=endpoint-here\n",
    "```\n",
    "\n",
    "**For everyone:**\n",
    "\n",
    "Replace your-api-key-here with the key you copied and endpoint-here with the correct endpoint\n",
    "\n",
    "\n",
    "No quotes around the values.  \n",
    "\n",
    "No spaces around the equals sign.\n",
    "If you are unsure where to create the file, run this in a notebook cell to confirm the expected path:\n",
    "\n",
    "\n",
    "The lab uses a small config helper that reads the `.env` file. \n",
    "\n",
    "Run the following cell to verify that everything loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d18b1822-df94-40e8-b42e-ea0b873f35f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.py expects .env at: /opt/app-root/src/Day3/.env\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "expected = Path(os.getcwd()).parent / \".env\"\n",
    "print(f\"config.py expects .env at: {expected}\")\n",
    "print(f\"File exists: {expected.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3615a81-23ae-4869-a782-42ee35c9589e",
   "metadata": {},
   "source": [
    "If it shows `File exists: False`, create the file at the printed path and re-run the config cell.\n",
    "\n",
    "The lab uses a small config helper that reads the `.env` file. Run the following cell to verify that everything loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aaa3e2d-8176-4698-8353-33328549ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: https://litellm-prod.apps.maas.redhatworkshops.io/v1\n",
      "API Key:  sk-UFHcL...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from config import API_KEY as key, ENDPOINT_BASE as endpoint_base\n",
    "\n",
    "print(f\"Endpoint: {endpoint_base}\")\n",
    "print(f\"API Key:  {key[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acaed1b-30c3-4aab-b038-fa88d0ec0651",
   "metadata": {},
   "source": [
    "The output will show your endpoint URL in full and the first 8 characters of your API key followed by an ellipsis. \n",
    "\n",
    "The key is deliberately truncated so it is not exposed in notebook output, screenshots, or screen shares. \n",
    "\n",
    "If either value is blank or shows `None`, check that the `.env` file exists in the Day3 directory and that the variable names match exactly: `API_KEY` and `ENDPOINT_BASE`.\n",
    "\n",
    "### 1.2.2 Install and Verify Dependencies\n",
    "\n",
    "The `its_hub` library is pre-installed in the lab environment along with most of its dependencies. Run the following cell to confirm the installation and install `nest_asyncio`, which is needed for running async code inside Jupyter notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a976069-9b99-4059-9fd7-7ea24eccd2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install its_hub nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ddc6e-9966-4b2f-8279-802b258f84e9",
   "metadata": {},
   "source": [
    "You will likely see \"Requirement already satisfied\" for most packages. That is expected. The output confirms what is present and at what version. If any package fails to install or reports a version conflict, notify the instructor.\n",
    "\n",
    "\n",
    "A quick note on why `nest_asyncio` is necessary. Under the hood, `its_hub` uses Python's `asyncio` to send generation requests in parallel. That is how Best-of-N can fire off 5 candidate requests concurrently rather than sequentially. The problem is that Jupyter itself already runs an event loop, and Python's default behavior is to reject a second event loop inside an existing one. `nest_asyncio` patches that restriction so the two can coexist. \n",
    "\n",
    "Without it, every call to `scaling_alg.infer()` would raise a `RuntimeError: This event loop is already running` exception. This is not specific to `its_hub`. Any async library used inside a Jupyter notebook has the same issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb4c58fa-c454-4de3-b4a6-f5ff7337053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9ab78-a316-48ac-8bb4-d5c29d1a0971",
   "metadata": {},
   "source": [
    "No output from that cell means it worked. If it throws an `ImportError`, the `pip install` cell above did not complete successfully.\n",
    "\n",
    "### 1.2.3 Connecting the Generator and Judge to MaaS\n",
    "\n",
    "No new infrastructure. No new credentials. We are using the same MaaS endpoint and the same API key from the previous cell.\n",
    "\n",
    "We need two model connections: one for generating answers and one for judging them. The generator is `granite-3-2-8b-instruct`, the same model we have been using throughout. The judge will be `qwen3-14b`, the largest model available on our MaaS endpoint. Using a separate, larger model for judgment matters because a model is not always the best evaluator of its own output. A bigger model with broader training can often spot weaknesses that the smaller model cannot recognize in its own responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a376d9-e931-4d55-a58a-dcca061f2785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: granite-3-2-8b-instruct\n",
      "Judge:     qwen3-14b (groupwise, overall_quality)\n",
      "Algorithm: Best-of-N\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "from its_hub.lms import OpenAICompatibleLanguageModel\n",
    "from its_hub.algorithms import BestOfN\n",
    "from its_hub.integration.reward_hub import LLMJudgeRewardModel\n",
    "\n",
    "# The generation model: same as Day 2\n",
    "lm = OpenAICompatibleLanguageModel(\n",
    "    endpoint=endpoint_base,\n",
    "    api_key=key,\n",
    "    model_name=\"granite-3-2-8b-instruct\",\n",
    "    temperature=0.7,  # Some variation across candidates\n",
    ")\n",
    "\n",
    "# The judge: a separate, larger model for evaluating candidates\n",
    "# Note: LLMJudgeRewardModel uses litellm internally, which requires\n",
    "# the \"openai/\" prefix to route to an OpenAI-compatible endpoint.\n",
    "judge = LLMJudgeRewardModel(\n",
    "    model=\"openai/qwen3-14b\",\n",
    "    criterion=\"overall_quality\",\n",
    "    judge_type=\"groupwise\",\n",
    "    api_key=key,\n",
    "    base_url=endpoint_base,\n",
    ")\n",
    "\n",
    "# Wire them together\n",
    "scaling_alg = BestOfN(judge)\n",
    "\n",
    "print(\"Generator: granite-3-2-8b-instruct\")\n",
    "print(\"Judge:     qwen3-14b (groupwise, overall_quality)\")\n",
    "print(\"Algorithm: Best-of-N\")\n",
    "print(\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a69da-2a41-47d7-a525-04102c08149b",
   "metadata": {},
   "source": [
    "Let's walk through what each piece does.\n",
    "\n",
    "`OpenAICompatibleLanguageModel` wraps any OpenAI-compatible API endpoint. It handles batched requests, retries, and concurrency internally. The `temperature=0.7` is important here. In Day 2 we used `temperature=0` for deterministic single-shot answers. Now we want variation. If every candidate is identical, there is nothing for the judge to choose between. A temperature of 0.7 gives the model enough freedom to explore different phrasings and reasoning paths while staying coherent.\n",
    "\n",
    "`LLMJudgeRewardModel` uses a second LLM to evaluate and rank candidate responses. The `criterion=\"overall_quality\"` tells the judge to evaluate on general quality rather than a narrow metric. The `judge_type=\"groupwise\"` means the judge sees all candidates at once and picks a winner, rather than scoring each one independently. This tends to produce better selections because the judge can compare directly.\n",
    "\n",
    "Two details on the judge configuration deserve attention.\n",
    "\n",
    "First, the `base_url` parameter. Without it, `LLMJudgeRewardModel` defaults to calling OpenAI's API. Setting base_url=endpoint_base routes the judge's calls through our MaaS endpoint instead. This is a common gotcha when using LLM-as-judge with custom serving infrastructure.\n",
    "\n",
    "Second, the `openai/` prefix on the model name. The `LLMJudgeRewardModel` uses `litellm` internally for its API calls, and litellm uses that prefix to determine which provider protocol to speak. Without it, litellm does not know that MaaS is an OpenAI-compatible endpoint and the call fails with a \"LLM Provider NOT provided\" error. The generator does not need this prefix because `OpenAICompatibleLanguageModel` makes direct HTTP requests to the endpoint, bypassing litellm entirely. This is a subtle but important difference between the two components. If you hit a provider routing error on the judge but the generator works fine, check the prefix first.\n",
    "\n",
    "`BestOfN` is the algorithm that ties them together. It takes the judge at construction time. When we call `scaling_alg.infer()`, it will generate N candidates using the language model, pass all of them to the judge, and return the one the judge ranked highest.\n",
    "\n",
    "If the cell runs without error, you should see the three confirmation lines printed. If you get a connection error, verify that your endpoint and API key are correct and that both `granite-3-2-8b-instruct` and `qwen3-14b` are in your MaaS subscriptions.\n",
    "\n",
    "## 1.3 Best-of-N with LLM Judge\n",
    "\n",
    "Now we apply this to the questions from Day 2. We will run all 10 questions through the Best-of-N pipeline so we can observe two things: whether the passing questions still pass (no regression), and whether the failing questions improve.\n",
    "\n",
    "\n",
    "### 1.3.1 Running the Target Questions Through Best-of-N\n",
    "\n",
    "First, we load the Day 2 results. This version of the evaluation data includes the retrieved context for each question so we can reconstruct the exact RAG prompts from Day 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a5d3651-46ae-4128-8ccf-bc8a8e9aa343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 questions from Day 2 evaluation\n",
      "Passes:   6\n",
      "Failures: 4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../prebuilt/eval_with_context.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "results_day2 = eval_data[\"results\"]\n",
    "\n",
    "print(f\"Loaded {len(results_day2)} questions from Day 2 evaluation\")\n",
    "print(f\"Passes:   {sum(1 for r in results_day2 if r['classification'] == 'pass')}\")\n",
    "print(f\"Failures: {sum(1 for r in results_day2 if r['classification'] != 'pass')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2ca37-9ce6-4450-aacc-5a59e2888938",
   "metadata": {},
   "source": [
    "Let's walk through the structure of this loop before running it.\n",
    "\n",
    "For each of the 10 questions, we build a prompt that includes the system instruction, the retrieved context from Day 2, and the question itself. This is the same prompt structure the model saw during the Day 2 evaluation. The only difference is what happens after the prompt is sent. Instead of a single generation call, `scaling_alg.infer()` sends 5 identical requests (the `budget`), collects the 5 candidate responses, passes them to the `qwen3-14b` judge, and returns the winner.\n",
    "\n",
    "The `return_response_only=False` flag tells `BestOfN` to return the full result object rather than just the winning answer. That gives us access to all 5 scores, which candidate was selected, and how many candidates were generated. We capture all of this for comparison in the next cell.\n",
    "\n",
    "Each question generates 5 candidate requests plus 1 judge request, so 10 questions means roughly 60 API calls total. \n",
    "\n",
    "Expect this to take 4 to 5 minutes depending on endpoint load, with individual questions ranging from 15 to 60 seconds. The variation is normal: longer answers and more complex comparisons take longer for the judge to evaluate.\n",
    "\n",
    "The cell below controls whether the loop runs live or loads previously saved results. For the first time through, leave `RUN_LIVE = True` and let it run. If you have already run it once, or if you need to move through the material faster, set it to `False` and the notebook will load the results from `../prebuilt/bon_results.json` instead. Either way, the analysis cells that follow work identically.\n",
    "\n",
    "\n",
    "> **Facilitator note**: If time is tight or the endpoint is slow, set RUN_LIVE = False and move on. This is expected behavior during the workshop, not a failure. The first clean run of the day should save results to the prebuilt directory, and every subsequent participant or session can load from there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "762b9833-4d50-4a6c-80b3-4a52cd2645a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to run Best-of-N live. Set to False to load saved results.\n",
    "RUN_LIVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce1daa36-ba72-4d68-9707-abca3de2d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING BEST-OF-N (budget=5) WITH LLM JUDGE\n",
      "======================================================================\n",
      "\n",
      "  [q01] What happens if a Thief fails an Open Locks attempt?...\n",
      "    Day 2: PASS\n",
      "    Best candidate: #3 of 5\n",
      "    Answer: If a Thief fails an Open Locks attempt, they must wait until they have gained another level of exper...\n",
      "    Time: 15.5s  (1/10)\n",
      "\n",
      "  [q02] Why can't Elves roll higher than a d6 for hit points?...\n",
      "    Day 2: FAIL\n",
      "    Best candidate: #1 of 5\n",
      "    Answer: According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. ...\n",
      "    Time: 24.6s  (2/10)\n",
      "\n",
      "  [q03] Can a character wear leather armor and cast spells?...\n",
      "    Day 2: PASS\n",
      "    Best candidate: #2 of 5\n",
      "    Answer: Yes, an Elf character can wear leather armor and cast spells. This is because Elves are a combinatio...\n",
      "    Time: 30.6s  (3/10)\n",
      "\n",
      "  [q04] What is the saving throw for a 3rd level Fighter against Dra...\n",
      "    Day 2: FAIL\n",
      "    Best candidate: #1 of 5\n",
      "    Answer: 3rd level Fighters have a saving throw of 15 against Dragon Breath. This is listed in the \"Dragon Br...\n",
      "    Time: 17.9s  (4/10)\n",
      "\n",
      "  [q05] How does a Cleric turn undead?...\n",
      "    Day 2: PASS\n",
      "    Best candidate: #2 of 5\n",
      "    Answer: A Cleric turns undead by looking up their level on the Clerics vs. Undead table and cross-referencin...\n",
      "    Time: 35.1s  (5/10)\n",
      "\n",
      "  [q06] If a character has a Strength of 16, what bonus do they get ...\n",
      "    Day 2: FAIL\n",
      "    Best candidate: #1 of 5\n",
      "    Answer: A character with a Strength of 16 gets a +2 bonus on melee attack rolls. This is determined by the a...\n",
      "    Time: 61.3s  (6/10)\n",
      "\n",
      "  [q07] What is the difference between a retainer and a hireling?...\n",
      "    Day 2: FAIL\n",
      "    Best candidate: #1 of 5\n",
      "    Answer: A retainer is a Non-Player Character who is a close associate and very loyal to their employer, will...\n",
      "    Time: 17.4s  (7/10)\n",
      "\n",
      "  [q08] When can a Magic-User learn new spells?...\n",
      "    Day 2: PASS\n",
      "    Best candidate: #1 of 5\n",
      "    Answer: A Magic-User can learn new spells at any point, but there are specific conditions. They must find a ...\n",
      "    Time: 18.9s  (8/10)\n",
      "\n",
      "  [q09] What happens to a character at exactly 0 hit points?...\n",
      "    Day 2: PASS\n",
      "    Best candidate: #3 of 5\n",
      "    Answer: At exactly 0 hit points, a character may be dead. However, this does not necessarily mean the end fo...\n",
      "    Time: 20.0s  (9/10)\n",
      "\n",
      "  [q10] Can a Halfling use a longbow?...\n",
      "    Day 2: PASS\n",
      "    Best candidate: #4 of 5\n",
      "    Answer: No, a Halfling cannot use a longbow. The context states that Halflings must use Medium weapons two-h...\n",
      "    Time: 32.2s  (10/10)\n",
      "\n",
      "======================================================================\n",
      "Best-of-N complete. Total time: 273.4s (4.6 min)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from its_hub.utils import extract_content_from_lm_response\n",
    "\n",
    "if RUN_LIVE:\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"Answer the question using only the provided context. \"\n",
    "        \"Be specific and cite rules where possible.\"\n",
    "    )\n",
    "\n",
    "    BUDGET = 5\n",
    "\n",
    "    bon_results = []\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RUNNING BEST-OF-N (budget={BUDGET}) WITH LLM JUDGE\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    for i, r in enumerate(results_day2):\n",
    "        print(f\"\\n  [{r['id']}] {r['question'][:60]}...\")\n",
    "\n",
    "        context = r.get(\"retrieved_context\", \"\")\n",
    "        prompt = f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {r['question']}\n",
    "Answer:\"\"\"\n",
    "\n",
    "        q_start = time.time()\n",
    "        result = scaling_alg.infer(\n",
    "            lm, prompt, budget=BUDGET, return_response_only=False\n",
    "        )\n",
    "        q_elapsed = time.time() - q_start\n",
    "\n",
    "        best_answer = extract_content_from_lm_response(result.the_one)\n",
    "\n",
    "        bon_results.append({\n",
    "            \"id\": r[\"id\"],\n",
    "            \"question\": r[\"question\"],\n",
    "            \"expected\": r[\"expected\"],\n",
    "            \"category\": r[\"category\"],\n",
    "            \"day2_answer\": r[\"answer\"],\n",
    "            \"day2_classification\": r[\"classification\"],\n",
    "            \"bon_answer\": best_answer,\n",
    "            \"bon_scores\": result.scores,\n",
    "            \"bon_selected_index\": result.selected_index,\n",
    "            \"bon_n_candidates\": len(result.responses),\n",
    "        })\n",
    "\n",
    "        day2_status = \"PASS\" if r[\"classification\"] == \"pass\" else \"FAIL\"\n",
    "        print(f\"    Day 2: {day2_status}\")\n",
    "        print(f\"    Best candidate: #{result.selected_index + 1} of {len(result.responses)}\")\n",
    "        print(f\"    Answer: {best_answer[:100]}...\")\n",
    "        print(f\"    Time: {q_elapsed:.1f}s  ({i+1}/{len(results_day2)})\")\n",
    "\n",
    "    total_elapsed = time.time() - total_start\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Best-of-N complete. Total time: {total_elapsed:.1f}s ({total_elapsed/60:.1f} min)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "else:\n",
    "    with open(\"../prebuilt/bon_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        bon_results = json.load(f)\n",
    "    BUDGET = bon_results[0][\"bon_n_candidates\"]\n",
    "    print(f\"Loaded {len(bon_results)} pre-built Best-of-N results (budget={BUDGET})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c27ffc4-7c85-41d8-95a9-625e61270a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 results to ../prebuilt/bon_results.json\n",
      "Saved 10 results to ./bon_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save results only if we ran live\n",
    "if RUN_LIVE:\n",
    "    with open(\"../prebuilt/bon_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bon_results, f, indent=2)\n",
    "\n",
    "    with open(\"bon_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bon_results, f, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(bon_results)} results to ../prebuilt/bon_results.json\")\n",
    "    print(f\"Saved {len(bon_results)} results to ./bon_results.json\")\n",
    "else:\n",
    "    print(\"Using pre-built results. Nothing to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2ab5e-e475-4020-9680-e46098267585",
   "metadata": {},
   "source": [
    "### 1.3.2 comparison. Here are the cells in order:\n",
    "\n",
    "\n",
    "Now we put the results side by side. Questions that failed on Day 2 are marked with `>>` so they stand out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2dd8a60-25ed-4b4c-b080-6c2746f284e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARISON: DAY 2 SINGLE-SHOT vs. BEST-OF-N\n",
      "======================================================================\n",
      "\n",
      "   [q01] What happens if a Thief fails an Open Locks attempt?\n",
      "   Category:     explicit_rule\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     If a Thief fails an Open Locks attempt, they must wait until they have gained another level of experience before trying again.\n",
      "\n",
      "   Best-of-N answer (#3 of 5):\n",
      "     If a Thief fails an Open Locks attempt, they must wait until they have gained another level of experience before trying again. This rule is stated in \n",
      "   Scores: [0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      ">> [q02] Why can't Elves roll higher than a d6 for hit points?\n",
      "   Category:     terminology\n",
      "   Day 2 status: implicit_reasoning_failure\n",
      "\n",
      "   Day 2 answer:\n",
      "     According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. The reason for this restriction is not explicitly \n",
      "\n",
      "   Best-of-N answer (#1 of 5):\n",
      "     According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. This is a restriction specific to Elves and is not\n",
      "   Scores: [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      "   [q03] Can a character wear leather armor and cast spells?\n",
      "   Category:     implicit_reasoning\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     Yes, according to the provided context, characters who can cast spells may wear leather armor.\n",
      "\n",
      "   Best-of-N answer (#2 of 5):\n",
      "     Yes, an Elf character can wear leather armor and cast spells. This is because Elves are a combination class that advances as both Fighters and Magic-U\n",
      "   Scores: [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      ">> [q04] What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "   Category:     table_lookup\n",
      "   Day 2 status: implicit_reasoning_failure\n",
      "\n",
      "   Day 2 answer:\n",
      "     The context does not provide specific information on the saving throw for a 3rd level Fighter against Dragon Breath. However, it does mention that cha\n",
      "\n",
      "   Best-of-N answer (#1 of 5):\n",
      "     3rd level Fighters have a saving throw of 15 against Dragon Breath. This is listed in the \"Dragon Breath\" column of the Fighter class table for levels\n",
      "   Scores: [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      "   [q05] How does a Cleric turn undead?\n",
      "   Category:     multi_step_rule\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     To turn undead, the GM looks up the Cleric's level on the Clerics vs. Undead table and cross-references it with the undead type or Hit Dice. If the ta\n",
      "\n",
      "   Best-of-N answer (#2 of 5):\n",
      "     A Cleric turns undead by looking up their level on the Clerics vs. Undead table and cross-referencing it with the undead type or Hit Dice. If the tabl\n",
      "   Scores: [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      ">> [q06] If a character has a Strength of 16, what bonus do they get on melee attack rolls?\n",
      "   Category:     table_lookup\n",
      "   Day 2 status: implicit_reasoning_failure\n",
      "\n",
      "   Day 2 answer:\n",
      "     The provided context does not specify how Strength affects melee attack rolls. In Basic Fantasy RPG, it is recommended not to waste time in detailing \n",
      "\n",
      "   Best-of-N answer (#1 of 5):\n",
      "     A character with a Strength of 16 gets a +2 bonus on melee attack rolls. This is determined by the ability bonus or penalty for Strength, which is +1 \n",
      "   Scores: [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      ">> [q07] What is the difference between a retainer and a hireling?\n",
      "   Category:     terminology\n",
      "   Day 2 status: implicit_reasoning_failure\n",
      "\n",
      "   Day 2 answer:\n",
      "     In the Basic Fantasy RPG system, a retainer and a hireling are both types of NPCs that can be hired by player characters. However, the key difference \n",
      "\n",
      "   Best-of-N answer (#1 of 5):\n",
      "     A retainer is a Non-Player Character who is a close associate and very loyal to their employer, willing to take reasonable risks and accompany them in\n",
      "   Scores: [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      "   [q08] When can a Magic-User learn new spells?\n",
      "   Category:     implicit_reasoning\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     A Magic-User can learn new spells at any point, but there are specific conditions. They must find a teacher or acquire a reference work, such as anoth\n",
      "\n",
      "   Best-of-N answer (#1 of 5):\n",
      "     A Magic-User can learn new spells at any point, but there are specific conditions. They must find a teacher or acquire a reference work, such as anoth\n",
      "   Scores: [1.0, 1.0, 1.0, 0.0, 1.0]\n",
      "   ---\n",
      "\n",
      "   [q09] What happens to a character at exactly 0 hit points?\n",
      "   Category:     explicit_rule\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     When a character's hit point total reaches exactly 0, the character may be dead. However, this is not necessarily the end for the character; the chara\n",
      "\n",
      "   Best-of-N answer (#3 of 5):\n",
      "     At exactly 0 hit points, a character may be dead. However, this does not necessarily mean the end for the character. The text suggests that further ac\n",
      "   Scores: [0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "   ---\n",
      "\n",
      "   [q10] Can a Halfling use a longbow?\n",
      "   Category:     implicit_reasoning\n",
      "   Day 2 status: pass\n",
      "\n",
      "   Day 2 answer:\n",
      "     No, according to the provided context, Halflings may not use Large weapons, and longbows are typically around 6 feet unstrung, which would be consider\n",
      "\n",
      "   Best-of-N answer (#4 of 5):\n",
      "     No, a Halfling cannot use a longbow. The context states that Halflings must use Medium weapons two-handed and may not use Large weapons. The longbow i\n",
      "   Scores: [0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "   ---\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARISON: DAY 2 SINGLE-SHOT vs. BEST-OF-N\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for br in bon_results:\n",
    "    day2_status = \"PASS\" if br[\"day2_classification\"] == \"pass\" else \"FAIL\"\n",
    "    marker = \"  \" if day2_status == \"PASS\" else \">>\"\n",
    "\n",
    "    print(f\"\\n{marker} [{br['id']}] {br['question']}\")\n",
    "    print(f\"   Category:     {br['category']}\")\n",
    "    print(f\"   Day 2 status: {br['day2_classification']}\")\n",
    "    print()\n",
    "    print(f\"   Day 2 answer:\")\n",
    "    print(f\"     {br['day2_answer'][:150]}\")\n",
    "    print()\n",
    "    print(f\"   Best-of-N answer (#{br['bon_selected_index']+1} of {br['bon_n_candidates']}):\")\n",
    "    print(f\"     {br['bon_answer'][:150]}\")\n",
    "    print(f\"   Scores: {br['bon_scores']}\")\n",
    "    print(f\"   {'---'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60a9f7-472e-4752-aacc-fecae00a4252",
   "metadata": {},
   "source": [
    "Read through the output carefully. For each question, ask yourself: did Best-of-N produce a better answer than single-shot?\n",
    "\n",
    "If yes, the model had the capability. It just did not surface it on the first try. This is a sampling win, not a knowledge win. The model's weights already contain something useful, and multiple attempts with a good judge found it.\n",
    "\n",
    "If no, the model tried 5 times, a separate larger judge picked the best attempt, and it is still wrong. That means the failure is not about sampling luck. The model genuinely struggles with this class of question.\n",
    "\n",
    "A note about the scores. Because we are using a groupwise judge, the scores are binary: `1.0` for the selected response and `0.0` for the others. The judge is making a ranking decision, not assigning granular scores. If you see all responses scoring identically, it means even the judge could not differentiate them, which itself is a signal.\n",
    "\n",
    "### 1.3.3 Where It Helps and Where It Doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00e1e06f-1581-463b-b5dc-2c151c24e919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BEST-OF-N IMPACT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "  Previously passing: 6 questions\n",
      "  Previously failing: 4 questions\n",
      "\n",
      "  --- Previously Failing Questions ---\n",
      "\n",
      "  [q02] Why can't Elves roll higher than a d6 for hit points?\n",
      "    Category:  terminology\n",
      "    Expected:  Elves use a d6 for hit points because that is the hit die assigned to the Elf combination class in Basic Fantasy RPG.\n",
      "    Day 2:     According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. The reason for this \n",
      "    Best-of-N: According to the provided context, Elves never roll larger than six-sided dice (d6) for hit points. This is a restrictio\n",
      "    Scores:    [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "  [q04] What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "    Category:  table_lookup\n",
      "    Expected:  Based on the Fighter saving throw table, a 3rd level Fighter has a Dragon Breath saving throw of 15.\n",
      "    Day 2:     The context does not provide specific information on the saving throw for a 3rd level Fighter against Dragon Breath. How\n",
      "    Best-of-N: 3rd level Fighters have a saving throw of 15 against Dragon Breath. This is listed in the \"Dragon Breath\" column of the \n",
      "    Scores:    [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "  [q06] If a character has a Strength of 16, what bonus do they get on melee attack rolls?\n",
      "    Category:  table_lookup\n",
      "    Expected:  A Strength score of 16 gives a +2 bonus, which applies to melee attack rolls and damage rolls.\n",
      "    Day 2:     The provided context does not specify how Strength affects melee attack rolls. In Basic Fantasy RPG, it is recommended n\n",
      "    Best-of-N: A character with a Strength of 16 gets a +2 bonus on melee attack rolls. This is determined by the ability bonus or pena\n",
      "    Scores:    [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "  [q07] What is the difference between a retainer and a hireling?\n",
      "    Category:  terminology\n",
      "    Expected:  Retainers are NPCs who accompany the party on adventures and gain experience. Hirelings are hired for specific non-adven\n",
      "    Day 2:     In the Basic Fantasy RPG system, a retainer and a hireling are both types of NPCs that can be hired by player characters\n",
      "    Best-of-N: A retainer is a Non-Player Character who is a close associate and very loyal to their employer, willing to take reasonab\n",
      "    Scores:    [1.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BEST-OF-N IMPACT ANALYSIS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "day2_failures = [br for br in bon_results if br[\"day2_classification\"] != \"pass\"]\n",
    "day2_passes = [br for br in bon_results if br[\"day2_classification\"] == \"pass\"]\n",
    "\n",
    "print(f\"\\n  Previously passing: {len(day2_passes)} questions\")\n",
    "print(f\"  Previously failing: {len(day2_failures)} questions\")\n",
    "\n",
    "print(f\"\\n  --- Previously Failing Questions ---\")\n",
    "for br in day2_failures:\n",
    "    print(f\"\\n  [{br['id']}] {br['question']}\")\n",
    "    print(f\"    Category:  {br['category']}\")\n",
    "    print(f\"    Expected:  {br['expected'][:120]}\")\n",
    "    print(f\"    Day 2:     {br['day2_answer'][:120]}\")\n",
    "    print(f\"    Best-of-N: {br['bon_answer'][:120]}\")\n",
    "    print(f\"    Scores:    {br['bon_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3aa174-1854-4ed6-95bb-312c5f6a04fc",
   "metadata": {},
   "source": [
    "> **Facilitator note:** This is a discussion moment, not just a code output. Walk the room through each of the 4 previously failing questions and ask:\n",
    "> \"Is this answer better? Is it correct? Is it good enough?\"\n",
    "\n",
    "Point participants to the specific patterns.\n",
    "\n",
    "**q04 (saving throw table lookup):** Day 2 said \"the context does not provide specific information.\" Best-of-N returned the correct answer: 15. One of the 5 candidates actually read the table. The judge picked it. This is the textbook sampling win. The model can do it. It just does not do it reliably on the first try.\n",
    "\n",
    "**q07 (retainer vs hireling):** Day 2 hedged with \"the context does not provide specific details about hirelings.\" Best-of-N correctly states that hirelings are normal people hired for specific services who do not go on adventures. One of the 5 candidates used both sections of the retrieved context. The judge picked it.\n",
    "\n",
    "**q06 (Strength bonus):** The answer lands on \"+2\" which is the correct number, but look at the reasoning. It says \"as per the standard D&D rules\" or refers to an ability score table \"not provided in the context.\" The model guessed correctly from its general training data, not from the retrieved context. In a customer setting where answers must be grounded in their documents, that is still a failure. The right number for the wrong reason is not trustworthy.\n",
    "\n",
    "**q02 (Elf hit dice):** All 5 candidates produced the same hedge: \"The reason for this restriction is not explicitly stated in the context.\" The model consistently cannot infer the \"why\" from the text. Five attempts, same wall. This is the clearest signal that the failure is systematic, not stochastic.\n",
    "\n",
    "The scorecard: \n",
    "\n",
    "* 2 clear wins (q04, q07),\n",
    "* 1 false positive (q06, right answer from wrong source),\n",
    "* 1 persistent failure (q02).\n",
    "\n",
    "Inference-time scaling helped but did not solve everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c84d1f-a83b-4068-98b0-aa16269f2422",
   "metadata": {},
   "source": [
    "## 1.4 Other Inference-Time Scaling Strategies (Conceptual Only)\n",
    "\n",
    "Best-of-N is the simplest algorithm in `its_hub`. There are others, and they are worth understanding conceptually even though we will not implement them all today. The point is not to teach every algorithm. It is to show that inference-time scaling is a family of techniques with different tradeoffs, and Best-of-N is just the entry point.\n",
    "\n",
    "### 1.4.1 Self-Consistency\n",
    "\n",
    "Self-consistency generates multiple reasoning paths and selects the answer that appears most frequently. Instead of using a judge to evaluate quality, it uses agreement as a signal. If 4 out of 5 attempts arrive at the same answer, that answer is probably more reliable than one that only appears once.\n",
    "\n",
    "The implementation in `its_hub` takes a **projection function** that extracts the \"answer\" from each response. For math problems, that might mean pulling a number out of a paragraph of reasoning. It then votes on the projected answers. The response whose projected answer matches the majority is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49923080-ad91-40c8-8610-ade6e083fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual only. Do not run this cell.\n",
    "# Shown to illustrate the Self-Consistency API pattern.\n",
    "\n",
    "from its_hub.algorithms import SelfConsistency\n",
    "\n",
    "# For math: extract the boxed answer and vote\n",
    "# sc = SelfConsistency(lambda s: extract_boxed(s))\n",
    "# result = sc.infer(lm, prompt, budget=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90656a6-77c6-46ac-81dd-0d545bc94dae",
   "metadata": {},
   "source": [
    "Self-consistency works well when the model can reason through the problem but occasionally takes a wrong turn. The majority vote filters out the unlucky paths. It works poorly when the model consistently reasons in the same incorrect direction, because all 5 attempts will agree on the wrong answer.\n",
    "\n",
    "For our use case, self-consistency is less useful because our questions have free-text answers, not extractable numeric results. There is no clean projection function for \"explain the difference between a retainer and a hireling.\" The algorithm needs a way to determine when two answers are \"the same,\" and for open-ended text that comparison is not straightforward.\n",
    "\n",
    "### 1.4.2 Particle Filtering and Beam Search\n",
    "\n",
    "These approaches operate at the token or step level rather than the response level. Instead of generating complete responses and selecting among them, they maintain multiple partial generations simultaneously and prune weak reasoning paths as they go.\n",
    "\n",
    "Particle Filtering treats generation as a search problem: generate one reasoning step at a time, score each partial solution with a Process Reward Model, resample the population toward higher-scoring paths, and continue. Think of it as Best-of-N applied at every sentence rather than at the end. `its_hub` includes both standard and entropic variants.\n",
    "\n",
    "Beam Search maintains the top-K partial sequences at each step and only expands from those. It is more aggressive about pruning than Particle Filtering, keeping only the highest-scoring paths alive at each step.\n",
    "\n",
    "Both require a **Process Reward Model** (PRM), which scores intermediate reasoning steps, not just final answers. That means they need `pip install its_hub[prm]` and a local GPU to run the PRM. They are most relevant when you control the serving infrastructure and can run reward models locally.\n",
    "\n",
    "**Why this matters for the field:** When a customer asks \"what else can we try before fine-tuning,\" you now have a layered answer. Best-of-N works with any API endpoint and no extra infrastructure. Self-consistency works when answers are extractable and comparable. Particle Filtering and Beam Search offer the most control but require GPU resources for the reward model. Each is a step up in infrastructure complexity, and each is still cheaper than model training.\n",
    "\n",
    "> **Facilitator note:** Keep this conceptual. Do not attempt to run Particle Filtering or Beam Search in this lab. The point is awareness, not implementation. If participants ask about these in depth, point them to the `its_hub` documentation and the quick-start examples in the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b771738-2599-4ea7-bdf0-307305d2a3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
