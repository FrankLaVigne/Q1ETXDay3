{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Adaptation: Changing What the Model Knows\n",
    "\n",
    "> **GPU Required.** This section requires a CUDA-capable GPU. The lab environment provides an NVIDIA L4 with 24GB of VRAM. If you are running this outside the lab without GPU access, you can follow the setup and data preparation cells, but training will not execute. Pre-built outputs are provided for that scenario.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "We have now exhausted the options that do not involve changing the model. RAG gave us grounded answers for 6 out of 10 questions. Inference-time scaling (Best-of-N) recovered 2 more. But for the remaining failures, particularly questions requiring implicit domain reasoning, every approach hit the same wall: the model's weights do not contain a reliable path to the correct answer.\n",
    "\n",
    "This is where model adaptation becomes justified. Not because it is the next thing on the list, but because two independent evaluation methods pointed to the same conclusion: the gap is in the model, not in the pipeline.\n",
    "\n",
    "We will use **LoRA SFT (Low-Rank Adaptation with Supervised Fine-Tuning)**, a parameter-efficient method that adds small trainable matrices alongside the frozen base model weights. The base model itself is not modified. Instead, LoRA learns a compact set of adjustments that shift the model's behavior toward our domain without destroying what it already knows.\n",
    "\n",
    "To fit an 8-billion parameter model into 24GB of GPU memory, we load the base weights in **4-bit quantization** using `bitsandbytes`. This reduces the model's memory footprint from roughly 16GB (in half precision) to around 5GB, leaving room for LoRA's trainable parameters, optimizer states, and activations. This combination of LoRA with 4-bit quantization is commonly known as **QLoRA**.\n",
    "\n",
    "The training data comes directly from Section 2: synthetic question-answer pairs generated from the customer's own documents. The model comes from HuggingFace. The training runs locally on GPU.\n",
    "\n",
    "By the end of this section, you will have an adapted model and evidence of whether the adaptation closed the gaps that RAG and inference-time scaling could not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Install Training Hub\n",
    "\n",
    "`training_hub` is an open-source library from the Red Hat AI Innovation Team. It provides a single Python interface for multiple post-training algorithms, each backed by a community implementation. You call a function, pass your model and data, and the library handles the rest.\n",
    "\n",
    "We install three extras: `[cuda]` for GPU dependencies including PyTorch with CUDA support, `[lora]` for the LoRA backend, and `bitsandbytes` for 4-bit quantization support.\n",
    "\n",
    "_This step typically takes 3 to 5 minutes. Package installation involves compiling several native extensions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training-hub with CUDA and LoRA support\n",
    "! pip install training-hub -q\n",
    "! pip install 'training-hub[cuda]' --no-build-isolation -q\n",
    "! pip install 'training-hub[lora]' -q\n",
    "\n",
    "# bitsandbytes provides 4-bit quantization, required to fit 8B models in 24GB VRAM\n",
    "! pip install bitsandbytes -q\n",
    "\n",
    "# Remove mamba-ssm and causal-conv1d: these ship pre-compiled CUDA extensions\n",
    "# built against an older PyTorch ABI, causing \"undefined symbol\" errors on import.\n",
    "# They are only needed for Mamba-architecture models. Granite is a transformer,\n",
    "# so removing them is safe. Unsloth handles their absence gracefully.\n",
    "! pip uninstall mamba-ssm causal-conv1d -y 2>/dev/null || true\n",
    "\n",
    "# Patch Unsloth compiler: peft 0.18+ added VARIANT_KWARG_KEYS to Linear forward(),\n",
    "# but Unsloth's code generator only imports symbols from the parent module, missing\n",
    "# constants defined elsewhere. This adds a fallback import for the missing constant.\n",
    "import unsloth_zoo.compiler, inspect, re\n",
    "_src = inspect.getsource(unsloth_zoo.compiler.create_new_function)\n",
    "if \"VARIANT_KWARG_KEYS\" not in _src:\n",
    "    _old = '    new_source = imports + \"\\\\n\\\\n\" + new_source'\n",
    "    _patch = (\n",
    "        '    if \"VARIANT_KWARG_KEYS\" in new_source and \"VARIANT_KWARG_KEYS\" not in items:\\n'\n",
    "        '        imports += \"\\\\ntry:\\\\n    from peft.tuners.lora.layer import VARIANT_KWARG_KEYS\\\\n'\n",
    "        'except ImportError:\\\\n    VARIANT_KWARG_KEYS = []\\\\n\"\\n'\n",
    "    )\n",
    "    _src = _src.replace(_old, _patch + _old, 1)\n",
    "    exec(compile(_src, unsloth_zoo.compiler.__file__, \"exec\"), unsloth_zoo.compiler.__dict__)\n",
    "    print(\"Applied VARIANT_KWARG_KEYS patch to Unsloth compiler\")\n",
    "else:\n",
    "    print(\"Unsloth compiler already handles VARIANT_KWARG_KEYS\")\n",
    "\n",
    "# Clear any stale compiled cache from previous failed runs\n",
    "import shutil, os\n",
    "_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(_cache):\n",
    "    shutil.rmtree(_cache)\n",
    "    print(f\"Cleared stale compiled cache at {_cache}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You may see dependency conflict warnings from pip. These are advisory, not fatal. The lab environment has many pre-installed packages (Docling, KFP, Feast, etc.) that pin older versions of shared dependencies like `transformers` and `click`. As long as the import in the next cell succeeds, the conflicts do not affect training_hub.\n",
    "\n",
    "If `flash-attn` fails to build, that is also acceptable. Training will fall back to standard attention. Flash-attention is a performance optimization, not a requirement.\n",
    "\n",
    "The install cell applies two compatibility fixes for this environment:\n",
    "\n",
    "1. **Removes `mamba-ssm` and `causal-conv1d`**: These packages contain pre-compiled CUDA extensions that are binary-incompatible with PyTorch 2.10.0+cu128, causing an `undefined symbol` error when Unsloth tries to import them. Since they are only used for Mamba-architecture models and Granite is a transformer, removing them is safe.\n",
    "\n",
    "2. **Patches the Unsloth compiler**: PEFT 0.18+ introduced a `VARIANT_KWARG_KEYS` constant used in LoRA Linear layer forward methods. Unsloth's code generator extracts these methods but only auto-imports symbols from the immediate parent module. When the constant is defined in a different submodule (`peft.tuners.lora.layer` vs `peft.tuners.lora.inc`), the generated code fails with a `NameError`. The patch adds a fallback import for this constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Verify the Installation\n",
    "\n",
    "A quick import check confirms that `training_hub` is installed and can enumerate its available algorithms. If this cell fails, revisit the install step above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_hub imported successfully\n",
      "Available algorithms: ['sft', 'osft', 'lora_sft']\n"
     ]
    }
   ],
   "source": [
    "# Verify the install\n",
    "from training_hub import AlgorithmRegistry\n",
    "print(\"training_hub imported successfully\")\n",
    "print(f\"Available algorithms: {AlgorithmRegistry.list_algorithms()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Environment Setup\n",
    "\n",
    "Same credentials, same endpoint pattern. We reuse the `.env` file and config helper from earlier sections. The API key and endpoint are needed later when we compare the adapted model's outputs against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: https://litellm-prod.apps.maas.redhatworkshops.io/v1\n",
      "API Key:  sk-UFHcL...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from config import API_KEY as key, ENDPOINT_BASE as endpoint_base\n",
    "\n",
    "print(f\"Endpoint: {endpoint_base}\")\n",
    "print(f\"API Key:  {key[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Verify GPU Access\n",
    "\n",
    "Before we go any further, we confirm that PyTorch can see the GPU and report its memory. The L4 should show approximately 24GB. If no GPU is detected, training cells will not execute, but you can still follow the data preparation steps and use pre-built outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available:  True\n",
      "GPU:             NVIDIA L4\n",
      "Memory:          23.6 GB\n",
      "NOTE: 4-bit quantization (QLoRA) will be used to fit the 8B model in available memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/torch/cuda/__init__.py:1007: UserWarning: Can't initialize NVML\n",
      "  raw_cnt = _raw_device_count_nvml()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU:             {gpu_name}\")\n",
    "    print(f\"Memory:          {gpu_mem_gb:.1f} GB\")\n",
    "    if gpu_mem_gb < 20:\n",
    "        print(\"WARNING: Less than 20GB VRAM. Training may fail even with 4-bit quantization.\")\n",
    "    elif gpu_mem_gb < 40:\n",
    "        print(\"NOTE: 4-bit quantization (QLoRA) will be used to fit the 8B model in available memory.\")\n",
    "    else:\n",
    "        print(\"NOTE: Sufficient memory for full-precision LoRA. 4-bit quantization is optional.\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training cells will not execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** LoRA training on an 8B model with 4-bit quantization requires approximately 12 to 18GB of GPU memory depending on batch size and sequence length. The L4 provides 24GB, which gives us reasonable headroom.\n",
    "\n",
    "If you ran Sections 1 or 2 in this same kernel session, embedding models or other objects may still be occupying GPU memory. Before continuing, **restart your kernel** (Kernel > Restart) and then run only the cells in this section from the top. This clears GPU memory for training.\n",
    "\n",
    "You do not need to re-run Sections 1 or 2. The training data file (`synthetic_qa_pairs.csv`) is already saved to disk from Section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Discover Available Algorithms\n",
    "\n",
    "Before writing any training code, we ask the library what it supports. This is the same discovery-first pattern we used with `sdg_hub` in Section 2. Rather than guessing at function names or reading source code, we let the registry tell us what is available and what each algorithm requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available algorithms: ['sft', 'osft', 'lora_sft']\n",
      "  sft: backends = ['instructlab-training']\n",
      "  osft: backends = ['mini-trainer']\n",
      "  lora_sft: backends = ['unsloth']\n"
     ]
    }
   ],
   "source": [
    "from training_hub import AlgorithmRegistry\n",
    "\n",
    "algorithms = AlgorithmRegistry.list_algorithms()\n",
    "print(\"Available algorithms:\", algorithms)\n",
    "\n",
    "for algo_name in algorithms:\n",
    "    backends = AlgorithmRegistry.list_backends(algo_name)\n",
    "    print(f\"  {algo_name}: backends = {backends}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see three algorithms: `sft`, `osft`, and `lora_sft`. Each maps to one or more backend implementations.\n",
    "\n",
    "**SFT (Supervised Fine-Tuning)** updates all model weights. It is the most straightforward approach but also the most destructive: the model can lose general capability while learning domain-specific behavior. It is also the most memory-intensive, requiring well over 24GB for an 8B model. That puts it out of reach on our hardware.\n",
    "\n",
    "**OSFT (Orthogonal Subspace Fine-Tuning)** is more surgical. It analyzes the model's weight matrices via singular value decomposition, identifies which directions are most critical to the model's existing behavior, freezes those, and only updates the orthogonal (least critical) directions. However, OSFT requires the full model in memory at half precision (roughly 16GB just for weights), plus optimizer states and activations. On a 24GB GPU, this does not leave sufficient margin.\n",
    "\n",
    "**LoRA SFT (Low-Rank Adaptation)** adds small trainable matrices alongside the frozen base weights. Because the base weights are frozen, they can be loaded in a quantized format (4-bit), dramatically reducing memory requirements. This makes LoRA the practical choice for our hardware.\n",
    "\n",
    "We use LoRA because it fits our hardware constraints and is the most widely adopted parameter-efficient method in the field. When a customer asks \"will fine-tuning break what the model already does well?\", LoRA gives you a concrete answer: the base weights are never modified. The adapter learns adjustments that sit on top of the original model.\n",
    "\n",
    "### 3.3.1 Inspect LoRA Parameters\n",
    "\n",
    "Let's see what the LoRA algorithm requires and what options it exposes. This tells us what decisions we actually need to make versus what the library handles for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required parameters:\n",
      "  model_path: <class 'str'>\n",
      "  data_path: <class 'str'>\n",
      "  ckpt_output_dir: <class 'str'>\n",
      "\n",
      "Optional parameters (56 total):\n",
      "  num_epochs: <class 'int'>\n",
      "  effective_batch_size: <class 'int'>\n",
      "  learning_rate: <class 'float'>\n",
      "  max_seq_len: <class 'int'>\n",
      "  max_tokens_per_gpu: <class 'int'>\n",
      "  data_output_dir: <class 'str'>\n",
      "  save_samples: <class 'int'>\n",
      "  warmup_steps: <class 'int'>\n",
      "  accelerate_full_state_at_epoch: <class 'bool'>\n",
      "  checkpoint_at_epoch: <class 'bool'>\n",
      "  nproc_per_node: typing.Union[str, int]\n",
      "  nnodes: <class 'int'>\n",
      "  node_rank: <class 'int'>\n",
      "  rdzv_id: typing.Union[str, int]\n",
      "  rdzv_endpoint: <class 'str'>\n",
      "  ... and 41 more\n"
     ]
    }
   ],
   "source": [
    "from training_hub import create_algorithm\n",
    "\n",
    "lora_algo = create_algorithm('lora_sft')\n",
    "\n",
    "required_params = lora_algo.get_required_params()\n",
    "print(\"Required parameters:\")\n",
    "for k, v in required_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print()\n",
    "optional_params = lora_algo.get_optional_params()\n",
    "print(f\"Optional parameters ({len(optional_params)} total):\")\n",
    "for k, v in list(optional_params.items())[:15]:\n",
    "    print(f\"  {k}: {v}\")\n",
    "if len(optional_params) > 15:\n",
    "    print(f\"  ... and {len(optional_params) - 15} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required parameters are minimal: a model path, a data path, and an output directory.\n",
    "\n",
    "Everything else has sensible defaults. This is intentional.\n",
    "\n",
    "Take a moment to appreciate what this means.\n",
    "\n",
    "Under the hood, LoRA is decomposing weight update matrices into low-rank factors, inserting trainable adapter layers at strategic points in an 8-billion parameter model, managing 4-bit quantization of the frozen base, configuring gradient computation only for the adapter weights, handling mixed-precision training, and coordinating all of it within a 24GB memory budget. That is a significant amount of engineering.\n",
    "\n",
    "And the interface is three required arguments and a function call.\n",
    "\n",
    "The library absorbs the complexity of backend configuration, quantization setup, and checkpointing so that the interface stays focused on the decisions that actually matter: which model, which data, which hyperparameters. This is the same design philosophy behind `sdg_hub` and `its_hub`. The field should not need a PhD in optimization theory to run a training job. They need to make good decisions about inputs and interpret the outputs. The tooling handles the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Prepare Training Data\n",
    "\n",
    "We take the synthetic QA pairs from Section 2 and convert them into the format the training pipeline expects. The standard for chat-based fine-tuning is JSONL where each line contains a `messages` array representing one training conversation.\n",
    "\n",
    "If Section 2 was not completed (or the CSV file is missing), this cell provides fallback training data drawn from the Basic Fantasy RPG rules. In a full workshop run, the synthetic data generation step produces this file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 QA pairs\n",
      "Columns: ['question', 'response', 'faithfulness_judgment']\n",
      "\n",
      "  Q1: What is the Thief's Open Locks ability score at level 8?\n",
      "  A1: At level 8, the Thief's Open Locks ability score is 60. This information is found in the Thief Abili...\n",
      "\n",
      "  Q2: How does the Thief's Move Silently ability progress from level 1 to level 15?\n",
      "  A2: Based on the provided document, the Thief's \"Move Silently\" ability progresses as follows from level...\n",
      "\n",
      "  Q3: Can a GM adjust the Thief's ability scores, and for what reason?\n",
      "  A3: Yes, a Game Master (GM) can adjust a Thief's ability scores in certain situations. The document spec...\n",
      "\n",
      "  Q4: Why might a GM apply a situational adjustment when a Thief is climbing a wall?\n",
      "  A4: A Game Master (GM) might apply a situational adjustment when a Thief is climbing a wall to account f...\n",
      "\n",
      "  Q5: What is the Thief's Climb Walls ability score at level 17?\n",
      "  A5: At level 17, the Thief's Climb Walls ability score is 96. This can be found in the Thief Abilities t...\n",
      "\n",
      "  Q6: If a Thief is level 10, what is their Pick Pockets ability score?\n",
      "  A6: If a Thief is level 10, their Pick Pockets ability score is 74. According to the provided table, thi...\n",
      "\n",
      "  Q7: What does a Thief gain when they advance from level 3 to level 4?\n",
      "  A7: When a Thief advances from level 3 to level 4, they gain the following increases in their abilities:...\n",
      "\n",
      "  Q8: Describe a situation where a GM might apply a bonus to a Thief's ability score.\n",
      "  A8: In a typical gameplay scenario, a Dungeon Master (GM) might apply a bonus to a Thief's ability score...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the SDG output from Section 2\n",
    "sdg_path = \"../02SyntheticDataGen/synthetic_qa_pairs.csv\"\n",
    "\n",
    "if not os.path.exists(sdg_path):\n",
    "    print(f\"NOTE: {sdg_path} not found.\")\n",
    "    print(\"Using fallback training data from the handbook's sample questions.\")\n",
    "    print(\"In a full run, Section 2 (Synthetic Data Generation) produces this file.\")\n",
    "    print()\n",
    "    fallback_data = [\n",
    "        {\n",
    "            \"question\": \"Why can't Elves roll higher than a d6 for hit points?\",\n",
    "            \"response\": (\n",
    "                \"In Basic Fantasy RPG, Elves use a d6 for hit points because they are a \"\n",
    "                \"combination class (Fighter/Magic-User). Their hit die is determined by the \"\n",
    "                \"lower of the two classes. Magic-Users use d4 and Fighters use d8, but the \"\n",
    "                \"combination class compromise gives Elves d6. This is specified in the \"\n",
    "                \"Character Races section.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What happens if a Thief fails an Open Locks attempt?\",\n",
    "            \"response\": (\n",
    "                \"Open Locks allows the Thief to unlock a lock without a proper key. It may \"\n",
    "                \"only be tried once per lock. If the attempt fails, the Thief must wait until \"\n",
    "                \"they have gained another level of experience before trying again.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the maximum number of retainers a character with Charisma 18 can hire?\",\n",
    "            \"response\": (\n",
    "                \"A character with Charisma 18 receives a +3 bonus. According to the Charisma \"\n",
    "                \"table, this affects the number of retainers a character may hire and their \"\n",
    "                \"loyalty. The Retainers section specifies that the base number is modified by \"\n",
    "                \"the Charisma bonus.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How does the Turning Undead table work for Clerics?\",\n",
    "            \"response\": (\n",
    "                \"The Turning Undead table cross-references the Cleric's level against the \"\n",
    "                \"undead type. The number shown is the target on 2d6 that must be rolled to \"\n",
    "                \"successfully turn the undead. A 'T' means the undead are automatically turned, \"\n",
    "                \"and a 'D' means they are automatically destroyed. If the roll fails, the Cleric \"\n",
    "                \"cannot attempt to turn that group again during the same encounter.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the movement rate for a character in plate armor?\",\n",
    "            \"response\": (\n",
    "                \"Characters wearing plate mail or plate mail with shield have a base movement \"\n",
    "                \"rate of 20 feet per round. This is the slowest standard movement rate. \"\n",
    "                \"Unarmored characters move at 40 feet per round. Movement rates are detailed \"\n",
    "                \"in the Equipment section.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How are saving throws determined for a first-level Fighter?\",\n",
    "            \"response\": (\n",
    "                \"Saving throws for a first-level Fighter are listed in the Fighter Saving \"\n",
    "                \"Throws table. The five categories are Death Ray or Poison, Magic Wands, \"\n",
    "                \"Paralysis or Turn to Stone, Dragon Breath, and Spells. Each has a target \"\n",
    "                \"number that must be rolled on a d20 or higher to succeed. Fighters generally \"\n",
    "                \"have the best saving throws against physical threats like Dragon Breath.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    qa_df = pd.DataFrame(fallback_data)\n",
    "else:\n",
    "    qa_df = pd.read_csv(sdg_path)\n",
    "\n",
    "print(f\"Loaded {len(qa_df)} QA pairs\")\n",
    "print(f\"Columns: {list(qa_df.columns)}\")\n",
    "\n",
    "for i, row in qa_df.iterrows():\n",
    "    print(f\"\\n  Q{i+1}: {row['question'][:100]}\")\n",
    "    print(f\"  A{i+1}: {row['response'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Convert to Chat JSONL Format\n",
    "\n",
    "Now we convert the QA pairs into JSONL chat format. Each training example becomes a three-turn conversation: a system prompt that establishes the model's role, a user question, and an assistant response. The model learns to produce the assistant turn given the other two.\n",
    "\n",
    "The system prompt is a design choice. It tells the model who it is during training, and the same prompt should be used at inference time. If you change the persona later, the model may not behave as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 8 examples to training_data.jsonl\n",
      "\n",
      "First example structure:\n",
      "  Keys: ['messages']\n",
      "  Roles: ['system', 'user', 'assistant']\n",
      "  User: What is the Thief's Open Locks ability score at level 8?...\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a rules expert for the Basic Fantasy Role-Playing Game. \"\n",
    "    \"Answer questions accurately based on the official rules. \"\n",
    "    \"Be specific and cite page references or table values where possible.\"\n",
    ")\n",
    "\n",
    "# Convert to chat messages JSONL format\n",
    "training_data = []\n",
    "for _, row in qa_df.iterrows():\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": row[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": row[\"response\"]},\n",
    "        ]\n",
    "    }\n",
    "    training_data.append(example)\n",
    "\n",
    "# Write JSONL\n",
    "data_path = \"training_data.jsonl\"\n",
    "with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in training_data:\n",
    "        f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(training_data)} examples to {data_path}\")\n",
    "\n",
    "# Verify structure\n",
    "with open(data_path, \"r\") as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "print(f\"\\nFirst example structure:\")\n",
    "print(f\"  Keys: {list(first_line.keys())}\")\n",
    "print(f\"  Roles: {[m['role'] for m in first_line['messages']]}\")\n",
    "print(f\"  User: {first_line['messages'][1]['content'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six training examples is very small. In a production engagement, you would generate hundreds or thousands of pairs across the full document corpus, filter on faithfulness, deduplicate, and review samples before committing to training. We use six because the goal is to demonstrate the process and observe the behavior, not to produce a production-quality adapter.\n",
    "\n",
    "Even with six examples, if the training works correctly, the model should show measurable change on questions that directly overlap with the training data. Whether it generalizes beyond those specific examples is a separate question, and one that a larger dataset would answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Download the Base Model\n",
    "\n",
    "LoRA operates on the model weights directly, so the model must be on disk, not behind an API. We download the same Granite model we have been using throughout the workshop.\n",
    "\n",
    "The download is approximately 16GB. The weights will be loaded in 4-bit quantization during training, so we need the full download on disk but only a fraction of that will occupy GPU memory.\n",
    "\n",
    "The `RUN_LIVE` toggle controls whether the download executes. If set to `False`, the cell assumes a pre-built copy exists at the expected path.\n",
    "\n",
    "_This step typically takes 2 to 5 minutes depending on network speed._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to download the model live. Set to False to use a pre-downloaded copy.\n",
    "RUN_LIVE_DOWNLOAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at ./models/granite-3.2-8b-instruct, skipping download.\n",
      "\n",
      "Model directory: 14 items, 4 safetensor shards\n",
      "Total size:      16.3 GB\n",
      "Elapsed:         0m 0s\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_ID = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "LOCAL_MODEL_DIR = \"./models/granite-3.2-8b-instruct\"\n",
    "\n",
    "if RUN_LIVE_DOWNLOAD:\n",
    "    start_time = time.time()\n",
    "\n",
    "    if os.path.exists(LOCAL_MODEL_DIR) and len(os.listdir(LOCAL_MODEL_DIR)) > 5:\n",
    "        print(f\"Model already exists at {LOCAL_MODEL_DIR}, skipping download.\")\n",
    "    else:\n",
    "        print(f\"Downloading {MODEL_ID}...\")\n",
    "        print(\"This may take several minutes.\")\n",
    "        snapshot_download(\n",
    "            repo_id=MODEL_ID,\n",
    "            local_dir=LOCAL_MODEL_DIR,\n",
    "        )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    minutes = int(elapsed // 60)\n",
    "    seconds = int(elapsed % 60)\n",
    "\n",
    "    # Verify\n",
    "    model_files = [f for f in os.listdir(LOCAL_MODEL_DIR) if not f.startswith('.')]\n",
    "    safetensor_files = [f for f in model_files if f.endswith('.safetensors')]\n",
    "    total_size = sum(\n",
    "        os.path.getsize(os.path.join(LOCAL_MODEL_DIR, f)) for f in model_files\n",
    "    )\n",
    "\n",
    "    print(f\"\\nModel directory: {len(model_files)} items, {len(safetensor_files)} safetensor shards\")\n",
    "    print(f\"Total size:      {total_size / 1e9:.1f} GB\")\n",
    "    print(f\"Elapsed:         {minutes}m {seconds}s\")\n",
    "else:\n",
    "    print(f\"Skipping download. Using pre-downloaded model at {LOCAL_MODEL_DIR}\")\n",
    "    if not os.path.exists(LOCAL_MODEL_DIR):\n",
    "        print(\"WARNING: Model directory does not exist. Training will fail.\")\n",
    "        print(\"Set RUN_LIVE_DOWNLOAD = True and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Run LoRA SFT Training\n",
    "\n",
    "This is the core of the section: a single function call that trains the model.\n",
    "\n",
    "We set conservative parameters tuned for a lab environment on a single L4 (24GB):\n",
    "\n",
    "| Parameter | Value | Why |\n",
    "|-----------|-------|-----|\n",
    "| `num_epochs` | 5 | Multiple passes over our small dataset to ensure the model sees each example enough times |\n",
    "| `effective_batch_size` | 2 | Small batch to stay well within GPU memory limits |\n",
    "| `max_seq_len` | 512 | Our training examples are short; longer sequences waste memory |\n",
    "| `max_tokens_per_gpu` | 512 | Conservative memory cap per GPU to avoid OOM errors |\n",
    "| `learning_rate` | 5e-6 | Conservative rate to avoid catastrophic forgetting of general knowledge |\n",
    "\n",
    "These values are deliberately conservative. On a 24GB GPU, we do not have the margin to be aggressive with batch sizes or sequence lengths. The goal is a successful training run that demonstrates the process, not maximum throughput.\n",
    "\n",
    "The `RUN_LIVE` toggle controls execution. When set to `False`, the cell skips training and points to a pre-built adapter checkpoint.\n",
    "\n",
    "_Training with these parameters on 6 examples typically takes 3 to 8 minutes on an L4. You will see progress output as epochs complete._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to run training live. Set to False to use pre-built adapter.\n",
    "RUN_LIVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_hub import lora_sft\n",
    "\n",
    "CKPT_DIR = \"./lora_output\"\n",
    "\n",
    "if RUN_LIVE:\n",
    "    # Clear any leftover GPU memory from previous sections\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        mem_used = torch.cuda.memory_allocated() / 1e9\n",
    "        mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU memory before training: {mem_used:.1f} GB used / {mem_total:.1f} GB total\")\n",
    "        print()\n",
    "\n",
    "    print(\"Starting LoRA SFT training...\")\n",
    "    print(f\"  Model:          {LOCAL_MODEL_DIR}\")\n",
    "    print(f\"  Data:           {data_path}\")\n",
    "    print(f\"  Output:         {CKPT_DIR}\")\n",
    "    print(f\"  Epochs:         5\")\n",
    "    print(f\"  Batch size:     2\")\n",
    "    print(f\"  Max seq len:    512\")\n",
    "    print(f\"  Max tokens/GPU: 512\")\n",
    "    print(f\"  Learning rate:  5e-6\")\n",
    "    print(f\"  Quantization:   4-bit (QLoRA)\")\n",
    "    print()\n",
    "    print(\"This typically takes 3 to 8 minutes. Progress will appear below.\")\n",
    "    print()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = lora_sft(\n",
    "        model_path=LOCAL_MODEL_DIR,\n",
    "        data_path=data_path,\n",
    "        ckpt_output_dir=CKPT_DIR,\n",
    "        data_output_dir=\"./lora_data_output\",\n",
    "        num_epochs=5,\n",
    "        effective_batch_size=2,\n",
    "        max_tokens_per_gpu=512,\n",
    "        max_seq_len=512,\n",
    "        learning_rate=5e-6,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    minutes = int(elapsed // 60)\n",
    "    seconds = int(elapsed % 60)\n",
    "    print(f\"\\nTraining complete in {minutes}m {seconds}s\")\n",
    "    print(f\"Result: {result}\")\n",
    "else:\n",
    "    print(\"Skipping live training. Using pre-built adapter checkpoint.\")\n",
    "    CKPT_DIR = \"../prebuilt/lora_output\"\n",
    "    if not os.path.exists(CKPT_DIR):\n",
    "        print(f\"WARNING: Pre-built checkpoint not found at {CKPT_DIR}\")\n",
    "        print(\"Set RUN_LIVE = True and re-run to train from scratch.\")\n",
    "    else:\n",
    "        print(f\"Using checkpoint at: {CKPT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Inspect Training Artifacts\n",
    "\n",
    "After training completes, the output directory contains the LoRA adapter weights and a training log. The adapter is small (typically tens of megabytes) compared to the full model (16GB). This is one of LoRA's key advantages: the artifact you produce, store, and version is compact.\n",
    "\n",
    "Let's verify that the output directory was populated and look at the training log for loss progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what training produced\n",
    "if os.path.exists(CKPT_DIR):\n",
    "    contents = os.listdir(CKPT_DIR)\n",
    "    print(f\"Checkpoint directory ({CKPT_DIR}): {len(contents)} items\")\n",
    "    for item in sorted(contents):\n",
    "        item_path = os.path.join(CKPT_DIR, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size_mb = os.path.getsize(item_path) / 1e6\n",
    "            print(f\"  {item:40s} {size_mb:8.1f} MB\")\n",
    "        else:\n",
    "            print(f\"  {item:40s} (directory)\")\n",
    "else:\n",
    "    print(f\"Checkpoint directory not found: {CKPT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for training log\n",
    "log_candidates = [\n",
    "    os.path.join(CKPT_DIR, \"training_log_node0.log\"),\n",
    "    os.path.join(\"./lora_output\", \"training_log_node0.log\"),\n",
    "    os.path.join(\"./lora_data_output\", \"training_log_node0.log\"),\n",
    "]\n",
    "\n",
    "log_found = False\n",
    "for log_path in log_candidates:\n",
    "    if os.path.exists(log_path):\n",
    "        print(f\"Training log found at: {log_path}\")\n",
    "        print(\"Last 30 lines:\")\n",
    "        print()\n",
    "        with open(log_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines[-30:]:\n",
    "                print(line.rstrip())\n",
    "        log_found = True\n",
    "        break\n",
    "\n",
    "if not log_found:\n",
    "    print(\"No training log found. This is expected if using pre-built outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Verify GPU State After Training\n",
    "\n",
    "Training loads the model, optimizer states, and gradient buffers into GPU memory. After training completes, these should be released. This cell confirms that GPU memory has been freed and reports current usage. If memory is still high, a kernel restart may be needed before running inference with the adapted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(f\"GPU memory used:  {torch.cuda.memory_allocated()/1e9:.1f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1e9:.1f} GB\")\n",
    "    print(f\"GPU memory total:  {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 What Just Happened\n",
    "\n",
    "If training completed successfully, you now have a LoRA adapter in the `lora_output` directory. This adapter contains only the learned adjustments, not a full copy of the model. To use it, you merge the adapter with the base model at inference time (or ahead of time as a one-time operation).\n",
    "\n",
    "A few things to note about what we just did and what it means:\n",
    "\n",
    "The base model was never modified. Every weight in Granite 3.2 8B is exactly as it was before training. The adapter sits alongside those weights and adds small corrections to the model's behavior. If the adapter makes things worse, you can discard it and return to the base model immediately. There is no rollback problem.\n",
    "\n",
    "Six training examples is not enough for production. We used a minimal dataset to demonstrate the mechanics. In a real engagement, you would generate hundreds or thousands of pairs, filter for quality, and evaluate the adapter against a held-out test set before declaring success.\n",
    "\n",
    "The real question is not \"did it train\" but \"did the adapted model close the gaps we identified in evaluation?\" That question gets answered in the next section, where we compare the adapted model's outputs against the baseline and RAG results from earlier.\n",
    "\n",
    "This is where the escalation ladder from Section 5 comes full circle. We started with a baseline. We improved with RAG. We evaluated. We identified specific, consistent failures that pointed to the model itself. And only then did we adapt the model, with a clear hypothesis about what should improve and a way to measure whether it did."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 3.8 Did Training Help?\n\nWe started this section because the evidence from Sections 0 and 1 pointed to a knowledge gap, not a retrieval gap and not a sampling gap. Now we close the loop. We load the adapter, run the same questions that failed earlier, and check whether the answers changed.\n\n> **Note:** If training ran in this kernel session, the model and optimizer may still occupy GPU memory. If the next cell fails with an out-of-memory error, restart the kernel (Kernel > Restart) and run only the cells in this subsection. The adapter and base model are already saved to disk.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Free GPU memory from training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)\n",
    "model = PeftModel.from_pretrained(base_model, CKPT_DIR)\n",
    "model.eval()\n",
    "print(\"Adapter loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a rules expert for the Basic Fantasy Role-Playing Game. \"\n",
    "    \"Answer questions accurately and concisely based on the rules.\"\n",
    ")\n",
    "\n",
    "TARGET_QUESTIONS = [\n",
    "    {\n",
    "        \"id\": \"q02\",\n",
    "        \"question\": \"Why do Elves use a d6 for hit dice instead of a d8?\",\n",
    "        \"expected\": \"Elves are a combination class. The d6 reflects the compromise between Fighter and Magic-User hit dice.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q06\",\n",
    "        \"question\": \"What melee attack and damage bonus does a character with Strength 16 receive?\",\n",
    "        \"expected\": \"+2 to attack and +2 to damage.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "def run_inference(question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
    "    ).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_new_tokens=200, do_sample=False)\n",
    "    return tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"Running target questions through fine-tuned model...\\n\")\n",
    "for q in TARGET_QUESTIONS:\n",
    "    answer = run_inference(q[\"question\"])\n",
    "    print(f\"[{q['id']}] {q['question']}\")\n",
    "    print(f\"  Expected: {q['expected']}\")\n",
    "    print(f\"  Got:      {answer[:200]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the two questions that resisted both RAG and inference-time scaling. If the adapted model produces better answers here, the training had the intended effect. If not, the training data or the number of examples may be insufficient.\n",
    "\n",
    "Either result is informative. Section 4 runs the full evaluation with all 10 questions and a systematic comparison.\n",
    "\n",
    "**Transition to Section 4:**\n",
    "\n",
    "\"We have an adapted model. The next step is to evaluate it properly: all 10 questions, side-by-side comparison, and an honest assessment of what improved and what did not.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}