{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 3. Model Adaptation: Giving the Model Domain Knowledge\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "A base language model is trained on broad internet text. When you ask it about a specific domain — like the rules of a particular tabletop RPG — it may give plausible-sounding but incorrect answers, or admit it does not know. The model's weights do not contain the domain knowledge you need.\n",
    "\n",
    "There are several ways to close that gap. The simplest and cheapest is **Retrieval-Augmented Generation (RAG)**: instead of changing the model, you give it the right information at query time by retrieving relevant document chunks and including them in the prompt. The model reads the context and answers based on it.\n",
    "\n",
    "This notebook demonstrates the RAG approach:\n",
    "\n",
    "1. **Baseline**: Ask the model domain-specific questions with no context. Observe where it fails.\n",
    "2. **RAG Pipeline**: Load the domain document, chunk it, embed the chunks into a vector store, and retrieve relevant context at query time.\n",
    "3. **Comparison**: Ask the same questions with retrieved context and compare the results.\n",
    "\n",
    "RAG is the first technique to try before escalating to more expensive approaches like inference-time scaling or fine-tuning. If RAG solves the problem, you avoid the cost and complexity of changing the model entirely.\n",
    "\n",
    "**Source Document:** `Basic-Fantasy-RPG-Rules-r142.md` — the complete Basic Fantasy RPG rulebook, converted to markdown by Docling in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d5",
   "metadata": {},
   "source": [
    "## 3.1 Install Dependencies\n",
    "\n",
    "`chromadb` is a lightweight vector database that stores document embeddings and supports similarity search. `sentence-transformers` is already installed in the lab environment and provides the embedding model.\n",
    "\n",
    "`pysqlite3-binary` is needed because the system sqlite3 (3.34) is below ChromaDB's minimum requirement (3.35). The binary package ships a newer version.\n",
    "\n",
    "If you see dependency conflict warnings from pip, they are advisory and do not affect functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b2c3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:07.530268Z",
     "iopub.status.busy": "2026-02-21T14:00:07.530140Z",
     "iopub.status.idle": "2026-02-21T14:00:08.676508Z",
     "shell.execute_reply": "2026-02-21T14:00:08.675806Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install chromadb pysqlite3-binary -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d7",
   "metadata": {},
   "source": [
    "## 3.2 Environment Setup\n",
    "\n",
    "Same credentials, same endpoint pattern. We reuse the `.env` file and config helper from earlier sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b2c3d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:08.678957Z",
     "iopub.status.busy": "2026-02-21T14:00:08.678804Z",
     "iopub.status.idle": "2026-02-21T14:00:08.682973Z",
     "shell.execute_reply": "2026-02-21T14:00:08.682342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: https://litellm-prod.apps.maas.redhatworkshops.io/v1\n",
      "API Key:  sk-UFHcL...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from config import API_KEY as key, ENDPOINT_BASE as endpoint_base\n",
    "\n",
    "print(f\"Endpoint: {endpoint_base}\")\n",
    "print(f\"API Key:  {key[:8]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b2c3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:08.684195Z",
     "iopub.status.busy": "2026-02-21T14:00:08.684053Z",
     "iopub.status.idle": "2026-02-21T14:00:09.858143Z",
     "shell.execute_reply": "2026-02-21T14:00:09.857546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: granite-3-2-8b-instruct\n",
      "Test:  Hello, it's a pleasure to assist you today!\n",
      "Connection OK.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=key,\n",
    "    base_url=endpoint_base,\n",
    ")\n",
    "\n",
    "MODEL = \"granite-3-2-8b-instruct\"\n",
    "\n",
    "# Quick connectivity check\n",
    "test = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}],\n",
    "    max_tokens=50,\n",
    ")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Test:  {test.choices[0].message.content.strip()}\")\n",
    "print(\"Connection OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2c3d0",
   "metadata": {},
   "source": [
    "## 3.3 Baseline: Model Responses Without Context\n",
    "\n",
    "Before building anything, we establish a baseline. We ask the model a set of domain-specific questions about the Basic Fantasy RPG rules **without providing any context**. The model must rely entirely on whatever it learned during pre-training.\n",
    "\n",
    "These questions come from the Day 2 evaluation set. They cover different types of domain knowledge: explicit rules, table lookups, terminology, and questions that require reasoning across multiple rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b2c3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:09.859844Z",
     "iopub.status.busy": "2026-02-21T14:00:09.859721Z",
     "iopub.status.idle": "2026-02-21T14:00:09.863579Z",
     "shell.execute_reply": "2026-02-21T14:00:09.862973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 evaluation questions\n",
      "  [q01] (explicit_rule) What happens if a Thief fails an Open Locks attempt?\n",
      "  [q04] (table_lookup) What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "  [q05] (multi_step_rule) How does a Cleric turn undead?\n",
      "  [q07] (terminology) What is the difference between a retainer and a hireling?\n",
      "  [q10] (implicit_reasoning) Can a Halfling use a longbow?\n"
     ]
    }
   ],
   "source": [
    "# Questions from the Day 2 evaluation set\n",
    "questions = [\n",
    "    {\n",
    "        \"id\": \"q01\",\n",
    "        \"question\": \"What happens if a Thief fails an Open Locks attempt?\",\n",
    "        \"expected\": \"The Thief must wait until gaining another level of experience before trying again. It may only be tried once per lock.\",\n",
    "        \"category\": \"explicit_rule\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q04\",\n",
    "        \"question\": \"What is the saving throw for a 3rd level Fighter against Dragon Breath?\",\n",
    "        \"expected\": \"Based on the Fighter saving throw table, a 3rd level Fighter has a Dragon Breath saving throw of 15.\",\n",
    "        \"category\": \"table_lookup\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q05\",\n",
    "        \"question\": \"How does a Cleric turn undead?\",\n",
    "        \"expected\": \"The Cleric rolls 2d6 and compares the result to the Turn Undead table. Success depends on the Cleric's level and the type of undead.\",\n",
    "        \"category\": \"multi_step_rule\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q07\",\n",
    "        \"question\": \"What is the difference between a retainer and a hireling?\",\n",
    "        \"expected\": \"Retainers are NPCs who accompany the party on adventures and gain experience. Hirelings are hired for specific non-adventuring tasks.\",\n",
    "        \"category\": \"terminology\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q10\",\n",
    "        \"question\": \"Can a Halfling use a longbow?\",\n",
    "        \"expected\": \"Halflings may not use Large weapons and must use Medium weapons two-handed. A longbow is a Large weapon, so a Halfling cannot use one.\",\n",
    "        \"category\": \"implicit_reasoning\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(questions)} evaluation questions\")\n",
    "for q in questions:\n",
    "    print(f\"  [{q['id']}] ({q['category']}) {q['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b2c3d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:09.864847Z",
     "iopub.status.busy": "2026-02-21T14:00:09.864713Z",
     "iopub.status.idle": "2026-02-21T14:00:09.867839Z",
     "shell.execute_reply": "2026-02-21T14:00:09.867323Z"
    }
   },
   "outputs": [],
   "source": [
    "def ask_model(question, context=None):\n",
    "    \"\"\"Ask the model a question, optionally with retrieved context.\"\"\"\n",
    "    if context:\n",
    "        system_msg = (\n",
    "            \"Answer the question using only the provided context. \"\n",
    "            \"Be specific and cite rules where possible.\"\n",
    "        )\n",
    "        user_msg = f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "    else:\n",
    "        system_msg = (\n",
    "            \"Answer the question about Basic Fantasy RPG rules. \"\n",
    "            \"Be specific and concise.\"\n",
    "        )\n",
    "        user_msg = question\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b2c3d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:09.869182Z",
     "iopub.status.busy": "2026-02-21T14:00:09.869048Z",
     "iopub.status.idle": "2026-02-21T14:00:31.195139Z",
     "shell.execute_reply": "2026-02-21T14:00:31.194533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE: No context provided\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q01] What happens if a Thief fails an Open Locks attempt?\n",
      "  Expected: The Thief must wait until gaining another level of experience before trying again. It may only be tried once per lock.\n",
      "  Model:    If a Thief fails an Open Locks attempt in Basic Fantasy RPG, they can try again immediately, but there's a cumulative -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q04] What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "  Expected: Based on the Fighter saving throw table, a 3rd level Fighter has a Dragon Breath saving throw of 15.\n",
      "  Model:    In Basic Fantasy RPG, a 3rd level Fighter would make a \"Save vs. Breath Weapon\" for a Dragon's breath attack. This is no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q05] How does a Cleric turn undead?\n",
      "  Expected: The Cleric rolls 2d6 and compares the result to the Turn Undead table. Success depends on the Cleric's level and the typ\n",
      "  Model:    In Basic Fantasy RPG, a Cleric can turn undead using the \"Turn Undead\" ability. Here's how it works:\n",
      "\n",
      "1. The Cleric must\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q07] What is the difference between a retainer and a hireling?\n",
      "  Expected: Retainers are NPCs who accompany the party on adventures and gain experience. Hirelings are hired for specific non-adven\n",
      "  Model:    In Basic Fantasy RPG, both retainers and hirelings are NPCs who can assist the player characters, but they differ in the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q10] Can a Halfling use a longbow?\n",
      "  Expected: Halflings may not use Large weapons and must use Medium weapons two-handed. A longbow is a Large weapon, so a Halfling c\n",
      "  Model:    Yes, in Basic Fantasy RPG, Halflings can use longbows. There are no racial restrictions in the rules that prevent Halfli\n"
     ]
    }
   ],
   "source": [
    "print(\"BASELINE: No context provided\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "baseline_answers = {}\n",
    "\n",
    "for q in questions:\n",
    "    answer = ask_model(q[\"question\"])\n",
    "    baseline_answers[q[\"id\"]] = answer\n",
    "\n",
    "    print(f\"\\n[{q['id']}] {q['question']}\")\n",
    "    print(f\"  Expected: {q['expected'][:120]}\")\n",
    "    print(f\"  Model:    {answer[:120]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2c3d4",
   "metadata": {},
   "source": [
    "Look at the baseline answers. Some may be partially correct — the model has seen tabletop RPG content during pre-training. But for specific rules, table values, and game-specific terminology, the model is guessing or hedging. It does not have reliable access to the exact rules of Basic Fantasy RPG.\n",
    "\n",
    "This is the gap that RAG addresses: not by changing the model, but by giving it the right information when it needs it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2c3d0",
   "metadata": {},
   "source": [
    "## 3.4 Building the RAG Pipeline\n",
    "\n",
    "A RAG pipeline has three stages:\n",
    "\n",
    "1. **Chunk** the source document into passages small enough to fit in a prompt.\n",
    "2. **Embed** each chunk into a vector representation using an embedding model.\n",
    "3. **Store** the vectors in a database that supports similarity search.\n",
    "\n",
    "At query time, the user's question is embedded with the same model, the most similar chunks are retrieved, and they are included in the prompt as context.\n",
    "\n",
    "### 3.4.1 Load the Source Document\n",
    "\n",
    "The document is the Basic Fantasy RPG rulebook, converted to markdown by Docling in Section 2. At ~900KB of text, it is too large to fit in a single prompt. Chunking breaks it into manageable pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b2c3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:31.196780Z",
     "iopub.status.busy": "2026-02-21T14:00:31.196661Z",
     "iopub.status.idle": "2026-02-21T14:00:31.201463Z",
     "shell.execute_reply": "2026-02-21T14:00:31.200806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: ../02SyntheticDataGen/Basic-Fantasy-RPG-Rules-r142.md\n",
      "Size:     908,392 characters\n",
      "Lines:    11,260\n",
      "Preview:  <!-- image -->\n",
      "\n",
      "Copyright © 2006-2025 Chris Gonnerman All Rights Reserved.  See next page for license information. www.basicfantasy.org\n",
      "\n",
      "Dedicated to Gary Gygax, Dave Arneson, Tom Moldvay, David Cook,...\n"
     ]
    }
   ],
   "source": [
    "doc_path = \"../02SyntheticDataGen/Basic-Fantasy-RPG-Rules-r142.md\"\n",
    "\n",
    "with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    document = f.read()\n",
    "\n",
    "print(f\"Document: {doc_path}\")\n",
    "print(f\"Size:     {len(document):,} characters\")\n",
    "print(f\"Lines:    {document.count(chr(10)):,}\")\n",
    "print(f\"Preview:  {document[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2c3d2",
   "metadata": {},
   "source": [
    "### 3.4.2 Chunk the Document\n",
    "\n",
    "We split the document into chunks of approximately 1000 characters with 200 characters of overlap. The overlap ensures that information at chunk boundaries is not lost. Splitting happens on paragraph breaks where possible, falling back to sentence boundaries, so chunks stay semantically coherent.\n",
    "\n",
    "These parameters match the chunking strategy used in the Day 2 evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b2c3d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:31.202820Z",
     "iopub.status.busy": "2026-02-21T14:00:31.202705Z",
     "iopub.status.idle": "2026-02-21T14:00:31.211248Z",
     "shell.execute_reply": "2026-02-21T14:00:31.210647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1246\n",
      "Avg length:   927 chars\n",
      "Min length:   164 chars\n",
      "Max length:   20378 chars\n",
      "\n",
      "First chunk preview:\n",
      "  <!-- image -->\n",
      "\n",
      "Copyright © 2006-2025 Chris Gonnerman All Rights Reserved.  See next page for license information. www.basicfantasy.org\n",
      "\n",
      "Dedicated to Gary Gygax, Dave Arneson, Tom Moldvay, David Cook,...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def chunk_document(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"Split text into overlapping chunks, preferring paragraph boundaries.\"\"\"\n",
    "    # Split on double newlines (paragraph breaks) first\n",
    "    paragraphs = re.split(r\"\\n\\n+\", text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "\n",
    "        # If adding this paragraph would exceed chunk_size, save current and start new\n",
    "        if len(current_chunk) + len(para) + 2 > chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            # Overlap: keep the tail of the previous chunk\n",
    "            if len(current_chunk) > overlap:\n",
    "                current_chunk = current_chunk[-overlap:].strip() + \"\\n\\n\" + para\n",
    "            else:\n",
    "                current_chunk = para\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                current_chunk += \"\\n\\n\" + para\n",
    "            else:\n",
    "                current_chunk = para\n",
    "\n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_document(document, chunk_size=1000, overlap=200)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Avg length:   {sum(len(c) for c in chunks) / len(chunks):.0f} chars\")\n",
    "print(f\"Min length:   {min(len(c) for c in chunks)} chars\")\n",
    "print(f\"Max length:   {max(len(c) for c in chunks)} chars\")\n",
    "print(f\"\\nFirst chunk preview:\")\n",
    "print(f\"  {chunks[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2c3d4",
   "metadata": {},
   "source": [
    "### 3.4.3 Load the Embedding Model\n",
    "\n",
    "We use IBM's `granite-embedding-30m-english`, a compact embedding model designed for retrieval tasks. At 30 million parameters, it is small enough to run on CPU and fast enough for interactive use.\n",
    "\n",
    "The model is downloaded from HuggingFace and cached locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b2c3d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:31.212677Z",
     "iopub.status.busy": "2026-02-21T14:00:31.212561Z",
     "iopub.status.idle": "2026-02-21T14:00:31.413921Z",
     "shell.execute_reply": "2026-02-21T14:00:31.413141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model already exists at ./models/granite-embedding-30m-english, skipping download.\n",
      "Model directory: 13 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "EMBEDDING_MODEL_ID = \"ibm-granite/granite-embedding-30m-english\"\n",
    "EMBEDDING_MODEL_DIR = \"./models/granite-embedding-30m-english\"\n",
    "\n",
    "if os.path.exists(EMBEDDING_MODEL_DIR) and len(os.listdir(EMBEDDING_MODEL_DIR)) > 3:\n",
    "    print(f\"Embedding model already exists at {EMBEDDING_MODEL_DIR}, skipping download.\")\n",
    "else:\n",
    "    print(f\"Downloading {EMBEDDING_MODEL_ID}...\")\n",
    "    snapshot_download(\n",
    "        repo_id=EMBEDDING_MODEL_ID,\n",
    "        local_dir=EMBEDDING_MODEL_DIR,\n",
    "    )\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "model_files = [f for f in os.listdir(EMBEDDING_MODEL_DIR) if not f.startswith(\".\")]\n",
    "print(f\"Model directory: {len(model_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b2c3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:31.415284Z",
     "iopub.status.busy": "2026-02-21T14:00:31.415153Z",
     "iopub.status.idle": "2026-02-21T14:00:36.724776Z",
     "shell.execute_reply": "2026-02-21T14:00:36.724056Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/torch/cuda/__init__.py:789: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded: ./models/granite-embedding-30m-english\n",
      "Embedding dimension:    384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL_DIR)\n",
    "\n",
    "# Verify it works\n",
    "test_embedding = embedder.encode([\"test sentence\"])\n",
    "print(f\"Embedding model loaded: {EMBEDDING_MODEL_DIR}\")\n",
    "print(f\"Embedding dimension:    {test_embedding.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2c3d7",
   "metadata": {},
   "source": [
    "### 3.4.4 Build the Vector Store\n",
    "\n",
    "We embed all document chunks and store them in a ChromaDB collection. ChromaDB is an in-memory vector database that supports cosine similarity search. It is lightweight and requires no external services — the entire database lives in this process.\n",
    "\n",
    "We provide a custom embedding function that wraps our local Granite model so ChromaDB uses it for both indexing and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b2c3d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:36.726526Z",
     "iopub.status.busy": "2026-02-21T14:00:36.726162Z",
     "iopub.status.idle": "2026-02-21T14:00:42.065348Z",
     "shell.execute_reply": "2026-02-21T14:00:42.064747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created collection: bfrpg_rules\n",
      "Embedding 1246 chunks...\n",
      "  Added chunks 0 to 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added chunks 100 to 199\n",
      "  Added chunks 200 to 299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added chunks 300 to 399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added chunks 400 to 499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added chunks 500 to 599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added chunks 600 to 699\n",
      "  Added chunks 700 to 799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added chunks 800 to 899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added chunks 900 to 999\n",
      "  Added chunks 1000 to 1099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added chunks 1100 to 1199\n",
      "  Added chunks 1200 to 1245\n",
      "\n",
      "Vector store ready: 1246 chunks indexed\n"
     ]
    }
   ],
   "source": [
    "# Workaround: system sqlite3 (3.34) is below chromadb's minimum (3.35).\n",
    "# pysqlite3-binary ships a newer sqlite3 that satisfies the requirement.\n",
    "__import__(\"pysqlite3\")\n",
    "import sys\n",
    "\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")\n",
    "\n",
    "import chromadb\n",
    "\n",
    "\n",
    "class LocalEmbeddingFunction(chromadb.EmbeddingFunction):\n",
    "    \"\"\"Wraps sentence-transformers model for use with ChromaDB.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, input):\n",
    "        embeddings = self.model.encode(input)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "\n",
    "embedding_fn = LocalEmbeddingFunction(embedder)\n",
    "\n",
    "# Create in-memory ChromaDB client and collection\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Delete collection if it already exists (re-run safety)\n",
    "try:\n",
    "    chroma_client.delete_collection(\"bfrpg_rules\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"bfrpg_rules\",\n",
    "    embedding_function=embedding_fn,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "print(f\"Created collection: bfrpg_rules\")\n",
    "print(f\"Embedding {len(chunks)} chunks...\")\n",
    "\n",
    "# Add chunks in batches to avoid memory issues\n",
    "BATCH_SIZE = 100\n",
    "for i in range(0, len(chunks), BATCH_SIZE):\n",
    "    batch = chunks[i : i + BATCH_SIZE]\n",
    "    ids = [f\"chunk_{j}\" for j in range(i, i + len(batch))]\n",
    "    collection.add(documents=batch, ids=ids)\n",
    "    print(f\"  Added chunks {i} to {i + len(batch) - 1}\")\n",
    "\n",
    "print(f\"\\nVector store ready: {collection.count()} chunks indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2c3d0",
   "metadata": {},
   "source": [
    "### 3.4.5 Test Retrieval\n",
    "\n",
    "Before using retrieval in the full pipeline, let's verify it works. We query the vector store with one of our test questions and inspect the returned chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b2c3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:42.067372Z",
     "iopub.status.busy": "2026-02-21T14:00:42.066964Z",
     "iopub.status.idle": "2026-02-21T14:00:42.077923Z",
     "shell.execute_reply": "2026-02-21T14:00:42.077362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What happens if a Thief fails an Open Locks attempt?\n",
      "Retrieved 3 chunks:\n",
      "\n",
      "  Chunk 1 (distance: 0.1897):\n",
      "  82 |             98 |              91 |            98 |     72 |       92 |\n",
      "|            20 |           88 |             83 |             99 |              93 |            99 |     73 |       95 |\n",
      "\n",
      "Th...\n",
      "\n",
      "  Chunk 2 (distance: 0.2443):\n",
      "  rolled by the GM.    The   Thief   will   usually   believe   they   are   moving silently regardless of the die roll, but opponents they are trying to avoid will hear the Thief if the roll is failed....\n",
      "\n",
      "  Chunk 3 (distance: 0.2502):\n",
      "  -->\n",
      "\n",
      "## Doors\n",
      "\n",
      "A stuck door can be opened on a roll of 1 on 1d6; add the character's Strength bonus to the range, so that a character with a bonus of +2 can open a stuck door on a roll of 1-3 on 1d6.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What happens if a Thief fails an Open Locks attempt?\"\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[test_query],\n",
    "    n_results=3,\n",
    ")\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved {len(results['documents'][0])} chunks:\\n\")\n",
    "\n",
    "for i, (doc, distance) in enumerate(zip(results[\"documents\"][0], results[\"distances\"][0])):\n",
    "    print(f\"  Chunk {i + 1} (distance: {distance:.4f}):\")\n",
    "    print(f\"  {doc[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2c3d2",
   "metadata": {},
   "source": [
    "The retrieved chunks should contain relevant rules about Thief abilities. The distances indicate how semantically similar each chunk is to the query (lower is more similar with cosine distance).\n",
    "\n",
    "## 3.5 RAG-Enhanced Responses\n",
    "\n",
    "Now we ask the same questions again, but this time we retrieve the top 3 relevant chunks from the vector store and include them as context in the prompt. The model reads the context and answers based on it.\n",
    "\n",
    "This is the core idea of RAG: the model's weights have not changed, but it now has access to the right information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1b2c3d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:42.079430Z",
     "iopub.status.busy": "2026-02-21T14:00:42.079310Z",
     "iopub.status.idle": "2026-02-21T14:00:42.082062Z",
     "shell.execute_reply": "2026-02-21T14:00:42.081494Z"
    }
   },
   "outputs": [],
   "source": [
    "def ask_with_rag(question, collection, n_results=3):\n",
    "    \"\"\"Retrieve relevant chunks and ask the model with context.\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=n_results,\n",
    "    )\n",
    "\n",
    "    # Combine retrieved chunks into a single context string\n",
    "    context = \"\\n\\n---\\n\\n\".join(results[\"documents\"][0])\n",
    "    distances = results[\"distances\"][0]\n",
    "\n",
    "    answer = ask_model(question, context=context)\n",
    "    return answer, context, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1b2c3d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:42.083278Z",
     "iopub.status.busy": "2026-02-21T14:00:42.083166Z",
     "iopub.status.idle": "2026-02-21T14:00:59.257211Z",
     "shell.execute_reply": "2026-02-21T14:00:59.256478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG-ENHANCED: With retrieved context\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q01] What happens if a Thief fails an Open Locks attempt?\n",
      "  Distances: ['0.1897', '0.2443', '0.2502']\n",
      "  Expected:  The Thief must wait until gaining another level of experience before trying again. It may only be tried once per lock.\n",
      "  Model:     If a Thief fails an Open Locks attempt, they cannot try again until they have gained another level of experience.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q04] What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "  Distances: ['0.2249', '0.2582', '0.2635']\n",
      "  Expected:  Based on the Fighter saving throw table, a 3rd level Fighter has a Dragon Breath saving throw of 15.\n",
      "  Model:     The context does not provide specific saving throw figures for a 3rd level Fighter against Dragon Breath. However, it do\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q05] How does a Cleric turn undead?\n",
      "  Distances: ['0.1782', '0.1891', '0.2030']\n",
      "  Expected:  The Cleric rolls 2d6 and compares the result to the Turn Undead table. Success depends on the Cleric's level and the typ\n",
      "  Model:     A Cleric turns undead by rolling a 20-sided die (1d20) and then referencing their level on the Cleric's vs. Undead table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q07] What is the difference between a retainer and a hireling?\n",
      "  Distances: ['0.1546', '0.2220', '0.2292']\n",
      "  Expected:  Retainers are NPCs who accompany the party on adventures and gain experience. Hirelings are hired for specific non-adven\n",
      "  Model:     The context does not explicitly define the difference between a retainer and a hireling. However, it does provide specif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[q10] Can a Halfling use a longbow?\n",
      "  Distances: ['0.1621', '0.1883', '0.2424']\n",
      "  Expected:  Halflings may not use Large weapons and must use Medium weapons two-handed. A longbow is a Large weapon, so a Halfling c\n",
      "  Model:     Yes, a Halfling can use a longbow. Despite their small stature, Halflings are not restricted from using longbows, which \n"
     ]
    }
   ],
   "source": [
    "print(\"RAG-ENHANCED: With retrieved context\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rag_answers = {}\n",
    "\n",
    "for q in questions:\n",
    "    answer, context, distances = ask_with_rag(q[\"question\"], collection)\n",
    "    rag_answers[q[\"id\"]] = {\n",
    "        \"answer\": answer,\n",
    "        \"distances\": distances,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n[{q['id']}] {q['question']}\")\n",
    "    print(f\"  Distances: {[f'{d:.4f}' for d in distances]}\")\n",
    "    print(f\"  Expected:  {q['expected'][:120]}\")\n",
    "    print(f\"  Model:     {answer[:120]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b2c3d0",
   "metadata": {},
   "source": [
    "## 3.6 Side-by-Side Comparison\n",
    "\n",
    "Now we put the baseline and RAG results next to each other. For each question, compare what the model said without context versus what it said with retrieved document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b2c3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:00:59.258739Z",
     "iopub.status.busy": "2026-02-21T14:00:59.258608Z",
     "iopub.status.idle": "2026-02-21T14:00:59.262810Z",
     "shell.execute_reply": "2026-02-21T14:00:59.262261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARISON: BASELINE vs. RAG\n",
      "======================================================================\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "[q01] What happens if a Thief fails an Open Locks attempt?\n",
      "  Category: explicit_rule\n",
      "  Expected: The Thief must wait until gaining another level of experience before trying again. It may only be tried once per lock.\n",
      "\n",
      "  BASELINE (no context):\n",
      "    If a Thief fails an Open Locks attempt in Basic Fantasy RPG, they can try again immediately, but there's a cumulative -1 penalty for each failure. This penalty resets to 0 after a successful attempt. \n",
      "\n",
      "  RAG (with retrieved context):\n",
      "    If a Thief fails an Open Locks attempt, they cannot try again until they have gained another level of experience.\n",
      "    Retrieval distances: ['0.1897', '0.2443', '0.2502']\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "[q04] What is the saving throw for a 3rd level Fighter against Dragon Breath?\n",
      "  Category: table_lookup\n",
      "  Expected: Based on the Fighter saving throw table, a 3rd level Fighter has a Dragon Breath saving throw of 15.\n",
      "\n",
      "  BASELINE (no context):\n",
      "    In Basic Fantasy RPG, a 3rd level Fighter would make a \"Save vs. Breath Weapon\" for a Dragon's breath attack. This is not a standard saving throw listed in the core rules, but it's typically handled a\n",
      "\n",
      "  RAG (with retrieved context):\n",
      "    The context does not provide specific saving throw figures for a 3rd level Fighter against Dragon Breath. However, it does mention that the saving throw vs. Dragon Breath is used for creatures with br\n",
      "    Retrieval distances: ['0.2249', '0.2582', '0.2635']\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "[q05] How does a Cleric turn undead?\n",
      "  Category: multi_step_rule\n",
      "  Expected: The Cleric rolls 2d6 and compares the result to the Turn Undead table. Success depends on the Cleric's level and the type of undead.\n",
      "\n",
      "  BASELINE (no context):\n",
      "    In Basic Fantasy RPG, a Cleric can turn undead using the \"Turn Undead\" ability. Here's how it works:\n",
      "\n",
      "1. The Cleric must be of a level that grants this ability. This usually starts from 1st level.\n",
      "\n",
      "2.\n",
      "\n",
      "  RAG (with retrieved context):\n",
      "    A Cleric turns undead by rolling a 20-sided die (1d20) and then referencing their level on the Cleric's vs. Undead table. The GM cross-references the Cleric's level with the type or Hit Dice of the un\n",
      "    Retrieval distances: ['0.1782', '0.1891', '0.2030']\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "[q07] What is the difference between a retainer and a hireling?\n",
      "  Category: terminology\n",
      "  Expected: Retainers are NPCs who accompany the party on adventures and gain experience. Hirelings are hired for specific non-adventuring tasks.\n",
      "\n",
      "  BASELINE (no context):\n",
      "    In Basic Fantasy RPG, both retainers and hirelings are NPCs who can assist the player characters, but they differ in their level of loyalty and commitment:\n",
      "\n",
      "1. Hirelings: These are typically one-time \n",
      "\n",
      "  RAG (with retrieved context):\n",
      "    The context does not explicitly define the difference between a retainer and a hireling. However, it does provide specific rules and details about retainers. By implication, a hireling could be a broa\n",
      "    Retrieval distances: ['0.1546', '0.2220', '0.2292']\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "[q10] Can a Halfling use a longbow?\n",
      "  Category: implicit_reasoning\n",
      "  Expected: Halflings may not use Large weapons and must use Medium weapons two-handed. A longbow is a Large weapon, so a Halfling cannot use one.\n",
      "\n",
      "  BASELINE (no context):\n",
      "    Yes, in Basic Fantasy RPG, Halflings can use longbows. There are no racial restrictions in the rules that prevent Halflings from using longbows or any other type of bow. However, due to their small si\n",
      "\n",
      "  RAG (with retrieved context):\n",
      "    Yes, a Halfling can use a longbow. Despite their small stature, Halflings are not restricted from using longbows, which are considered Medium weapons. However, they must wield it with both hands due t\n",
      "    Retrieval distances: ['0.1621', '0.1883', '0.2424']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: BASELINE vs. RAG\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\"[{q['id']}] {q['question']}\")\n",
    "    print(f\"  Category: {q['category']}\")\n",
    "    print(f\"  Expected: {q['expected']}\")\n",
    "    print()\n",
    "    print(f\"  BASELINE (no context):\")\n",
    "    print(f\"    {baseline_answers[q['id']][:200]}\")\n",
    "    print()\n",
    "    print(f\"  RAG (with retrieved context):\")\n",
    "    print(f\"    {rag_answers[q['id']]['answer'][:200]}\")\n",
    "    print(f\"    Retrieval distances: {[f'{d:.4f}' for d in rag_answers[q['id']]['distances']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b2c3d2",
   "metadata": {},
   "source": [
    "## 3.7 What This Tells Us\n",
    "\n",
    "Compare the two columns for each question:\n",
    "\n",
    "- **Baseline answers** are vague, hedge with phrases like \"it depends\" or \"in many RPG systems,\" or confidently state incorrect specifics. The model is interpolating from its general training data, not citing actual rules.\n",
    "\n",
    "- **RAG answers** are grounded in the actual document text. When the retrieval finds the right chunk, the model can read and cite specific rules, table values, and definitions.\n",
    "\n",
    "### Where RAG Works Well\n",
    "\n",
    "RAG excels at **explicit rule lookups** and **terminology questions** where the answer is stated directly in a single passage. If the retriever finds the right chunk, the model just needs to read it.\n",
    "\n",
    "### Where RAG Has Limits\n",
    "\n",
    "RAG struggles when:\n",
    "- The answer requires reasoning across **multiple separate sections** of the document\n",
    "- The answer requires reading a **table** and extracting a specific value\n",
    "- The question uses different phrasing than the document, causing the retriever to miss the best chunk\n",
    "\n",
    "These failure modes do not mean RAG is broken. They mean that RAG alone may not be sufficient for all question types. That is the signal that tells you whether to stop here or escalate to more expensive techniques.\n",
    "\n",
    "### The Decision Framework\n",
    "\n",
    "1. **RAG solves it** → Deploy RAG. No model changes needed. This is the cheapest option.\n",
    "2. **RAG gets close but not reliable** → Try inference-time scaling (Best-of-N). Let the model try multiple times and pick the best answer. Still no model changes.\n",
    "3. **Neither works** → The gap is in the model's weights. Model adaptation (fine-tuning) is justified.\n",
    "\n",
    "This progression — RAG → inference-time scaling → fine-tuning — is the central framework of this workshop. Each step is more expensive and more invasive than the last. You escalate only when you have evidence that the cheaper approach is insufficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
