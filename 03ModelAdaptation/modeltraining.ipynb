{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e44119-fb72-4524-8ccd-da06e82fcebb",
   "metadata": {},
   "source": [
    "# 3. Model Adaptation: Changing What the Model Knows\n",
    "\n",
    "> **GPU Required.** This section requires a CUDA-capable GPU with at least 40GB of memory. The lab environment provides an NVIDIA L40S (46GB). If you are running this outside the lab without GPU access, you can follow the setup and data preparation cells, but training will not execute. Pre-built outputs are provided for that scenario.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "We have now exhausted the options that do not involve changing the model. RAG gave us grounded answers for 6 out of 10 questions. Inference-time scaling (Best-of-N) recovered 2 more. But for the remaining failures, particularly questions requiring implicit domain reasoning, every approach hit the same wall: the model's weights do not contain a reliable path to the correct answer.\n",
    "\n",
    "This is where model adaptation becomes justified. Not because it is the next thing on the list, but because two independent evaluation methods pointed to the same conclusion: the gap is in the model, not in the pipeline.\n",
    "\n",
    "We will use **OSFT (Orthogonal Subspace Fine-Tuning)**, a parameter-efficient method that updates only the least critical directions in the model's weight matrices while preserving the most important ones. This is not full fine-tuning. It is a controlled modification designed to teach the model new domain knowledge without destroying what it already knows.\n",
    "\n",
    "The training data comes directly from Section 2: synthetic question-answer pairs generated from the customer's own documents. The model comes from HuggingFace. The training runs locally on GPU.\n",
    "\n",
    "By the end of this section, you will have an adapted model and evidence of whether the adaptation closed the gaps that RAG and inference-time scaling could not.\n",
    "\n",
    "## 3.1 Install Training Hub\n",
    "\n",
    "`training_hub` is an open-source library from the Red Hat AI Innovation Team. It provides a single Python interface for multiple post-training algorithms, each backed by a community implementation. You call a function, pass your model and data, and the library handles the rest.\n",
    "\n",
    "The `[cuda]` extra installs GPU dependencies including PyTorch with CUDA support and flash-attention. \n",
    "\n",
    "_This step may take several minutes._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7cfac5-2837-4848-b1a9-425f23ac7035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install base package first (provides torch, packaging, wheel, ninja)\n",
    "! pip install training-hub -q\n",
    "\n",
    "# Then install CUDA extras\n",
    "! pip install training-hub[cuda] --no-build-isolation -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc4bfa9-29b8-4eb8-922b-f5cc621cbcb2",
   "metadata": {},
   "source": [
    "**Note:** You may see dependency conflict warnings from pip. These are advisory, not fatal. The lab environment has many pre-installed packages (Docling, KFP, Feast, etc.) that pin older versions of shared dependencies like `transformers` and `click`. As long as the import in the next cell succeeds, the conflicts do not affect training_hub.\n",
    "\n",
    "If `flash-attn` fails to build, that is also acceptable. Training will fall back to standard attention. Flash-attention is a performance optimization, not a requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986c1cad-d6f3-4f7e-b165-ea1a4866ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu128 for torchao version 0.16.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_hub imported successfully\n",
      "Available algorithms: ['sft', 'osft', 'lora_sft']\n"
     ]
    }
   ],
   "source": [
    "# Verify the install\n",
    "from training_hub import AlgorithmRegistry\n",
    "print(\"training_hub imported successfully\")\n",
    "print(f\"Available algorithms: {AlgorithmRegistry.list_algorithms()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a746c1-3b23-41d7-b6f4-3dd143d845b1",
   "metadata": {},
   "source": [
    "## 3.2 Environment Setup\n",
    "\n",
    "Same credentials, same endpoint pattern. We reuse the `.env` file and config helper from earlier sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "745ee1b5-c5e6-4dc7-b2f3-140169c24d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: https://litellm-prod.apps.maas.redhatworkshops.io/v1\n",
      "API Key:  sk-UFHcL...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from config import API_KEY as key, ENDPOINT_BASE as endpoint_base\n",
    "\n",
    "print(f\"Endpoint: {endpoint_base}\")\n",
    "print(f\"API Key:  {key[:8]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06b1f20-50e5-4e4e-aaf3-cd53211a2817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available:  True\n",
      "GPU:             NVIDIA L40S\n",
      "Memory:          47.7 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:             {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory:          {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training cells will not execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6153d2b-008f-43c2-b3c3-42366975e130",
   "metadata": {},
   "source": [
    "**Important:** OSFT training on an 8B model requires approximately 35-40GB of GPU memory. The L40S provides 46GB, which is sufficient but does not leave much margin.\n",
    "\n",
    "If you ran Sections 1 or 2 in this same kernel session, embedding models or other objects may still be occupying GPU memory. Before continuing, **restart your kernel** (Kernel > Restart) and then run only the cells in this section from the top. This ensures the full 46GB is available for training.\n",
    "\n",
    "You do not need to re-run Sections 1 or 2. The training data file (`synthetic_qa_pairs.csv`) is already saved to disk from Section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c1ad7-db78-452c-abd8-c211dfb869f3",
   "metadata": {},
   "source": [
    "## 3.3 Discover Available Algorithms\n",
    "\n",
    "Before writing any training code, we ask the library what it supports. This is the same discovery-first pattern we used with `sdg_hub` in Section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a3a7e9-df28-4b5e-8c39-531474fab90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available algorithms: ['sft', 'osft', 'lora_sft']\n",
      "  sft: backends = ['instructlab-training']\n",
      "  osft: backends = ['mini-trainer']\n",
      "  lora_sft: backends = ['unsloth']\n"
     ]
    }
   ],
   "source": [
    "from training_hub import AlgorithmRegistry\n",
    "\n",
    "algorithms = AlgorithmRegistry.list_algorithms()\n",
    "print(\"Available algorithms:\", algorithms)\n",
    "\n",
    "for algo_name in algorithms:\n",
    "    backends = AlgorithmRegistry.list_backends(algo_name)\n",
    "    print(f\"  {algo_name}: backends = {backends}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab612e4-0d3e-434a-b811-b828df2076c8",
   "metadata": {},
   "source": [
    "You should see three algorithms: `sft`, `osft`, and `lora_sft`. Each maps to one or more backend implementations.\n",
    "\n",
    "**SFT (Supervised Fine-Tuning)** updates all model weights. It is the most straightforward approach but also the most destructive: the model can lose general capability while learning domain-specific behavior. It is also the most memory-intensive.\n",
    "\n",
    "**OSFT (Orthogonal Subspace Fine-Tuning)** is more surgical. It analyzes the model's weight matrices via singular value decomposition, identifies which directions are most critical to the model's existing behavior, freezes those, and only updates the orthogonal (least critical) directions. The `unfreeze_rank_ratio` parameter controls how much of the model is trainable. At `0.25`, 75% of the model's most critical weights are frozen and 25% are available for new learning.\n",
    "\n",
    "**LoRA SFT (Low-Rank Adaptation)** adds small trainable matrices alongside the frozen base weights. It is widely used and well understood. LoRA and OSFT solve similar problems from different angles: LoRA adds capacity, OSFT reallocates existing capacity.\n",
    "\n",
    "We use OSFT because it is developed by the same team that built the rest of this toolkit, and its continual learning properties are well suited to iterative domain adaptation. When a customer asks \"will fine-tuning break what the model already does well?\", OSFT gives you a concrete answer: we control exactly how much of the model changes, and we protect what matters most.\n",
    "\n",
    "### 3.3.1 Inspect OSFT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8975fb-c6a5-4577-83a0-6963276fc50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required parameters:\n",
      "  model_path: <class 'str'>\n",
      "  data_path: <class 'str'>\n",
      "  unfreeze_rank_ratio: <class 'float'>\n",
      "  effective_batch_size: <class 'int'>\n",
      "  max_tokens_per_gpu: <class 'int'>\n",
      "  max_seq_len: <class 'int'>\n",
      "  learning_rate: <class 'float'>\n",
      "  ckpt_output_dir: <class 'str'>\n",
      "\n",
      "Optional parameters (26 total):\n",
      "  target_patterns: list[str]\n",
      "  seed: <class 'int'>\n",
      "  use_liger: <class 'bool'>\n",
      "  lr_scheduler: <class 'str'>\n",
      "  warmup_steps: <class 'int'>\n",
      "  lr_scheduler_kwargs: <class 'dict'>\n",
      "  beta1: <class 'float'>\n",
      "  beta2: <class 'float'>\n",
      "  eps: <class 'float'>\n",
      "  weight_decay: <class 'float'>\n",
      "  checkpoint_at_epoch: <class 'bool'>\n",
      "  save_final_checkpoint: <class 'bool'>\n",
      "  num_epochs: <class 'int'>\n",
      "  use_processed_dataset: <class 'bool'>\n",
      "  unmask_messages: <class 'bool'>\n",
      "  ... and 11 more\n"
     ]
    }
   ],
   "source": [
    "from training_hub import create_algorithm\n",
    "\n",
    "osft_algo = create_algorithm('osft')\n",
    "\n",
    "required_params = osft_algo.get_required_params()\n",
    "print(\"Required parameters:\")\n",
    "for k, v in required_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print()\n",
    "optional_params = osft_algo.get_optional_params()\n",
    "print(f\"Optional parameters ({len(optional_params)} total):\")\n",
    "for k, v in list(optional_params.items())[:15]:\n",
    "    print(f\"  {k}: {v}\")\n",
    "if len(optional_params) > 15:\n",
    "    print(f\"  ... and {len(optional_params) - 15} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ce73e-5b25-4225-b1e6-ae4bb8279e97",
   "metadata": {},
   "source": [
    "The required parameters are minimal: \n",
    "* a model path,\n",
    "* a data path, and\n",
    "* an output directory.\n",
    "\n",
    "Everything else has sensible defaults. This is intentional.\n",
    "\n",
    "Take a moment to appreciate what this means. \n",
    "\n",
    "Under the hood, OSFT is performing singular value decomposition on every weight matrix in an 8-billion parameter model, ranking directions by criticality, freezing the most important ones, configuring gradient computation only for the orthogonal subspace, managing mixed-precision training, handling checkpointing, and coordinating all of it across GPU memory boundaries. That is a significant amount of engineering.\n",
    "\n",
    "And the interface is three required arguments and a function call.\n",
    "\n",
    "The library absorbs the complexity of backend configuration, distributed setup, and checkpointing so that the interface stays focused on the decisions that actually matter: which model, which data, which hyperparameters. This is the same design philosophy behind `sdg_hub` and `its_hub`. The field should not need a PhD in optimization theory to run a training job. They need to make good decisions about inputs and interpret the outputs. The tooling handles the rest.\n",
    "\n",
    "## 3.4 Prepare Training Data\n",
    "\n",
    "We take the synthetic QA pairs from Section 2 and convert them into the format the training pipeline expects. The standard for chat-based fine-tuning is JSONL where each line contains a `messages` array representing one training conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35bb2f88-98ce-4540-9209-11776c3b9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 QA pairs from Section 2\n",
      "Columns: ['question', 'response', 'faithfulness_judgment']\n",
      "\n",
      "  Q1: What is the Thief's Open Locks ability score at level 8?...\n",
      "  A1: At level 8, the Thief's Open Locks ability score is 60. This information is foun...\n",
      "\n",
      "  Q2: How does the Thief's Move Silently ability progress from level 1 to level 15?...\n",
      "  A2: Based on the provided document, the Thief's \"Move Silently\" ability progresses a...\n",
      "\n",
      "  Q3: Can a GM adjust the Thief's ability scores, and for what reason?...\n",
      "  A3: Yes, a Game Master (GM) can adjust a Thief's ability scores in certain situation...\n",
      "\n",
      "  Q4: Why might a GM apply a situational adjustment when a Thief is climbing a wall?...\n",
      "  A4: A Game Master (GM) might apply a situational adjustment when a Thief is climbing...\n",
      "\n",
      "  Q5: What is the Thief's Climb Walls ability score at level 17?...\n",
      "  A5: At level 17, the Thief's Climb Walls ability score is 96. This can be found in t...\n",
      "\n",
      "  Q6: If a Thief is level 10, what is their Pick Pockets ability score?...\n",
      "  A6: If a Thief is level 10, their Pick Pockets ability score is 74. According to the...\n",
      "\n",
      "  Q7: What does a Thief gain when they advance from level 3 to level 4?...\n",
      "  A7: When a Thief advances from level 3 to level 4, they gain the following increases...\n",
      "\n",
      "  Q8: Describe a situation where a GM might apply a bonus to a Thief's ability score....\n",
      "  A8: In a typical gameplay scenario, a Dungeon Master (GM) might apply a bonus to a T...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the SDG output from Section 2\n",
    "qa_df = pd.read_csv(\"../02SyntheticDataGen/synthetic_qa_pairs.csv\")\n",
    "\n",
    "print(f\"Loaded {len(qa_df)} QA pairs from Section 2\")\n",
    "print(f\"Columns: {list(qa_df.columns)}\")\n",
    "\n",
    "for i, row in qa_df.iterrows():\n",
    "    print(f\"\\n  Q{i+1}: {row['question'][:80]}...\")\n",
    "    print(f\"  A{i+1}: {row['response'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78416b9e-5b8c-43ef-8ec4-a29abf2298a1",
   "metadata": {},
   "source": [
    "Now we convert the CSV into JSONL chat format. Each training example becomes a three-turn conversation: a system prompt that establishes the model's role, a user question, and an assistant response. The model learns to produce the assistant turn given the other two.\n",
    "\n",
    "The system prompt is a design choice. It tells the model who it is during training, and the same prompt should be used at inference time. If you change the persona later, the model may not behave as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e376445-c836-40bb-87f8-228243afc7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 8 examples to training_data.jsonl\n",
      "\n",
      "First example structure:\n",
      "  Keys: ['messages']\n",
      "  Roles: ['system', 'user', 'assistant']\n",
      "  User: What is the Thief's Open Locks ability score at level 8?...\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a rules expert for the Basic Fantasy Role-Playing Game. \"\n",
    "    \"Answer questions accurately based on the official rules. \"\n",
    "    \"Be specific and cite page references or table values where possible.\"\n",
    ")\n",
    "\n",
    "# Convert to chat messages JSONL format\n",
    "training_data = []\n",
    "for _, row in qa_df.iterrows():\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": row[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": row[\"response\"]},\n",
    "        ]\n",
    "    }\n",
    "    training_data.append(example)\n",
    "\n",
    "# Write JSONL\n",
    "data_path = \"training_data.jsonl\"\n",
    "with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in training_data:\n",
    "        f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(training_data)} examples to {data_path}\")\n",
    "\n",
    "# Verify structure\n",
    "with open(data_path, \"r\") as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "print(f\"\\nFirst example structure:\")\n",
    "print(f\"  Keys: {list(first_line.keys())}\")\n",
    "print(f\"  Roles: {[m['role'] for m in first_line['messages']]}\")\n",
    "print(f\"  User: {first_line['messages'][1]['content'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3ed1f-8ef1-4b39-b331-730534d95e29",
   "metadata": {},
   "source": [
    "Six training examples is very small. In a production engagement, you would generate hundreds or thousands of pairs across the full document corpus, filter on faithfulness, deduplicate, and review samples before committing to training. We use six because the goal is to demonstrate the process and observe the behavior, not to produce a production-quality adapter.\n",
    "\n",
    "Even with six examples, if the training works correctly, the model should show measurable change on questions that directly overlap with the training data. Whether it generalizes beyond those specific examples is a separate question, and one that a larger dataset would answer.\n",
    "\n",
    "## 3.5 Download the Base Model\n",
    "\n",
    "OSFT operates on the model weights directly, so the model must be on disk, not behind an API. We download the same Granite model we have been using throughout the workshop.\n",
    "\n",
    "This downloads approximately 16GB and may take several minutes depending on network speed.\n",
    "\n",
    "In tests, this took under 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb58176-7009-412b-9886-dbffcf970f49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at ./models/granite-3.2-8b-instruct, skipping download.\n",
      "\n",
      "Model directory: 14 items, 4 safetensor shards\n",
      "Total size:      16.3 GB\n",
      "Elapsed:         0m 0s\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_ID = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "LOCAL_MODEL_DIR = \"./models/granite-3.2-8b-instruct\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if os.path.exists(LOCAL_MODEL_DIR) and len(os.listdir(LOCAL_MODEL_DIR)) > 5:\n",
    "    print(f\"Model already exists at {LOCAL_MODEL_DIR}, skipping download.\")\n",
    "else:\n",
    "    print(f\"Downloading {MODEL_ID}...\")\n",
    "    print(\"This may take several minutes.\")\n",
    "    snapshot_download(\n",
    "        repo_id=MODEL_ID,\n",
    "        local_dir=LOCAL_MODEL_DIR,\n",
    "    )\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "\n",
    "# Verify\n",
    "model_files = [f for f in os.listdir(LOCAL_MODEL_DIR) if not f.startswith('.')]\n",
    "safetensor_files = [f for f in model_files if f.endswith('.safetensors')]\n",
    "total_size = sum(\n",
    "    os.path.getsize(os.path.join(LOCAL_MODEL_DIR, f)) for f in model_files\n",
    ")\n",
    "\n",
    "print(f\"\\nModel directory: {len(model_files)} items, {len(safetensor_files)} safetensor shards\")\n",
    "print(f\"Total size:      {total_size / 1e9:.1f} GB\")\n",
    "print(f\"Elapsed:         {minutes}m {seconds}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8da187-57ac-48d2-b8ff-7b00a111a906",
   "metadata": {},
   "source": [
    "## 3.6 Run OSFT Training\n",
    "\n",
    "This is the core of the section: a single function call that trains the model.\n",
    "\n",
    "We set conservative parameters for a lab environment on a single L40S:\n",
    "\n",
    "- `unfreeze_rank_ratio=0.25`: 75% of critical weights frozen, 25% trainable\n",
    "- `num_epochs=5`: Multiple passes over our small dataset\n",
    "- `effective_batch_size=4`: Small batch for memory safety\n",
    "- `max_seq_len=1024`: Our training examples are short, no need for longer\n",
    "- `max_tokens_per_gpu=2048`: Hard memory cap per GPU\n",
    "- `learning_rate=5e-6`: Conservative to avoid catastrophic forgetting\n",
    "\n",
    "The `RUN_LIVE` toggle works the same way as previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b8fff1-1e62-43c6-ab9a-8ddc0ec911e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to run training live. Set to False to use pre-built adapter.\n",
    "RUN_LIVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad7e62c-a2c1-412a-8619-e03eb61d3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install 'training-hub[lora]' -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bc10b23-54e6-422d-847c-3f7ee21a5417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA SFT training...\n",
      "  Model:          ./models/granite-3.2-8b-instruct\n",
      "  Data:           training_data.jsonl\n",
      "  Output:         ./lora_output\n",
      "  Epochs:         5\n",
      "  Batch size:     4\n",
      "  Learning rate:  5e-6\n",
      "\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed editing tqdm to replace Inductor Compilation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/training_hub/algorithms/lora.py:17: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import dependencies for Unsloth backend: cannot import name 'CompileCounterInt' from 'torch._dynamo.utils' (/opt/app-root/lib64/python3.12/site-packages/torch/_dynamo/utils.py)\nInstall LoRA dependencies with: pip install 'training-hub[lora]'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/training_hub/algorithms/lora.py:17\u001b[39m, in \u001b[36mUnslothLoRABackend.execute_training\u001b[39m\u001b[34m(self, algorithm_params)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer, SFTConfig\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/unsloth/__init__.py:77\u001b[39m\n\u001b[32m     69\u001b[39m         \u001b[38;5;66;03m# if os.environ.get(\"UNSLOTH_DISABLE_AUTO_UPDATES\", \"0\") == \"0\":\u001b[39;00m\n\u001b[32m     70\u001b[39m         \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[32m     71\u001b[39m         \u001b[38;5;66;03m#         os.system(\"pip install --upgrade --no-cache-dir --no-deps unsloth_zoo\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m#         except:\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/unsloth_zoo/__init__.py:191\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m os\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtemporary_patches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    192\u001b[39m     encode_conversations_with_harmony,\n\u001b[32m    193\u001b[39m )\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrl_environments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    195\u001b[39m     check_python_modules,\n\u001b[32m    196\u001b[39m     create_locked_down_function,\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m     launch_openenv,\n\u001b[32m    201\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/unsloth_zoo/temporary_patches/__init__.py:21\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgemma3n\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgpt_oss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/unsloth_zoo/temporary_patches/gemma3n.py:51\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# We only execute this for float16 so it's not always executed\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# TEMPORARY_PATCHES.append(patch_Gemma3nConvNormAct_forward)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;129m@torch_compile\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mGemma3nRMSNorm_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# Llama does x.to(float16) * w whilst Gemma2 is (x * w).to(float16)\u001b[39;00m\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/pull/29402\u001b[39;00m\n\u001b[32m     55\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._norm(x.float()) * \u001b[38;5;28mself\u001b[39m.weight.float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/torch/__init__.py:2572\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[39m\n\u001b[32m   2546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile\u001b[39m(\n\u001b[32m   2547\u001b[39m     model: _Callable[_InputT, _RetT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2548\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m     | _Callable[_InputT, _RetT]\n\u001b[32m   2558\u001b[39m ):\n\u001b[32m   2559\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2560\u001b[39m \u001b[33;03m    Optimizes given model/function using TorchDynamo and specified backend.\u001b[39;00m\n\u001b[32m   2561\u001b[39m \u001b[33;03m    If you are compiling an :class:`torch.nn.Module`, you can also use :meth:`torch.nn.Module.compile`\u001b[39;00m\n\u001b[32m   2562\u001b[39m \u001b[33;03m    to compile the module inplace without changing its structure.\u001b[39;00m\n\u001b[32m   2563\u001b[39m \n\u001b[32m   2564\u001b[39m \u001b[33;03m    Concretely, for every frame executed within the compiled region, we will attempt\u001b[39;00m\n\u001b[32m   2565\u001b[39m \u001b[33;03m    to compile it and cache the compiled result on the code object for future\u001b[39;00m\n\u001b[32m   2566\u001b[39m \u001b[33;03m    use.  A single frame may be compiled multiple times if previous compiled\u001b[39;00m\n\u001b[32m   2567\u001b[39m \u001b[33;03m    results are not applicable for subsequent calls (this is called a \"guard\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[33;03m    failure\"), you can use TORCH_LOGS=guards to debug these situations.\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[33;03m    Multiple compiled results can be associated with a frame up to\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[33;03m    ``torch._dynamo.config.recompile_limit``, which defaults to 8; at which\u001b[39;00m\n\u001b[32m   2571\u001b[39m \u001b[33;03m    point we will fall back to eager.  Note that compile caches are per\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m \u001b[33;03m    *code object*, not frame; if you dynamically create multiple copies of a\u001b[39;00m\n\u001b[32m   2573\u001b[39m \u001b[33;03m    function, they will all share the same code cache.\u001b[39;00m\n\u001b[32m   2574\u001b[39m \n\u001b[32m   2575\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[33;03m       model (Callable or None): Module/function to optimize\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[33;03m       fullgraph (bool): If False (default), torch.compile attempts to discover compilable regions\u001b[39;00m\n\u001b[32m   2578\u001b[39m \u001b[33;03m        in the function that it will optimize. If True, then we require that the entire function be\u001b[39;00m\n\u001b[32m   2579\u001b[39m \u001b[33;03m        capturable into a single graph. If this is not possible (that is, if there are graph breaks),\u001b[39;00m\n\u001b[32m   2580\u001b[39m \u001b[33;03m        then this will raise an error. This also opts into unbacked semantics, notably it will turn on\u001b[39;00m\n\u001b[32m   2581\u001b[39m \u001b[33;03m        capture_scalar_outputs and capture_dynamic_output_shape_ops on by default.\u001b[39;00m\n\u001b[32m   2582\u001b[39m \u001b[33;03m       dynamic (bool or None): Use dynamic shape tracing.  When this is True, we will up-front attempt\u001b[39;00m\n\u001b[32m   2583\u001b[39m \u001b[33;03m        to generate a kernel that is as dynamic as possible to avoid recompilations when\u001b[39;00m\n\u001b[32m   2584\u001b[39m \u001b[33;03m        sizes change.  This may not always work as some operations/optimizations will\u001b[39;00m\n\u001b[32m   2585\u001b[39m \u001b[33;03m        force specialization; use TORCH_LOGS=dynamic to debug overspecialization.\u001b[39;00m\n\u001b[32m   2586\u001b[39m \u001b[33;03m        When this is False, we will NEVER generate dynamic kernels, we will always specialize.\u001b[39;00m\n\u001b[32m   2587\u001b[39m \u001b[33;03m        By default (None), we automatically detect if dynamism has occurred and compile a more\u001b[39;00m\n\u001b[32m   2588\u001b[39m \u001b[33;03m        dynamic kernel upon recompile.\u001b[39;00m\n\u001b[32m   2589\u001b[39m \u001b[33;03m       backend (str or Callable): backend to be used\u001b[39;00m\n\u001b[32m   2590\u001b[39m \n\u001b[32m   2591\u001b[39m \u001b[33;03m        - \"inductor\" is the default backend, which is a good balance between performance and overhead\u001b[39;00m\n\u001b[32m   2592\u001b[39m \n\u001b[32m   2593\u001b[39m \u001b[33;03m        - Non experimental in-tree backends can be seen with `torch._dynamo.list_backends()`\u001b[39;00m\n\u001b[32m   2594\u001b[39m \n\u001b[32m   2595\u001b[39m \u001b[33;03m        - Experimental or debug in-tree backends can be seen with `torch._dynamo.list_backends(None)`\u001b[39;00m\n\u001b[32m   2596\u001b[39m \n\u001b[32m   2597\u001b[39m \u001b[33;03m        - To register an out-of-tree custom backend:\u001b[39;00m\n\u001b[32m   2598\u001b[39m \u001b[33;03m          https://pytorch.org/docs/main/torch.compiler_custom_backends.html#registering-custom-backends\u001b[39;00m\n\u001b[32m   2599\u001b[39m \u001b[33;03m       mode (str): Can be either \"default\", \"reduce-overhead\", \"max-autotune\" or \"max-autotune-no-cudagraphs\"\u001b[39;00m\n\u001b[32m   2600\u001b[39m \n\u001b[32m   2601\u001b[39m \u001b[33;03m        - \"default\" is the default mode, which is a good balance between performance and overhead\u001b[39;00m\n\u001b[32m   2602\u001b[39m \n\u001b[32m   2603\u001b[39m \u001b[33;03m        - \"reduce-overhead\" is a mode that reduces the overhead of python with CUDA graphs,\u001b[39;00m\n\u001b[32m   2604\u001b[39m \u001b[33;03m          useful for small batches.  Reduction of overhead can come at the cost of more memory\u001b[39;00m\n\u001b[32m   2605\u001b[39m \u001b[33;03m          usage, as we will cache the workspace memory required for the invocation so that we\u001b[39;00m\n\u001b[32m   2606\u001b[39m \u001b[33;03m          do not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed\u001b[39;00m\n\u001b[32m   2607\u001b[39m \u001b[33;03m          to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.\u001b[39;00m\n\u001b[32m   2608\u001b[39m \u001b[33;03m          There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints\u001b[39;00m\n\u001b[32m   2609\u001b[39m \u001b[33;03m          to debug.\u001b[39;00m\n\u001b[32m   2610\u001b[39m \n\u001b[32m   2611\u001b[39m \u001b[33;03m        - \"max-autotune\" is a mode that leverages Triton or template based matrix multiplications\u001b[39;00m\n\u001b[32m   2612\u001b[39m \u001b[33;03m          on supported devices and Triton based convolutions on GPU.\u001b[39;00m\n\u001b[32m   2613\u001b[39m \u001b[33;03m          It enables CUDA graphs by default on GPU.\u001b[39;00m\n\u001b[32m   2614\u001b[39m \n\u001b[32m   2615\u001b[39m \u001b[33;03m        - \"max-autotune-no-cudagraphs\" is a mode similar to \"max-autotune\" but without CUDA graphs\u001b[39;00m\n\u001b[32m   2616\u001b[39m \n\u001b[32m   2617\u001b[39m \u001b[33;03m        - To see the exact configs that each mode sets you can call `torch._inductor.list_mode_options()`\u001b[39;00m\n\u001b[32m   2618\u001b[39m \n\u001b[32m   2619\u001b[39m \u001b[33;03m       options (dict): A dictionary of options to pass to the backend. Some notable ones to try out are\u001b[39;00m\n\u001b[32m   2620\u001b[39m \n\u001b[32m   2621\u001b[39m \u001b[33;03m        - `epilogue_fusion` which fuses pointwise ops into templates. Requires `max_autotune` to also be set\u001b[39;00m\n\u001b[32m   2622\u001b[39m \n\u001b[32m   2623\u001b[39m \u001b[33;03m        - `max_autotune` which will profile to pick the best matmul configuration\u001b[39;00m\n\u001b[32m   2624\u001b[39m \n\u001b[32m   2625\u001b[39m \u001b[33;03m        - `fallback_random` which is useful when debugging accuracy issues\u001b[39;00m\n\u001b[32m   2626\u001b[39m \n\u001b[32m   2627\u001b[39m \u001b[33;03m        - `shape_padding` which pads matrix shapes to better align loads on GPUs especially for tensor cores\u001b[39;00m\n\u001b[32m   2628\u001b[39m \n\u001b[32m   2629\u001b[39m \u001b[33;03m        - `triton.cudagraphs` which will reduce the overhead of python with CUDA graphs\u001b[39;00m\n\u001b[32m   2630\u001b[39m \n\u001b[32m   2631\u001b[39m \u001b[33;03m        - `trace.enabled` which is the most useful debugging flag to turn on\u001b[39;00m\n\u001b[32m   2632\u001b[39m \n\u001b[32m   2633\u001b[39m \u001b[33;03m        - `trace.graph_diagram` which will show you a picture of your graph after fusion\u001b[39;00m\n\u001b[32m   2634\u001b[39m \n\u001b[32m   2635\u001b[39m \u001b[33;03m        - `guard_filter_fn` that controls which dynamo guards are saved with compilations.\u001b[39;00m\n\u001b[32m   2636\u001b[39m \u001b[33;03m          This is an unsafe feature and there is no backward compatibility guarantee provided\u001b[39;00m\n\u001b[32m   2637\u001b[39m \u001b[33;03m          for dynamo guards as data types.\u001b[39;00m\n\u001b[32m   2638\u001b[39m \u001b[33;03m          For stable helper functions to use, see the documentations in `torch.compiler`, for example:\u001b[39;00m\n\u001b[32m   2639\u001b[39m \u001b[33;03m          - `torch.compiler.skip_guard_on_inbuilt_nn_modules_unsafe`\u001b[39;00m\n\u001b[32m   2640\u001b[39m \u001b[33;03m          - `torch.compiler.skip_guard_on_all_nn_modules_unsafe`\u001b[39;00m\n\u001b[32m   2641\u001b[39m \u001b[33;03m          - `torch.compiler.keep_tensor_guards_unsafe`\u001b[39;00m\n\u001b[32m   2642\u001b[39m \n\u001b[32m   2643\u001b[39m \u001b[33;03m        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`\u001b[39;00m\n\u001b[32m   2644\u001b[39m \u001b[33;03m       disable (bool): Turn torch.compile() into a no-op for testing\u001b[39;00m\n\u001b[32m   2645\u001b[39m \n\u001b[32m   2646\u001b[39m \u001b[33;03m    Example::\u001b[39;00m\n\u001b[32m   2647\u001b[39m \n\u001b[32m   2648\u001b[39m \u001b[33;03m        @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\u001b[39;00m\n\u001b[32m   2649\u001b[39m \u001b[33;03m        def foo(x):\u001b[39;00m\n\u001b[32m   2650\u001b[39m \u001b[33;03m            return torch.sin(x) + torch.cos(x)\u001b[39;00m\n\u001b[32m   2651\u001b[39m \n\u001b[32m   2652\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2653\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msysconfig\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py:944\u001b[39m, in \u001b[36moptimize\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m     _set_error_on_graph_break(self.error_on_graph_break)\n\u001b[32m    940\u001b[39m # Ensure that if an assertion occurs after graph pushes\n\u001b[32m    941\u001b[39m # something onto the DynamicLayerStack then we pop it off (the\n\u001b[32m    942\u001b[39m # constructed graph code isn't guarded with try/finally).\n\u001b[32m    943\u001b[39m #\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m # This used to be a context but putting a `with` here is a noticeable\n\u001b[32m    945\u001b[39m # perf regression (#126293)\n\u001b[32m    946\u001b[39m saved_dynamic_layer_stack_depth = (\n\u001b[32m    947\u001b[39m     torch._C._functorch.get_dynamic_layer_stack_depth()\n\u001b[32m    948\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py:998\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(rebuild_ctx, backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic)\u001b[39m\n\u001b[32m    995\u001b[39m compile_wrapper._torchdynamo_orig_callable = fn  # type: ignore[attr-defined]\n\u001b[32m    997\u001b[39m # when compiling user function instead of nn.Module\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m # provide public api _fn.get_compiler_config()\n\u001b[32m    999\u001b[39m assert not hasattr(compile_wrapper, \"get_compiler_config\")\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py:878\u001b[39m, in \u001b[36mget_compiler_fn\u001b[39m\u001b[34m(compiler_fn)\u001b[39m\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m     filename = inspect.getsourcefile(fn)\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py:33\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     34\u001b[39m     AccuracyError,\n\u001b[32m     35\u001b[39m     backend_accuracy_fails,\n\u001b[32m     36\u001b[39m     BUCK_CMD_PREFIX,\n\u001b[32m     37\u001b[39m     BuckTargetWriter,\n\u001b[32m     38\u001b[39m     extra_imports,\n\u001b[32m     39\u001b[39m     generate_config_string,\n\u001b[32m     40\u001b[39m     generate_env_vars_string,\n\u001b[32m     41\u001b[39m     helper_for_dump_minify,\n\u001b[32m     42\u001b[39m     InputReader,\n\u001b[32m     43\u001b[39m     InputWriter,\n\u001b[32m     44\u001b[39m     minifier_dir,\n\u001b[32m     45\u001b[39m     NNModuleToString,\n\u001b[32m     46\u001b[39m     NopInputReader,\n\u001b[32m     47\u001b[39m     run_fwd_maybe_bwd,\n\u001b[32m     48\u001b[39m     same_two_models,\n\u001b[32m     49\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fx_placeholder_targets\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/torch/_dynamo/debug_utils.py:43\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtesting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rand_strided\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_path_separator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/torch/_dynamo/testing.py:45\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConvertFrameReturn, DynamoFrameType, wrap_guarded_code\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompileCounterInt, same\n\u001b[32m     48\u001b[39m np: Optional[types.ModuleType] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'CompileCounterInt' from 'torch._dynamo.utils' (/opt/app-root/lib64/python3.12/site-packages/torch/_dynamo/utils.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     17\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m result = \u001b[43mlora_sft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOCAL_MODEL_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCKPT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./lora_data_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43meffective_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens_per_gpu\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m elapsed = time.time() - start_time\n\u001b[32m     32\u001b[39m minutes = \u001b[38;5;28mint\u001b[39m(elapsed // \u001b[32m60\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/training_hub/algorithms/lora.py:817\u001b[39m, in \u001b[36mlora_sft\u001b[39m\u001b[34m(model_path, data_path, ckpt_output_dir, backend, lora_r, lora_alpha, lora_dropout, target_modules, num_epochs, effective_batch_size, micro_batch_size, gradient_accumulation_steps, learning_rate, max_seq_len, lr_scheduler, warmup_steps, load_in_4bit, load_in_8bit, bnb_4bit_quant_type, bnb_4bit_compute_dtype, bnb_4bit_use_double_quant, flash_attention, sample_packing, bf16, fp16, tf32, save_steps, eval_steps, logging_steps, save_total_limit, wandb_project, wandb_entity, wandb_run_name, dataset_type, field_messages, field_instruction, field_input, field_output, nproc_per_node, nnodes, node_rank, rdzv_id, rdzv_endpoint, master_addr, master_port, enable_model_splitting, **kwargs)\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_algorithm\n\u001b[32m    816\u001b[39m algorithm = create_algorithm(\u001b[33m'\u001b[39m\u001b[33mlora_sft\u001b[39m\u001b[33m'\u001b[39m, backend)\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_r\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_r\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43meffective_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43meffective_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflash_attention\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflash_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_packing\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_packing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf32\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_entity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwandb_entity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_messages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_instruction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_instruction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnproc_per_node\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnproc_per_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnode_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnode_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrdzv_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrdzv_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrdzv_endpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrdzv_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaster_addr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaster_addr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaster_port\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaster_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_model_splitting\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_model_splitting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/training_hub/algorithms/lora.py:584\u001b[39m, in \u001b[36mLoRASFTAlgorithm.train\u001b[39m\u001b[34m(self, model_path, data_path, ckpt_output_dir, num_epochs, effective_batch_size, learning_rate, max_seq_len, max_tokens_per_gpu, data_output_dir, save_samples, warmup_steps, accelerate_full_state_at_epoch, checkpoint_at_epoch, lora_r, lora_alpha, lora_dropout, target_modules, use_rslora, use_dora, init_lora_weights, rank_pattern, alpha_pattern, loftq_config, load_in_4bit, load_in_8bit, bnb_4bit_quant_type, bnb_4bit_compute_dtype, bnb_4bit_use_double_quant, micro_batch_size, gradient_accumulation_steps, lr_scheduler, weight_decay, max_grad_norm, flash_attention, sample_packing, bf16, fp16, tf32, save_steps, eval_steps, logging_steps, save_total_limit, wandb_project, wandb_entity, wandb_run_name, dataset_type, field_messages, field_instruction, field_input, field_output, nproc_per_node, nnodes, node_rank, rdzv_id, rdzv_endpoint, master_addr, master_port, enable_model_splitting, **kwargs)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# Apply PEFT configuration using the extender\u001b[39;00m\n\u001b[32m    582\u001b[39m params = \u001b[38;5;28mself\u001b[39m.peft_extender.apply_peft_config(params)\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/training_hub/algorithms/lora.py:34\u001b[39m, in \u001b[36mUnslothLoRABackend.execute_training\u001b[39m\u001b[34m(self, algorithm_params)\u001b[39m\n\u001b[32m     29\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTRL is required for Unsloth LoRA training. Install with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtraining-hub[lora]\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     35\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import dependencies for Unsloth backend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mInstall LoRA dependencies with: pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtraining-hub[lora]\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Use all parameters as training parameters\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Note: Torchrun parameters (nproc_per_node, etc.) are handled by the torchrun launcher,\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# not by the Python training code. The training code auto-detects distributed environment\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# via environment variables (WORLD_SIZE, LOCAL_RANK, etc.) set by torchrun.\u001b[39;00m\n\u001b[32m     43\u001b[39m training_params = algorithm_params\n",
      "\u001b[31mImportError\u001b[39m: Failed to import dependencies for Unsloth backend: cannot import name 'CompileCounterInt' from 'torch._dynamo.utils' (/opt/app-root/lib64/python3.12/site-packages/torch/_dynamo/utils.py)\nInstall LoRA dependencies with: pip install 'training-hub[lora]'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from training_hub import lora_sft\n",
    "\n",
    "CKPT_DIR = \"./lora_output\"\n",
    "\n",
    "if RUN_LIVE:\n",
    "    print(\"Starting LoRA SFT training...\")\n",
    "    print(f\"  Model:          {LOCAL_MODEL_DIR}\")\n",
    "    print(f\"  Data:           {data_path}\")\n",
    "    print(f\"  Output:         {CKPT_DIR}\")\n",
    "    print(f\"  Epochs:         5\")\n",
    "    print(f\"  Batch size:     4\")\n",
    "    print(f\"  Learning rate:  5e-6\")\n",
    "    print()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = lora_sft(\n",
    "        model_path=LOCAL_MODEL_DIR,\n",
    "        data_path=data_path,\n",
    "        ckpt_output_dir=CKPT_DIR,\n",
    "        data_output_dir=\"./lora_data_output\",\n",
    "        num_epochs=5,\n",
    "        effective_batch_size=4,\n",
    "        max_tokens_per_gpu=2048,\n",
    "        max_seq_len=1024,\n",
    "        learning_rate=5e-6,\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    minutes = int(elapsed // 60)\n",
    "    seconds = int(elapsed % 60)\n",
    "    print(f\"\\nTraining complete in {minutes}m {seconds}s\")\n",
    "    print(f\"Result: {result}\")\n",
    "else:\n",
    "    print(\"Using pre-built checkpoint. Skipping training.\")\n",
    "    CKPT_DIR = \"../prebuilt/lora_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b53f29-eade-4931-a300-03f7a6901d12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880de66d-239d-49a5-bca4-ddf10a04fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat osft_output/training_log_node0.log | tail -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f122efd-c8e7-4912-8b1c-f2d8481359da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3141603c-764c-45c0-998d-fd4bf7fbc9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45dd995b-9201-44d2-ab9f-105e309c343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2025.11.1\n"
     ]
    }
   ],
   "source": [
    "! pip show unsloth 2>/dev/null | grep Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644dce1-0b9a-4403-958b-2fe01598bcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
