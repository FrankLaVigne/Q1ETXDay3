{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêá Down the Orthogonal Rabbit Hole\n",
    "\n",
    "## Exploring Orthogonal Subspaces in Fine-Tuning AI Models\n",
    "\n",
    "**by Frank La Vigne** ¬∑ Interactive Notebook Edition\n",
    "\n",
    "---\n",
    "\n",
    "| 90%+ Parameter Reduction | Stable Training | Zero Forgetting |\n",
    "|:---:|:---:|:---:|\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [What is OFT?](#1)\n",
    "2. [Orthogonal Matrix Properties ‚Äî Interactive](#2)\n",
    "3. [Visualizing Orthogonal Transformations](#3)\n",
    "4. [Hyperspherical Energy Preservation](#4)\n",
    "5. [The Deeper Rabbit Hole: OSFT](#5)\n",
    "6. [SVD Decomposition Explorer ‚Äî Interactive](#6)\n",
    "7. [Gradient Projection Demo ‚Äî Interactive](#7)\n",
    "8. [Hands-on: NumPy Warm-Up](#8)\n",
    "9. [Hands-on: PyTorch OSFT](#9)\n",
    "10. [OFT vs LoRA ‚Äî Parameter Calculator](#10)\n",
    "11. [Training Dynamics Simulation](#11)\n",
    "12. [Real-World Applications](#12)\n",
    "13. [Wrap-Up](#13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öôÔ∏è Setup\n",
    "\n",
    "Run this cell first to install dependencies and import everything we need."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install numpy torch matplotlib ipywidgets scipy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from scipy.stats import special_ortho_group\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': '#0d0d0f',\n",
    "    'axes.facecolor': '#16161a',\n",
    "    'axes.edgecolor': '#2a2a32',\n",
    "    'axes.labelcolor': '#e4e4e7',\n",
    "    'text.color': '#e4e4e7',\n",
    "    'xtick.color': '#8888a0',\n",
    "    'ytick.color': '#8888a0',\n",
    "    'grid.color': '#1f1f2a',\n",
    "    'grid.alpha': 0.5,\n",
    "    'figure.dpi': 120,\n",
    "    'font.family': 'monospace',\n",
    "    'font.size': 9,\n",
    "})\n",
    "\n",
    "# Utility: generate orthogonal matrix via QR\n",
    "def generate_orthogonal_matrix(dim):\n",
    "    random_matrix = np.random.randn(dim, dim)\n",
    "    q, r = np.linalg.qr(random_matrix)\n",
    "    d = np.diag(np.sign(np.diag(r)))\n",
    "    return q @ d\n",
    "\n",
    "print(\"Setup complete -- all imports loaded.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"1\"></a>\n",
    "## üö™ [1] Through the Looking Glass: What is OFT?\n",
    "\n",
    "Just as Alice stepped through the looking glass and found a world that was familiar yet rearranged, **Orthogonal Fine-Tuning (OFT)** adapts pre-trained models by **rotating** their learned representations rather than distorting them.\n",
    "\n",
    "OFT is a parameter-efficient fine-tuning (PEFT) technique that applies **orthogonal transformations** to weight matrices. Unlike LoRA, OFT preserves the *hyperspherical energy* -- the geometric relationships between neuron activations.\n",
    "\n",
    "### üîÆ The Magic Mirror Properties\n",
    "\n",
    "An orthogonal matrix **Q** satisfies: **Q^T Q = QQ^T = I**\n",
    "\n",
    "- **Preserves distances:** `||Qx|| = ||x||`\n",
    "- **Preserves angles:** The Cheshire Cat's grin keeps its shape\n",
    "- **Identity when transposed:** `Q^T Q = I`\n",
    "- **Represents rotations and reflections:** Turn the mirror, don't bend it\n",
    "- **Determinant is +/-1:** The mirror's magic constant\n",
    "\n",
    "### üé≠ The Looking Glass Formula\n",
    "\n",
    "The key insight: **W' = W x R**\n",
    "\n",
    "Where **W** is the original weight matrix, **R** is an orthogonal matrix learned during fine-tuning, and **W'** is the adapted weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"2\"></a>\n",
    "## üÉè [2] The Queen's Matrix Garden -- Interactive\n",
    "\n",
    "An orthogonal matrix is like the Queen's decree -- it can rearrange the cards (rotate them), but must preserve their ranks and suits. **Run the cell below and click the button** to generate new random orthogonal matrices and verify their properties.\n",
    "\n",
    "> *\"Off with their heads!\" cries the Queen -- but an orthogonal matrix is merciful. It moves the cards without changing their essence.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interactive Matrix Explorer\n",
    "button_gen = widgets.Button(description='Shuffle the Cards', layout=widgets.Layout(width='200px'))\n",
    "output_matrix = widgets.Output()\n",
    "\n",
    "def generate_and_display(_=None):\n",
    "    with output_matrix:\n",
    "        clear_output(wait=True)\n",
    "        Q = generate_orthogonal_matrix(4)\n",
    "        QTQ = Q.T @ Q\n",
    "        det = np.linalg.det(Q)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "        im0 = axes[0].imshow(Q, cmap='RdBu_r', vmin=-1, vmax=1, aspect='equal')\n",
    "        axes[0].set_title('Matrix Q', fontsize=11, fontweight='bold')\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                axes[0].text(j, i, f'{Q[i,j]:.3f}', ha='center', va='center',\n",
    "                           fontsize=8, color='white' if abs(Q[i,j]) > 0.5 else '#aaa')\n",
    "        axes[0].set_xticks([]); axes[0].set_yticks([])\n",
    "        plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "        im1 = axes[1].imshow(QTQ, cmap='RdBu_r', vmin=-1, vmax=1, aspect='equal')\n",
    "        axes[1].set_title('Q^T x Q  (should be Identity)', fontsize=11, fontweight='bold')\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                val = QTQ[i,j]\n",
    "                color = '#4ade80' if (i == j and abs(val - 1) < 0.01) else '#888'\n",
    "                axes[1].text(j, i, f'{val:.3f}', ha='center', va='center',\n",
    "                           fontsize=8, color=color, fontweight='bold' if i==j else 'normal')\n",
    "        axes[1].set_xticks([]); axes[1].set_yticks([])\n",
    "        plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        is_orth = np.allclose(QTQ, np.eye(4), atol=1e-10)\n",
    "        print(f\"  Determinant:       {det:.6f}\")\n",
    "        print(f\"  Is Orthogonal:     {is_orth}\")\n",
    "        print(f\"  Preserves Norm:    True\")\n",
    "        print(f\"  Preserves Angles:  True\")\n",
    "\n",
    "button_gen.on_click(generate_and_display)\n",
    "display(button_gen, output_matrix)\n",
    "generate_and_display()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"3\"></a>\n",
    "## üîÑ [3] Visualizing Orthogonal Transformations\n",
    "\n",
    "Watch how orthogonal transformations preserve geometric relationships while rotating the feature space. **Drag the slider** to change the rotation angle. Notice the dashed lines connecting two reference points stay the same length!\n",
    "\n",
    "> *Everything moves, but relationships stay true. The mirror spins, but nothing warps.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interactive Rotation Visualization\n",
    "np.random.seed(42)\n",
    "n_pts = 80\n",
    "orig_pts = np.random.randn(n_pts, 2) * 1.2\n",
    "\n",
    "angle_slider = widgets.FloatSlider(value=45, min=0, max=360, step=1,\n",
    "    description='Angle:', layout=widgets.Layout(width='500px'),\n",
    "    style={'description_width': '60px'})\n",
    "output_rot = widgets.Output()\n",
    "\n",
    "def draw_rotation(change=None):\n",
    "    angle = angle_slider.value\n",
    "    with output_rot:\n",
    "        clear_output(wait=True)\n",
    "        rad = np.radians(angle)\n",
    "        R = np.array([[np.cos(rad), -np.sin(rad)],\n",
    "                      [np.sin(rad),  np.cos(rad)]])\n",
    "        transformed = orig_pts @ R.T\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(14, 4.2))\n",
    "        for ax in axes:\n",
    "            ax.set_xlim(-4.5, 4.5); ax.set_ylim(-4.5, 4.5)\n",
    "            ax.set_aspect('equal'); ax.grid(True, alpha=0.3)\n",
    "            ax.axhline(0, color='#2a2a3a', linewidth=0.8)\n",
    "            ax.axvline(0, color='#2a2a3a', linewidth=0.8)\n",
    "\n",
    "        axes[0].scatter(orig_pts[:,0], orig_pts[:,1], alpha=0.5, s=12, c='#60a5fa')\n",
    "        axes[0].plot(orig_pts[:2,0], orig_pts[:2,1], '--', color='#60a5fa', alpha=0.5)\n",
    "        axes[0].set_title('Original', fontsize=10)\n",
    "\n",
    "        axes[1].scatter(transformed[:,0], transformed[:,1], alpha=0.7, s=12, c='#fb923c')\n",
    "        axes[1].plot(transformed[:2,0], transformed[:2,1], '--', color='#fb923c', alpha=0.7)\n",
    "        axes[1].set_title(f'Rotated {angle:.0f} degrees', fontsize=10)\n",
    "\n",
    "        axes[2].scatter(orig_pts[:,0], orig_pts[:,1], alpha=0.3, s=10, c='#60a5fa', label='Original')\n",
    "        axes[2].scatter(transformed[:,0], transformed[:,1], alpha=0.6, s=10, c='#fb923c', label='Rotated')\n",
    "        axes[2].plot(orig_pts[:2,0], orig_pts[:2,1], '--', color='#60a5fa', alpha=0.4)\n",
    "        axes[2].plot(transformed[:2,0], transformed[:2,1], '--', color='#fb923c', alpha=0.6)\n",
    "        axes[2].legend(fontsize=8, loc='upper right')\n",
    "        axes[2].set_title('Overlay -- Structure Preserved', fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        d_orig = np.linalg.norm(orig_pts[0] - orig_pts[1])\n",
    "        d_trans = np.linalg.norm(transformed[0] - transformed[1])\n",
    "        print(f\"  Distance (original):    {d_orig:.6f}\")\n",
    "        print(f\"  Distance (transformed): {d_trans:.6f}\")\n",
    "        print(f\"  Preserved: {np.isclose(d_orig, d_trans)}\")\n",
    "\n",
    "angle_slider.observe(draw_rotation, names='value')\n",
    "display(angle_slider, output_rot)\n",
    "draw_rotation()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"4\"></a>\n",
    "## üé© [4] Hyperspherical Energy Preservation\n",
    "\n",
    "### The Mad Hatter's Tea Party Problem\n",
    "\n",
    "**Traditional fine-tuning** is like the Hatter shouting \"Move down!\" -- everyone shifts chaotically. **OFT** is like rotating the entire table. Everyone maintains their relative positions.\n",
    "\n",
    "**Run the cell** to compare how different transformations affect hyperspherical energy. Click regenerate for fresh random features.\n",
    "\n",
    "> *Down here in Wonderland, we don't break what already works -- we just rotate it to see it from a new angle.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Hyperspherical Energy Comparison\n",
    "button_energy = widgets.Button(description='Regenerate', layout=widgets.Layout(width='160px'))\n",
    "output_energy = widgets.Output()\n",
    "\n",
    "def compute_hyperspherical_energy(features):\n",
    "    norms = np.linalg.norm(features, axis=-1, keepdims=True)\n",
    "    normalized = features / (norms + 1e-8)\n",
    "    sims = normalized @ normalized.T\n",
    "    n = features.shape[0]\n",
    "    mask = 1 - np.eye(n)\n",
    "    energy = np.sum(np.abs(sims) * mask) / (n * (n - 1))\n",
    "    return energy\n",
    "\n",
    "def draw_energy(_=None):\n",
    "    with output_energy:\n",
    "        clear_output(wait=True)\n",
    "        n_samp, n_feat = 50, 8\n",
    "        orig = np.random.randn(n_samp, n_feat)\n",
    "\n",
    "        Q = generate_orthogonal_matrix(n_feat)\n",
    "        oft = orig @ Q.T\n",
    "\n",
    "        M = np.random.randn(n_feat, n_feat) * 0.3 + np.eye(n_feat)\n",
    "        rand = orig @ M.T\n",
    "\n",
    "        A = np.random.randn(4, n_feat) * 0.1\n",
    "        B = np.random.randn(n_feat, 4) * 0.1\n",
    "        lora = orig + orig @ A.T @ B.T\n",
    "\n",
    "        energies = {\n",
    "            'Original': compute_hyperspherical_energy(orig),\n",
    "            'OFT': compute_hyperspherical_energy(oft),\n",
    "            'Random': compute_hyperspherical_energy(rand),\n",
    "            'LoRA': compute_hyperspherical_energy(lora),\n",
    "        }\n",
    "        colors_map = {'Original':'#6b7280', 'OFT':'#60a5fa', 'Random':'#f87171', 'LoRA':'#fbbf24'}\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(9, 3.8))\n",
    "        bars = ax.bar(energies.keys(), energies.values(),\n",
    "                      color=[colors_map[k] for k in energies], width=0.5, edgecolor='none')\n",
    "        for bar, (name, val) in zip(bars, energies.items()):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.003,\n",
    "                    f'{val:.4f}', ha='center', va='bottom', fontsize=9, color='#e4e4e7')\n",
    "        ax.set_ylabel('Hyperspherical Energy')\n",
    "        ax.set_title('Energy Comparison -- OFT Preserves Best', fontsize=11, fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.2)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        orig_e = energies['Original']\n",
    "        for name, e in energies.items():\n",
    "            delta = abs(e - orig_e)\n",
    "            marker = 'OK' if delta < 0.01 else '~' if delta < 0.05 else 'X'\n",
    "            print(f\"  [{marker}] {name:10s}: {e:.4f}  (delta = {delta:.4f})\")\n",
    "\n",
    "button_energy.on_click(draw_energy)\n",
    "display(button_energy, output_energy)\n",
    "draw_energy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"5\"></a>\n",
    "## üóùÔ∏è [5] The Deeper Rabbit Hole: OSFT\n",
    "\n",
    "OSFT is all about teaching your model new tricks without it forgetting the old ones. If you've ever fine-tuned a model and watched it suddenly get dumber at stuff it used to know -- that's **catastrophic forgetting**. OSFT is how we fight back.\n",
    "\n",
    "### The Problem: Eating the Wrong Mushroom\n",
    "\n",
    "Standard fine-tuning is like Alice eating random mushrooms -- she might grow taller (learn new tasks) but forget how to get back to normal size.\n",
    "\n",
    "### The Solution: The Caterpillar's Wisdom\n",
    "\n",
    "*\"One side makes you taller, the other makes you shorter.\"* OSFT identifies **safe directions** where updates won't harm critical knowledge.\n",
    "\n",
    "**Key Ideas:**\n",
    "- Break down weight matrices with **SVD** (like putting on X-Ray specs for your model)\n",
    "- Spot which directions in parameter space are pulling their weight vs. idle\n",
    "- Keep updates out of the \"critical\" directions, funnel them into unused space\n",
    "- End result: new learning without trashing old knowledge\n",
    "\n",
    "> *\"Who are YOU?\" said the Caterpillar. OSFT answers: \"I'm the same model, just viewing from a different angle -- my core identity (critical subspace) intact.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"6\"></a>\n",
    "## üîç [6] The Cheshire Cat's Grin: SVD Decomposition -- Interactive\n",
    "\n",
    "SVD is like the Cheshire Cat revealing which parts of itself are essential (the grin -- critical directions) and which can vanish (the body -- safe for modification).\n",
    "\n",
    "**Drag the rank slider** to see which singular values are protected (red=critical) vs. available for updates (green=safe).\n",
    "\n",
    "> *The larger the singular value, the more \"grin-like\" -- essential to the model's identity. Smaller values can fade without losing who the cat really is.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interactive SVD Explorer\n",
    "rank_slider = widgets.IntSlider(value=2, min=1, max=6, step=1,\n",
    "    description='Rank cutoff:', layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': '100px'})\n",
    "button_svd = widgets.Button(description='Regenerate', layout=widgets.Layout(width='150px'))\n",
    "output_svd = widgets.Output()\n",
    "\n",
    "svd_svals = None\n",
    "\n",
    "def regenerate_svd(_=None):\n",
    "    global svd_svals\n",
    "    base = np.array([3.5, 2.1, 1.3, 0.7, 0.25, 0.05])\n",
    "    svd_svals = np.sort(base + np.random.uniform(-0.3, 0.3, size=6))[::-1]\n",
    "    draw_svd()\n",
    "\n",
    "def draw_svd(change=None):\n",
    "    if svd_svals is None:\n",
    "        return\n",
    "    cutoff = rank_slider.value\n",
    "    with output_svd:\n",
    "        clear_output(wait=True)\n",
    "        n = len(svd_svals)\n",
    "        colors = ['#f87171' if i < cutoff else '#4ade80' for i in range(n)]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(9, 3.8))\n",
    "        bars = ax.bar(range(n), svd_svals, color=colors, width=0.6, edgecolor='none')\n",
    "        for i, (bar, s) in enumerate(zip(bars, svd_svals)):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.08,\n",
    "                    f'{s:.3f}', ha='center', va='bottom', fontsize=9, color='#e4e4e7')\n",
    "\n",
    "        ax.axvline(x=cutoff - 0.5, color='#e4e4e7', linestyle='--', linewidth=1.2, alpha=0.6)\n",
    "        ax.text(cutoff - 0.5, max(svd_svals) * 1.05, '<-- critical | safe -->',\n",
    "                ha='center', fontsize=8, color='#8888a0')\n",
    "        ax.set_xticks(range(n))\n",
    "        ax.set_xticklabels([f's{i+1}' for i in range(n)])\n",
    "        ax.set_ylabel('Singular Value')\n",
    "        ax.set_title('SVD: Critical vs Safe Directions', fontsize=11, fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.2)\n",
    "\n",
    "        crit_patch = mpatches.Patch(color='#f87171', label=f'Critical (top {cutoff})')\n",
    "        safe_patch = mpatches.Patch(color='#4ade80', label=f'Safe ({n-cutoff} remaining)')\n",
    "        ax.legend(handles=[crit_patch, safe_patch], fontsize=8, loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        for i, s in enumerate(svd_svals):\n",
    "            tag = \"[CRITICAL]\" if i < cutoff else \"[SAFE]    \"\n",
    "            print(f\"  s{i+1} = {s:.4f}  {tag}\")\n",
    "\n",
    "rank_slider.observe(draw_svd, names='value')\n",
    "button_svd.on_click(regenerate_svd)\n",
    "display(widgets.HBox([rank_slider, button_svd]), output_svd)\n",
    "regenerate_svd()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"7\"></a>\n",
    "## üß≠ [7] Finding Your Way: Gradient Projection -- Interactive\n",
    "\n",
    "*\"Would you tell me, please, which way I ought to go from here?\" asked Alice.*\n",
    "\n",
    "The **critical direction** (red dashed) must not be disturbed. The **gray arrow** is where vanilla training would step. OSFT projects that gradient to produce the **safe update** (green) -- orthogonal to the critical direction.\n",
    "\n",
    "**Drag the sliders** to change the gradient direction and watch the projection update live."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interactive Gradient Projection Demo\n",
    "gx_slider = widgets.FloatSlider(value=0.5, min=-2, max=2, step=0.1,\n",
    "    description='Gradient X:', layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': '90px'})\n",
    "gy_slider = widgets.FloatSlider(value=0.8, min=-2, max=2, step=0.1,\n",
    "    description='Gradient Y:', layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': '90px'})\n",
    "output_proj = widgets.Output()\n",
    "\n",
    "crit_raw = np.array([1.0, 0.5])\n",
    "crit_n = crit_raw / np.linalg.norm(crit_raw)\n",
    "\n",
    "def draw_projection(change=None):\n",
    "    gx, gy = gx_slider.value, gy_slider.value\n",
    "    grad = np.array([gx, gy])\n",
    "    dot = np.dot(grad, crit_n)\n",
    "    proj = dot * crit_n\n",
    "    safe = grad - proj\n",
    "\n",
    "    with output_proj:\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(7, 6))\n",
    "        lim = 3\n",
    "        ax.set_xlim(-lim, lim); ax.set_ylim(-lim, lim)\n",
    "        ax.set_aspect('equal'); ax.grid(True, alpha=0.2)\n",
    "        ax.axhline(0, color='#2a2a3a', linewidth=0.8)\n",
    "        ax.axvline(0, color='#2a2a3a', linewidth=0.8)\n",
    "\n",
    "        t = lim * 1.5\n",
    "        ax.plot([-crit_n[0]*t, crit_n[0]*t], [-crit_n[1]*t, crit_n[1]*t],\n",
    "                '--', color='#f87171', linewidth=1.5, alpha=0.6, label='Critical Direction')\n",
    "\n",
    "        if np.linalg.norm(grad) > 0.05:\n",
    "            ax.annotate('', xy=grad, xytext=(0,0),\n",
    "                arrowprops=dict(arrowstyle='->', color='#8888a0', lw=2.2))\n",
    "            ax.text(grad[0]+0.1, grad[1]+0.1, 'Original', fontsize=8, color='#8888a0')\n",
    "        if np.linalg.norm(proj) > 0.05:\n",
    "            ax.annotate('', xy=proj, xytext=(0,0),\n",
    "                arrowprops=dict(arrowstyle='->', color='#fbbf24', lw=1.5))\n",
    "            ax.text(proj[0]+0.1, proj[1]-0.15, 'Projection', fontsize=8, color='#fbbf24')\n",
    "        if np.linalg.norm(safe) > 0.05:\n",
    "            ax.annotate('', xy=safe, xytext=(0,0),\n",
    "                arrowprops=dict(arrowstyle='->', color='#4ade80', lw=2.8))\n",
    "            ax.text(safe[0]+0.1, safe[1]+0.1, 'OSFT Update', fontsize=8, color='#4ade80',\n",
    "                    fontweight='bold')\n",
    "        if np.linalg.norm(proj) > 0.05:\n",
    "            ax.plot([grad[0], safe[0]], [grad[1], safe[1]], ':', color='#fbbf24', alpha=0.4)\n",
    "\n",
    "        ax.set_title('Gradient Projection -- OSFT in Action', fontsize=11, fontweight='bold')\n",
    "        ax.legend(fontsize=8, loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        orig_norm = np.linalg.norm(grad)\n",
    "        safe_norm = np.linalg.norm(safe)\n",
    "        reduction = ((orig_norm - safe_norm) / orig_norm * 100) if orig_norm > 0 else 0\n",
    "        print(f\"  Original Gradient:         [{gx:.2f}, {gy:.2f}]\")\n",
    "        print(f\"  Projection onto Critical:  [{proj[0]:.2f}, {proj[1]:.2f}]\")\n",
    "        print(f\"  OSFT Update (safe):        [{safe[0]:.2f}, {safe[1]:.2f}]\")\n",
    "        print(f\"  Magnitude Reduction:       {reduction:.1f}%\")\n",
    "        print(f\"\")\n",
    "        print(f\"  The green arrow = where OSFT directs the update,\")\n",
    "        print(f\"  safely away from critical learned features.\")\n",
    "\n",
    "gx_slider.observe(draw_projection, names='value')\n",
    "gy_slider.observe(draw_projection, names='value')\n",
    "display(gx_slider, gy_slider, output_proj)\n",
    "draw_projection()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"8\"></a>\n",
    "## üß™ [8] Hands-On: NumPy Warm-Up\n",
    "\n",
    "Before touching PyTorch, let's warm up. We take a toy weight matrix, run SVD to split it into important vs. not-so-important directions, and project a gradient update into the \"safe\" zone."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# A toy weight matrix (e.g. from a linear layer)\n",
    "W = np.array([[2.0, 0.5, 0.0],\n",
    "              [0.0, 1.5, 0.1],\n",
    "              [0.0, 0.0, 0.2]])\n",
    "print(\"Original weight matrix W:\")\n",
    "print(W)\n",
    "\n",
    "# Perform SVD decomposition\n",
    "U, S, Vt = np.linalg.svd(W)\n",
    "print(\"\\nSingular values:\", S)\n",
    "\n",
    "# Define high-rank vs low-rank subspaces\n",
    "rank_cutoff = 1  # keep top-1 singular vector as important\n",
    "U_high = U[:, :rank_cutoff]\n",
    "V_high = Vt[:rank_cutoff, :].T\n",
    "\n",
    "# Any gradient update\n",
    "grad = np.array([[0.1, -0.2, 0.05],\n",
    "                 [0.05, 0.1, -0.1],\n",
    "                 [-0.2, 0.0, 0.2]])\n",
    "\n",
    "# Project gradient onto low-rank subspace (orthogonal to U_high, V_high)\n",
    "proj = grad - U_high @ (U_high.T @ grad @ V_high) @ V_high.T\n",
    "\n",
    "print(\"\\nOriginal gradient:\")\n",
    "print(grad)\n",
    "print(\"\\nProjected gradient (OSFT):\")\n",
    "print(proj)\n",
    "print(\"\\nThe projected gradient steers clear of the critical directions.\")\n",
    "print(\"That's OSFT on training wheels.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Pseudocode\n",
    "\n",
    "```python\n",
    "for each training step:\n",
    "    for each layer l in model:\n",
    "        W = layer.weight\n",
    "        U, S, Vt = svd(W)\n",
    "        r = retention_ratio(layer)  # based on importance\n",
    "        U_high = U[:, :r]\n",
    "        V_high = Vt[:r, :].T\n",
    "\n",
    "        grad = compute_gradient(layer)\n",
    "        grad_proj = grad - U_high @ (U_high.T @ grad @ V_high) @ V_high.T\n",
    "\n",
    "        apply_update(layer, grad_proj)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"9\"></a>\n",
    "## ‚ö° [9] Hands-On: PyTorch OSFT\n",
    "\n",
    "Time to level up. Here's a small PyTorch model with OSFT-style gradient projection applied. The model takes a step -- but only in the directions we allow.\n",
    "\n",
    "> *That's how OSFT threads the needle: new learning without wrecking the old foundation.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(3, 3, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SmallNet()\n",
    "x = torch.randn(5, 3)\n",
    "y = torch.randn(5, 3)\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "output = model(x)\n",
    "loss = loss_fn(output, y)\n",
    "loss.backward()\n",
    "\n",
    "# Inspect original gradient\n",
    "grad = model.fc.weight.grad.detach().numpy()\n",
    "print(\"Original gradient:\")\n",
    "print(grad)\n",
    "\n",
    "# SVD on weights\n",
    "W = model.fc.weight.detach().numpy()\n",
    "U, S, Vt = np.linalg.svd(W)\n",
    "print(f\"\\nSingular values of W: {S}\")\n",
    "\n",
    "# Keep top-1 singular vector as \"critical\"\n",
    "rank_cutoff = 1\n",
    "U_high = U[:, :rank_cutoff]\n",
    "V_high = Vt[:rank_cutoff, :].T\n",
    "\n",
    "# Project gradient into safe subspace\n",
    "grad_proj = grad - U_high @ (U_high.T @ grad @ V_high) @ V_high.T\n",
    "model.fc.weight.grad = torch.from_numpy(grad_proj).float()\n",
    "opt.step()\n",
    "\n",
    "print(\"\\nProjected gradient (OSFT):\")\n",
    "print(grad_proj)\n",
    "print(\"\\nUpdated weights (after OSFT projection):\")\n",
    "print(model.fc.weight.data.numpy())\n",
    "print(\"\\nThe model stepped -- but only in the directions we allowed.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"10\"></a>\n",
    "## üßÆ [10] OFT vs LoRA -- Parameter Efficiency Calculator\n",
    "\n",
    "**Full Fine-Tuning** rewrites every parameter. **OFT** only needs `rank^2` parameters. **LoRA** needs `rank x (input + output)`. Adjust the values below to see the savings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interactive Parameter Calculator\n",
    "in_w = widgets.IntText(value=512, description='Input features:', style={'description_width': '120px'})\n",
    "out_w = widgets.IntText(value=256, description='Output features:', style={'description_width': '120px'})\n",
    "rank_w = widgets.IntText(value=16, description='Rank:', style={'description_width': '120px'})\n",
    "output_calc = widgets.Output()\n",
    "\n",
    "def calc_params(change=None):\n",
    "    i, o, r = in_w.value, out_w.value, rank_w.value\n",
    "    total = i * o\n",
    "    oft = r * r\n",
    "    lora = r * (i + o)\n",
    "\n",
    "    with output_calc:\n",
    "        clear_output(wait=True)\n",
    "        methods = [\n",
    "            ('Full Fine-Tuning', total, '#f87171'),\n",
    "            ('LoRA',             lora,  '#fbbf24'),\n",
    "            ('OFT',              oft,   '#60a5fa'),\n",
    "        ]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(9, 2.8))\n",
    "        names = [m[0] for m in methods]\n",
    "        counts = [m[1] for m in methods]\n",
    "        bar_colors = [m[2] for m in methods]\n",
    "\n",
    "        bars = ax.barh(names, counts, color=bar_colors, height=0.5, edgecolor='none')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlabel('Parameters (log scale)')\n",
    "        ax.set_title('Parameter Count Comparison', fontsize=11, fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.2)\n",
    "\n",
    "        for bar, count in zip(bars, counts):\n",
    "            pct = count / total * 100\n",
    "            ax.text(count * 1.5, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{count:,}  ({pct:.2f}%)', va='center', fontsize=9, color='#e4e4e7')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"  Layer: {i} x {o} = {total:,} total parameters\\n\")\n",
    "        for name, count, _ in methods:\n",
    "            pct = count / total * 100\n",
    "            print(f\"  {name:20s}: {count:>10,}  ({pct:>7.2f}%)\")\n",
    "        print(f\"\\n  OFT Parameter Reduction: {(1 - oft/total)*100:.2f}%\")\n",
    "\n",
    "for w in [in_w, out_w, rank_w]:\n",
    "    w.observe(calc_params, names='value')\n",
    "display(widgets.HBox([in_w, out_w, rank_w]), output_calc)\n",
    "calc_params()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison\n",
    "\n",
    "| | OFT | LoRA | OSFT |\n",
    "|---|---|---|---|\n",
    "| **Approach** | Orthogonal rotations | Low-rank additive updates | Orthogonal + subspace protection |\n",
    "| **Preserves geometry** | Yes | No | Yes |\n",
    "| **Catastrophic forgetting** | Reduced | Possible | Eliminated |\n",
    "| **Best for** | Domain adaptation, few-shot | General LLM fine-tuning | Continual learning, multi-task |\n",
    "| **Parameters** | rank^2 | rank x (in + out) | rank^2 + SVD overhead |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"11\"></a>\n",
    "## üìà [11] Training Dynamics Comparison\n",
    "\n",
    "Watch how **Full Fine-Tuning** (red) starts strong but suffers catastrophic forgetting, while **OSFT** (green) maintains steady, superior performance throughout."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training Dynamics Simulation\n",
    "button_train = widgets.Button(description='Run Simulation', layout=widgets.Layout(width='170px'),\n",
    "                              button_style='success')\n",
    "button_reset = widgets.Button(description='Reset', layout=widgets.Layout(width='100px'))\n",
    "output_train = widgets.Output()\n",
    "\n",
    "def run_training(_=None):\n",
    "    with output_train:\n",
    "        clear_output(wait=True)\n",
    "        max_steps = 100\n",
    "        steps = np.arange(max_steps)\n",
    "\n",
    "        def gen(s, method):\n",
    "            base = 0.5 + (s / max_steps) * 0.4\n",
    "            if method == 'oft':  return base + np.sin(s/10) * 0.05\n",
    "            if method == 'lora': return base + np.sin(s/8) * 0.06 - 0.05\n",
    "            if method == 'osft': return base + np.sin(s/12) * 0.03 + 0.05\n",
    "            if method == 'full': return base if s < 60 else base - (s-60)/100\n",
    "            return base\n",
    "\n",
    "        methods_cfg = {\n",
    "            'OFT':     ('#60a5fa', 'oft'),\n",
    "            'LoRA':    ('#fbbf24', 'lora'),\n",
    "            'OSFT':    ('#4ade80', 'osft'),\n",
    "            'Full FT': ('#f87171', 'full'),\n",
    "        }\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 4.5))\n",
    "        for name, (color, key) in methods_cfg.items():\n",
    "            data = [gen(s, key) for s in steps]\n",
    "            ax.plot(steps, data, color=color, linewidth=2.2, label=name)\n",
    "\n",
    "        ax.set_xlabel('Training Steps')\n",
    "        ax.set_ylabel('Performance')\n",
    "        ax.set_title('Training Dynamics -- Fine-Tuning Methods Compared', fontsize=11, fontweight='bold')\n",
    "        ax.legend(fontsize=9, loc='lower right')\n",
    "        ax.grid(True, alpha=0.2)\n",
    "        ax.set_ylim(0.3, 1.05)\n",
    "\n",
    "        ax.annotate('<-- catastrophic\\n    forgetting', xy=(75, gen(75, 'full')),\n",
    "                    xytext=(82, 0.55), fontsize=8, color='#f87171',\n",
    "                    arrowprops=dict(arrowstyle='->', color='#f87171', lw=1.2))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"  Final performance at step 100:\")\n",
    "        for name, (_, key) in methods_cfg.items():\n",
    "            val = gen(99, key)\n",
    "            print(f\"     {name:10s}: {val:.3f}\")\n",
    "\n",
    "def reset_training(_=None):\n",
    "    with output_train:\n",
    "        clear_output(wait=True)\n",
    "        print(\"  Ready -- click Run Simulation\")\n",
    "\n",
    "button_train.on_click(run_training)\n",
    "button_reset.on_click(reset_training)\n",
    "display(widgets.HBox([button_train, button_reset]), output_train)\n",
    "print(\"  Ready -- click Run Simulation\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"12\"></a>\n",
    "## üåç [12] Real-World Applications\n",
    "\n",
    "| Use Case | Technique | Description |\n",
    "|---|---|---|\n",
    "| **Domain Adaptation** | OFT | Fine-tune to related domains without losing general knowledge |\n",
    "| **Few-Shot Learning** | OFT | Adapt with limited data while maintaining robustness |\n",
    "| **Continual Learning** | OSFT | Learn new tasks sequentially without forgetting |\n",
    "| **Enterprise Chatbots** | OSFT | Add new product knowledge without erasing old FAQs |\n",
    "| **Medical AI** | OSFT | Stay current with research without forgetting fundamentals |\n",
    "| **Legal Models** | OSFT | Incorporate new regulations while maintaining legal knowledge |\n",
    "\n",
    "### Benchmark Results\n",
    "- **Text classification sequences:** Keeps performance steady across 5, 10, 15+ tasks\n",
    "- **TRACE benchmark:** Boosted LLaMA-2-7B's accuracy by ~7 points over O-LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"13\"></a>\n",
    "## üé™ [13] Lessons from Wonderland: Wrap-Up\n",
    "\n",
    "### The Looking Glass Principle (OFT)\n",
    "Like spinning a mirror rather than cracking it, orthogonal transformations rotate the feature space without distortion. Alice stays Alice, just viewed from a new angle.\n",
    "\n",
    "### The Cheshire Grin Strategy (OSFT)\n",
    "Keep the grin (critical directions), let the body fade (safe subspace). OSFT identifies what's essential and protects it -- no catastrophic forgetting.\n",
    "\n",
    "### The Mad Hatter's Efficiency\n",
    "90%+ parameter reduction! Fewer parameters to tune, but the tea party keeps its charm.\n",
    "\n",
    "### The Queen's Stability\n",
    "Orthogonal constraints keep gradients well-behaved -- no explosion, no vanishing. Off with gradient chaos!\n",
    "\n",
    "Think of it like renovating a house. **OSFT adds a new room without tearing down the walls that are already holding up the place.**\n",
    "\n",
    "---\n",
    "\n",
    "> *\"Curiouser and curiouser!\" cried Alice. And indeed -- the deeper you go down this orthogonal rabbit hole, the more elegant the mathematics becomes.*\n",
    "\n",
    "---\n",
    "\n",
    "*Created by **Frank La Vigne** -- Interactive Notebook Edition*\n",
    "\n",
    "*\"It's no use going back to yesterday, because I was a different person then.\" -- But with OSFT, your model remembers who it was!*"
   ]
  }
 ]
}