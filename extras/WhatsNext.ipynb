{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2ac952-9905-4a3e-af27-3c51a744575e",
   "metadata": {},
   "source": [
    "# What Comes Next\n",
    "\n",
    "You just spent four hours building a pipeline that took a document, chunked it,\n",
    "embedded it, retrieved from it, evaluated it, and adapted a model against it.\n",
    "That is real. The pipeline works. The eval scores are real scores.\n",
    "\n",
    "And it was four hours.\n",
    "\n",
    "What follows is an honest map of what a production engagement with this\n",
    "technology actually looks like.\n",
    "\n",
    "---\n",
    "\n",
    "## What You Built Is a Mental Model\n",
    "\n",
    "The lab was not designed to produce a deliverable. It was designed to give you\n",
    "felt experience with a problem space that your customers are trying to navigate,\n",
    "so you can ask better questions, spot bad assumptions earlier, and know when to\n",
    "escalate to the people who do the build.\n",
    "\n",
    "That is a different kind of value than a working system. It is also the right\n",
    "kind for your role.\n",
    "\n",
    "Production is a different problem than the lab. Not a harder version of the same\n",
    "problem. A different one.\n",
    "\n",
    "The difference is not the model. It is everything around the model.\n",
    "\n",
    "---\n",
    "\n",
    "## This is a Process and Works as a Chain\n",
    "\n",
    "Every stage of the pipeline depends on the stage before it. Bad ingestion\n",
    "produces bad chunks. Bad chunks produce bad embeddings. Bad embeddings produce\n",
    "bad retrieval. Bad retrieval means the model never sees the right information,\n",
    "and no amount of fine-tuning will fix that.\n",
    "\n",
    "This is not a collection of independent problems. It is a chain. And a chain is\n",
    "only as strong as its weakest link.\n",
    "\n",
    "In the lab, we controlled for this. The document was clean. The format was\n",
    "consistent. The evaluation questions were chosen to be answerable. Real\n",
    "engagements do not come pre-cleaned.\n",
    "\n",
    "The work of a real engagement is largely the work of finding the weak link,\n",
    "fixing it, and confirming the fix improved the system. Then finding the next\n",
    "one. That process takes time not because the technology is immature, but because\n",
    "the problem is genuinely complex and the complexity lives in the details of each\n",
    "customer's specific corpus, questions, and definition of correct.\n",
    "\n",
    "---\n",
    "\n",
    "### Corpus deduplication\n",
    "In the multi-document section of Day 2, three retrieval\n",
    "slots were consumed by three versions of the same rulebook. The current edition\n",
    "was not retrieved at all. In a customer corpus with hundreds of versioned\n",
    "documents, this problem gets worse, not better. The fix is upstream corpus\n",
    "curation, not a model change.\n",
    "\n",
    "### Metadata filtering\n",
    "Once you have a multi-document corpus, retrieval needs\n",
    "to be smarter than \"find the most similar chunks.\" You often need to filter by\n",
    "document type, date range, department, or authority level before ranking by\n",
    "similarity. This requires metadata strategy at ingestion time. Without it, a\n",
    "question about current policy might return a chunk from a three-year-old version\n",
    "that scored higher on similarity.\n",
    "\n",
    "### Chunking strategy comparison\n",
    "The lab used a single chunking approach. In\n",
    "practice, the right strategy depends on document structure. Fixed-size chunking,\n",
    "semantic chunking, hierarchical chunking, and document-structure-aware chunking\n",
    "produce meaningfully different retrieval behavior against different content types.\n",
    "A chunk boundary that splits a table in half means neither piece is retrievable\n",
    "on its own, and no retrieval tuning will fix that.\n",
    "\n",
    "### LLM-as-judge evaluation\n",
    "Keyword matching catches clear failures. It misses\n",
    "nuanced correctness issues. Production evaluation typically requires a second\n",
    "model evaluating the first model's answers against reference answers, with human\n",
    "spot-checking on a sample.\n",
    "\n",
    "### Embedding model selection\n",
    "We used a specific Granite embedding model. In\n",
    "production, embedding model choice depends on domain vocabulary, context length\n",
    "requirements, multilingual needs, and latency constraints. This is a\n",
    "benchmarking exercise, not a default. A model trained on general web text can\n",
    "consistently fail to retrieve the right chunks simply because the domain uses\n",
    "technical terminology the embedding space has never seen.\n",
    "\n",
    "### Reranking\n",
    "Retrieval returns candidates. Reranking re-scores them using a\n",
    "separate, more expensive model before passing them to the generator. It\n",
    "consistently improves answer quality in production systems and is consistently\n",
    "skipped in early-stage work.\n",
    "\n",
    "Each of these is a place where the chain can break.\n",
    "\n",
    "---\n",
    "\n",
    "## The Conversation to Have with Your Customer\n",
    "\n",
    "When a customer says they want to build a RAG system, the instinct is to talk\n",
    "about models. The right conversation starts somewhere else.\n",
    "\n",
    "Ask them: how are you measuring correct?\n",
    "\n",
    "If they cannot answer that, neither can you. And any change you make, to\n",
    "retrieval, to chunking, to the model, becomes unjustifiable, because there is no\n",
    "before and after to compare.\n",
    "\n",
    "The evaluation set you build with their subject matter experts, even ten or\n",
    "twenty questions with agreed-upon reference answers, is often the most productive\n",
    "artifact of the first month. It forces the customer to articulate standards they\n",
    "have never written down. It gives you a shared definition of done. And it gives\n",
    "every subsequent recommendation a defensible basis.\n",
    "\n",
    "When they ask whether they need fine-tuning, the answer that earns trust is not\n",
    "yes or no. It is: \"Here is what the evaluation shows. Here is what is failing\n",
    "and why. Here is what we would try before we consider changing the model.\"\n",
    "\n",
    "That positions you as someone who solves problems methodically, not someone who\n",
    "sells the most expensive option.\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "### Retrieval and RAG\n",
    "\n",
    "[RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)\n",
    "(paper) -- the academic basis for automated RAG evaluation, introducing metrics\n",
    "that do not require human-annotated ground truth.\n",
    "\n",
    "[LangChain Text Splitters documentation](https://python.langchain.com/docs/concepts/text_splitters/)\n",
    "-- practical comparisons of fixed-size, semantic, and hierarchical chunking\n",
    "approaches with code examples.\n",
    "\n",
    "[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)\n",
    "(paper) -- explains why what you retrieve and where it appears in the prompt\n",
    "both matter. Performance degrades significantly when relevant information appears\n",
    "in the middle of a long context.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "[Hugging Face TRL documentation](https://huggingface.co/docs/trl/index) -- the\n",
    "library used in Day 3, with production-grade examples for training and\n",
    "evaluation workflows.\n",
    "\n",
    "[MLflow documentation](https://mlflow.org/docs/latest/index.html) -- experiment\n",
    "tracking and model registry, relevant to managing iteration cycles across a real\n",
    "engagement.\n",
    "\n",
    "### Fine-Tuning and Model Adaptation\n",
    "\n",
    "[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "(paper) -- the foundational paper for the adaptation technique used in Day 3.\n",
    "Explains why you can train a tiny fraction of parameters and still get\n",
    "meaningful results.\n",
    "\n",
    "[LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206) (paper) --\n",
    "evidence that data quality matters more than data quantity. A model fine-tuned\n",
    "on 1,000 carefully curated examples outperformed models trained on far larger\n",
    "datasets.\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "[Building LLM Applications for Production](https://huyenchip.com/2023/04/11/llm-engineering.html)\n",
    "(Chip Huyen) -- one of the most referenced practical guides on the gap between\n",
    "demos and production. The core observation maps directly to what you just built:\n",
    "it is easy to make something cool with LLMs, and very hard to make something\n",
    "production-ready.\n",
    "\n",
    "---\n",
    "\n",
    "*This document is part of the extras folder. Nothing here is required.\n",
    "All of it is real.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}